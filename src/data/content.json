{
  "blog": [
    {
      "slug": "aic-bic",
      "title": "Akaike and Bayesian Information Criterion",
      "date": "2023-12-27",
      "description": "Akaike information criterion (AIC) and Bayesian information criterion (BIC) are powerful tools for model selection and comparison in machine learning. In this article we will see what AIC and BIC are and how to use them.",
      "content": "<p>One of the key problems in machine learning is knowing which model to use. Akaike information criterion (AIC), and Bayesian information criterion (BIC) \n                            are powerful tools used to perform model selection that can help us determine which model is best for our data. In this article we will see what AIC and BIC are and how to use them.</p>\n                        \n                        <h4>Important Background Information</h4>\n                        <p>Before diving into what AIC is, we must establish some foundational knowledge and terminology.</p>\n\n                        <p>First, consider a collection of models, given by\n                            $$\\mathscr{M}_j = \\{f_j(z | \\boldsymbol{\\theta})|\\boldsymbol{\\theta}\\in \\Theta_j \\}.$$\n\n                            In this, we have that each model is parameterized by some set of distributions, $\\boldsymbol{\\theta}$.\n                            Let $\\widehat{\\boldsymbol{\\theta}}$ be the maximum likelihood estimator for $\\boldsymbol{\\theta}$ in model $j$.\n                        </p>\n\n                        <p>\n                            It is important to note that the true distribution $f(z)$ for the data not might be in $\\mathscr{M}_j$.\n                            In this case, we want to identify the model that is closest to the true distribution. If other model selection criteria do not apply\n                            (such as train-test split or cross validation), then we can use AIC and BIC to help us determine which model is best.\n                        </p>\n\n                        <h4>Akaike Information Criterion</h4>\n\n                        <p>\n                            THe main idea behind AIC is that we want to choose the MLE $\\widehat{\\boldsymbol{\\theta}}$ for model $M_j$ that minimizes the relative entropy \n                            (KL-divergence) between our selected distribution, and the true distribution $f(z)$.\n\n                            So, given some estimate $\\widehat{\\boldsymbol{\\theta}_j}$ with $\\widehat{f_j}(z) = f(z|\\widehat{\\boldsymbol{\\theta}_j})$, we want to minimize\n\n                            $$\\mathscr{D}_{\\text{KL}}(f||\\widehat{f}_j) = \\int f(z)\\log(f(z))dz - \\int f(z)\\log(\\widehat{f}_j(z))dz.$$\n\n                            Note, the first integral is completely independent of $j$ and $\\widehat{\\boldsymbol{\\theta}_j}$. This means\n                            that in order to effectively minimize the relative entropy with respect to $j$, we need to minimize the second integral, given by\n\n                            $$\\widehat{K}_j = - \\int f(z)\\log(\\widehat{f}_j(z))dz.$$\n\n                            Given this integral is intractable (and considering $f(z)$ is probably not known), we can instead use the following approximation:\n\n                            $$\n                            \\widehat{K}_j \\approx -\\frac{1}{n}\\sum_{i=1}^n\\log(\\widehat{f}_j(z_i)).$$\n                        </p>\n\n                        <p>\n                            Nice as this may seem, it is important to note that this estimator is biased. This is because the data \n                            $z_1, \\dots, z_n$ is used to estimate $\\widehat{\\boldsymbol{\\theta}_j}$, which is then used to estimate $\\widehat{K}_j$.\n                            But, Akaike found and <a href=\"https://ieeexplore.ieee.org/document/1100705\" target=\"_blank\" rel=\"noopener noreferrer\">proved</a> \n                            that the bias of this estimator is approximately $-\\frac{k_j}{n}$, where $k_j$ is the dimension of the parameter space\n                            $\\Theta_j$.\n\n                            So, we now have that the approximation for the unbiased estimator for $\\widehat{K}_j$ is given by\n\n                            $$\\widehat{K}_j \\approx \\frac{k_j}{n}-\\frac{1}{n}\\sum_{i=1}^n\\log(\\widehat{f}_j(z_i)).$$\n\n                            If we multiply by $2n$, this give us the AIC. \n\n                            It is important to note that that the AIC is typically given by\n\n                            $$\\text{AIC}(j) = 2k_j - 2\\ell_j(\\widehat{\\boldsymbol{\\theta}}_j)$$\n\n                            where $\\ell_j(\\widehat{\\boldsymbol{\\theta}}_j) = \\sum_{i=1}^{n}\\log(f_j(z_i|\\widehat{\\boldsymbol{\\theta}}))$ is the MLE, and $k_j = \\text{dim}(\\Theta_j)$ is the dimension of the parameter space. Also, \n                            it is important to know that the $2$ is present for historical reasons. Since we are minimizing the AIC, and constant really doesn't matter.\n                        </p>\n\n                        <h4>Bayesian Information Criterion</h4>\n\n                        <p>\n                            Next we talk about Bayesian Information Criterion (BIC). BIC is simply an alternative to the AIC and is similar in many ways. The key difference between AIC and BIC, however, \n                            is that in BIC, instead of minimizing the relative entropy between the true distribution and the selected distribution, we instead maximize the posterior probability of a selected model. \n                            Thus, we define the BIC to be\n\n                            $$\\text{BIC}(j) = k_j \\log(n) - 2\\ell_j(\\widehat{\\boldsymbol{\\theta}}_j),$$\n\n                            again, where $\\ell_j(\\widehat{\\boldsymbol{\\theta}}_j) = \\sum_{i=1}^{n}\\log(f_j(z_i|\\widehat{\\boldsymbol{\\theta}}))$ is the MLE, and $k_j = \\text{dim}(\\Theta_j)$ is the dimension of the parameter space. In this case,\n                            $n$ is the number of data points.\n                            (For those interested in the derivation and proof of the BIC, you can find it <a href=\"https://statproofbook.github.io/P/bic-der.html\" target=\"_blank\" rel=\"noopener noreferrer\">here</a>. Note the notational\n                            difference between the proof and the formula presented here.)\n                        </p>\n\n                        <h4>Differences Between AIC and BIC</h4>\n\n                        <p>\n                            It is easy to see that AIC and BIC only differ by the first term of their formula. AIC's first term is $2k_j$, and BIC's first term is $k_j \\log(n)$. Since\n                            $n$ is typically large, this means that, generally, $\\log(n) > 2$. This tells us that BIC will penalize models with more parameters more than AIC will. This means that BIC will tend to select simpler models than AIC.\n                            Thus, AIC is more likely to choose a model that is too complex, and BIC is more liekly to choose a model that is too simple. So, which one you choose to use is dependent entirely on your situation and what you are trying\n                            to accomplish. For example, if you think there are unnecessary parameters in your model, then BIC might be a better choice. If you think that there are important parameters that you do not want to leave out, then AIC might be a better choice.\n                        </p>\n\n                        <h4>A Quick Python Example</h4>\n\n                        <p>\n                            Now that we have seen what AIC and BIC are, let's see how we can use them in Python. For this example, we will use the <code>OLS</code> model from <code>statsmodels</code>, and the \n                             <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html\" target=\"_blank\" rel=\"noopener noreferrer\">California housing dataset</a> from scikit-learn.\n                        </p>\n\n                        <p>\n                            First, begin by importing the necessary libraries and loading in the data.\n                        </p>\n\n                        <pre>\n                            <code class=\"language-python\">\n# Import all the necessary things from sklearn.\nfrom sklearn.datasets import fetch_california_housing\nimport statsmodels.api as sm\n\n# Load in the data. Split into X and y, and make them Pandas dataframes.\nX, y = fetch_california_housing(return_X_y=True, as_frame=True)</code>\n                        </pre>\n\n                        <p>\n                            Briefly looking at the data, we see that there are 8 features, and 20,640 data points, with the $X$ looking like\n                        </p>\n\n                        <pre>\n                            <code class=\"language-python\">\nX.head()\n   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \n0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n                                \n   Longitude  \n0    -122.23  \n1    -122.22  \n2    -122.24  \n3    -122.25  \n4    -122.25  </code>\n                        </pre>\n\n                        <p>\n                            and the $y$ looking like\n                        </p>\n\n                        <pre>\n                            <code class=\"language-python\">\ny.head()\n0    4.526\n1    3.585\n2    3.521\n3    3.413\n4    3.422\nName: MedHouseVal, dtype: float64</code>\n                        </pre>\n\n                        <p>\n                            We now want to add a constant to the $X$ data (since <code>statsmodels</code> doesn't do that naturally), and then fit the model.\n                        </p>\n\n                        <pre>\n                            <code class=\"language-python\">\n# Add a constant to the X matrix, as statsmodels doesn't do that by default.\nX_sm = sm.add_constant(X)\n\n# Initialize our OLS model, and fit the data.\nmodel_sm = sm.OLS(y, X_sm)\nresults = model_sm.fit()</code>\n                        </pre>\n\n                        <p>\n                            Now that we have fit the model, we simply use the <code>aic</code> and <code>bic</code> attributes of the <code>results</code> object to get the AIC and BIC values.\n                        </p>\n\n                        <pre>\n                            <code class=\"language-python\">\n# Get the AIC and BIC\nresults.aic\n>>> 45265.54161\nresults.bic\n>>> 45336.95649</code>\n                        </pre>\n\n                        <p>\n                            Now, these numbers by themselves are more or less meaningless. However, if we begin to perform model selection (such as stepwise feature removal), then we can use these numbers to help us determine which model is best.\n                        </p>\n\n                        \n                        \n                        <h4>Conclusion</h4>\n                        <p>And there you have it! AIC and BIC are powerful ideas that can help us build useful models for our data. I hope that you can now see how useful and important these ideas are and how they can help\n                            you with your next machine learning project. To see the code use in this article, you can find it \n                            on my <a href=\"https://github.com/dylanskinner65/dylanskinner65.github.io/blob/ec4b663f532eaafd71172c3334de2b97610bb0d2/blog/blog_files/aic_bic/aic_bic.ipynb\" target=\"_blank\" rel=\"noopener noreferrer\">Github</a>.\n\n                            \n                        </p>",
      "quote": "All models are wrong, but some are useful.",
      "quoteAuthor": "George Box",
      "category": "Math & ML"
    },
    {
      "slug": "braids",
      "title": "Introduction to Knot Theory: Braids, Alexander's Theorem, and Markov's Theorem",
      "date": "2024-02-21",
      "description": "In this blog post we build upon a previous post. We will be discussing braids, Alexander's theorem, and Markov's theorem.",
      "content": "<p>\n                           Continuing on from my previous <a href=\"intro_to_knots.html\" target=\"_blank\" rel=\"noopener noreferrer\">blog post</a>, this blog\n                           post focuses on braids, Alexander's Theorem, and Markov's Theorem.\n                        </p>\n\n                        <figure>\n                            <img src=\"blog_files/braids/braid_intro_pic.jpg\" alt=\"Fun knitted braid.\" width=\"90%\" height=\"90%\">\n                            <figcaption text-align=center>A fun braid. Credit to <a href=\"https://en.wikipedia.org/wiki/File:Knitcable.jpg\" target=\"_blank\" rel=\"noopener noreferrer\">Wikipedia</a>.</figcaption>\n                        </figure>\n                        \n                        <p></p>\n\n                        <h4>\n                            Introduction to Braids\n                        </h4>\n\n                        <p>\n                            In knot theory, knots can be represented in many different ways besides using the planar projections described in my previous  \n                            <a href=\"intro_to_knots.html\" target=\"_blank\" rel=\"noopener noreferrer\">blog post</a>. One of these ways is using <em>braid</em> closures. \n                            Braids are particularly helpful because every knot can be described as the closure of a braid. A braid is a set of $n$ strings that are \n                            attached to a horizontal bar at the top and the bottom and which travel monotonically downwards \n                            (see <a href=\"#figure1\">Figure 1</a>). \n                            These strings can cross over or underneath each other, but they cannot loop back up. Another way of putting this is that each \n                            string can cross any horizontal plane only one time.  We let $B_n$ denote the set of all braids with $n$ strands.\n                        </p>\n\n                        <figure>\n                            <img src=\"blog_files/braids/Figure1.png\" alt=\"Example braid.\" width=\"90%\" height=\"90%\" id=\"figure1\">\n                            <figcaption text-align=center><b>Figure 1:</b> An example of a braid. Braids can have any number of strings \n                                and any number of crossings.</figcaption>\n                        </figure>\n\n                        <p>\n                            Similar to knots, there is a notion of equivalency for braids. In order to see that two braids are equivalent, \n                            we must be able to rearrange the strings of the braid without removing the strings from the top or bottom bar, \n                            and without allowing the strings to pass through each other or themselves.\n                        </p>\n\n                        <figure>\n                            <img src=\"blog_files/braids/Figure2.png\" alt=\"Equivalent briads.\" width=\"90%\" height=\"90%\" id=\"figure2\">\n                            <figcaption text-align=center><b>Figure 2:</b> Even though these braids are not completely the same, since you \n                                can apply a series of Reidemeister moves to one and get the other, they are equivalent.</figcaption>\n                        </figure>\n\n                        <p>\n                            When we have a braid, we can turn that braid into a knot by connecting the top and bottom bars together.\n                            The resulting form is a knot or a link, and this is called the <em>closure of the braid</em> (see <a href=\"#figure3\">Figure 3</a>).\n                        </p>\n\n                        <figure>\n                            <img src=\"blog_files/braids/Figure3.png\" alt=\"Closure of a braid.\" width=\"90%\" height=\"90%\" id=\"figure3\">\n                            <figcaption text-align=center><b>Figure 3:</b> On the left, we have a braid. On the right, we have the \n                                closure of this braid, which is a knot.</figcaption>\n                        </figure>\n\n                        <p>\n                            As mentioned previously, one nice feature of braids is that every knot can be represented as the closure of a braid. \n                            This helpful fact was proven by J.W. Alexander in 1923 and is known as Alexander's Theorem.\n                        </p>\n\n                        <h4>Alexander's Theorem</h4>\n\n                        <p>\n                            Alexander's theorem states that every knot or link can be expressed as the closure of a braid. (See chapter 5.4 in <a href=\"#adams\">[1]</a> for more.)\n                        </p>\n\n                        <p>\n                            Simple as this theorem may be, it is incredibly helpful for working with knots. Since every knot can be represented \n                            as a braid closure, we have another useful way of studying and classifying knots. In the same vein, one useful \n                            quantity to consider when thinking of knots as braid closures is the <em>braid index</em>.\n                        </p>\n\n                        <p>\n                            The braid index of a knot is defined to be the fewest number of strings in a braid whose closure is the knot of \n                            interest <a href=\"#adams\">[1]</a>. For example, the unknot (which is simply a circle) has a braid index of 1 since it can be expressed as the closure of a braid with a single strand (and no crossings). \n                        </p>\n\n                        <figure>\n                            <img src=\"blog_files/braids/Figure4.png\" alt=\"Closure of a braid.\" width=\"90%\" height=\"90%\" id=\"figure4\">\n                            <figcaption text-align=center><b>Figure 4:</b> On the left, we have the simplest diagram of the unknot. On the right, \n                                we still have the unknot, but as the closure of a two strand braid with a single crossing. This simple example \n                                shows that different braids can have the same closure.</figcaption>\n                        </figure>\n\n                        <p>\n                            While calculating the braid index seems simple, it can actually be quite tricky. If we represent a \n                            knot in braid form and then count the strings of the braid, it does not guarantee that we have achieved \n                            the least number of strings possible for that knot. Counting the strings of a braid representative can \n                            certainly give us an upper bound on the braid index, but finding the actual minimal braid index requires more work.\n                        </p>\n\n                        <h4>Braid Words</h4>\n\n                        <p>\n                            In order to fully describe a braid, we  look first at its projection. Once we have the projection of the braid\u2014and \n                            ensure that no two crossings occur at the same height\u2014we describe the braid by listing the strings that cross over \n                            and under other strings as we move toward the bottom bar. We always label the crossings from left to right. When \n                            the first string crosses under the second string, we call this crossing a $\\sigma_{1}$ crossing. On the other hand, \n                            if the first string crosses over the second string, we call this crossing a $\\sigma_{1}^{-1}$ crossing. If the \n                            second string crosses under the third, it is a $\\sigma_{2}$ crossing, and if the second string crosses over \n                            the third, it is a $\\sigma_{2}^{-1}$ crossing. This pattern continues, with a crossing of the $j^{th}$ strand \n                            passing under the $(j+1)^{th}$ strand being labeled as $\\sigma_j$, while $\\sigma_j^{-1}$ denotes the $j^{th}$ strand \n                            passing over the $(j+1)^{th}$ strand.  To describe a braid then we simply start at the top of the braid, and list \n                            off the crossings we encounter as we travel from top to bottom. We call the resulting sequence of crossings a <em>braid word</em>.\n                        </p>\n\n                        <figure>\n                            <img src=\"blog_files/braids/Figure5.png\" alt=\"Braid word example.\" width=\"60%\" height=\"0%\" id=\"figure5\">\n                            <figcaption text-align=center><b>Figure 5:</b> The braid word for this braid is $\\sigma_{1}^{-1}\\sigma_{2}\\sigma_{1}^{-1}$</figcaption>\n                        </figure>\n\n                        <p>\n                            If we look at the braid in <a href=\"#figure5\">Figure 5</a>, we can see that by listing the crossings from top to \n                            bottom, we obtain the braid word $\\sigma_{1}^{-1}\\sigma_{2}\\sigma_{1}^{-1}$.\n                        </p>\n\n                        <p>\n                            Along with giving a convenient way to describe the braid, there are other advantages to using braid representations of knots. \n                            One advantage is identifying which Reidemeister moves can be applied to simplify the braid. For example, if a braid word \n                            contains $\\sigma_{k}\\sigma_{k}^{-1}$, we know that the $k^{th}$ string goes under the ${(k+1)}^{th}$, and then \n                            immediately passes back underneath of it, returning to its original position. If we apply a simple Reidemester II move \n                            to this pair of crossings, the strings will straighten out and we are left with an equivalent braid \n                            (see <a href=\"#figure6\">Figure 6</a>). \n                        </p>\n\n                        <figure>\n                            <img src=\"blog_files/braids/Figure6.png\" alt=\"Braid word example.\" width=\"100%\" height=\"80%\" id=\"figure6\">\n                            <figcaption text-align=center><b>Figure 6:</b> A Reidemester II move applied to a braid.</figcaption>\n                        </figure>\n\n                        <p>\n                            For a more complicated example, say we have the braid word $\\sigma_{1}\\sigma_{3}\\sigma_{2}\\sigma_{2}^{-1}\\sigma_{3}^{-1}\\sigma_{4}\\sigma_{3}$. \n                            Through a series of Reidemester II moves, we can take this word and simplify it to $\\sigma_{1}\\sigma_{3}\\sigma_{3}^{-1}\\sigma_{4}\\sigma_{3}$,\n                            and then further to $\\sigma_{1}\\sigma_{4}\\sigma_{3}$. This leaves us with a much simpler braid word which represents a \n                            braid that is equivalent to the original braid.\n                        </p>\n\n                        <p>\n                            Another modification we can apply to braid projections is the Reidemeister III move. If we are given a braid projection \n                            and wish to move a strand over or under a crossing we are allowed to do this since a string does not need to be \n                            cut in the process. In general if you braid word contains $\\sigma_{i}\\sigma_{i+1}\\sigma_{i}$, for $1 \\leq i \\leq n-2$, \n                            then it can be replaced using the substitution $\\sigma_{i}\\sigma_{i+1}\\sigma_{i} = \\sigma_{i+1}\\sigma_{i}\\sigma_{i+1}$ \n                            (see <a href=\"#figure7\">Figure 7</a>, where the equivalent substitution \n                            $\\sigma_{i}^{-1}\\sigma_{i+1}^{-1}\\sigma_{i}^{-1} = \\sigma_{i+1}^{-1}\\sigma_{i}^{-1}\\sigma_{i+1}^{-1}$ is illustrated).\n                        </p>\n\n                        <figure>\n                            <img src=\"blog_files/braids/Figure7.png\" alt=\"Reidemeister III move on a braid.\" width=\"100%\" height=\"80%\" id=\"figure7\">\n                            <figcaption text-align=center><b>Figure 7:</b> An example of a Reidemeister III move being applied to a general braid.</figcaption>\n                        </figure>\n\n                        <p>\n                            The final move that we can apply to braid projections is not a Reidemeister move; instead, it is a switch. \n                            If our braid word contains $\\sigma_{i}\\sigma_{j}$, where $|i-j| > 1$, then we can switch the order of \n                            $\\sigma_{i}$ and $\\sigma_{j}$. So $\\sigma_{i}\\sigma_{j}$ becomes $\\sigma_{j}\\sigma_{i}$ (see <a href=\"#figure8\">Figure 8</a>).\n                        </p>\n\n                        <figure>\n                            <img src=\"blog_files/braids/Figure8.png\" alt=\"Reidemeister III move on a braid.\" width=\"100%\" height=\"80%\" id=\"figure8\">\n                            <figcaption text-align=center><b>Figure 8:</b> On the left we have $\\sigma_{1}^{-1}\\sigma_{4}^{-1}$. Since $|1 - 4| > 1$, we can switch \n                                the order to $\\sigma_{4}^{-1}\\sigma_{1}^{-1}$.</figcaption>\n                        </figure>\n\n                        <p>\n                            Using these three moves allows us to determine when two braids $b_{1}$ and $b_{2}$ are equivalent. \n                            This ideas leads to another very important theorem for working with braids: Markov's theorem.\n                        </p>\n\n                        <h4>Markov's Theorem</h4>\n\n                        <p>\n                            Markov's theorem <a href=\"#birman\">[2]</a> states:\n                        </p>\n\n                        <p>\n                            Given two braid words $\\beta_{n}\\in B_{n}$, $\\beta'_{m}\\in B_{m}$ with $n$ and $m$ strands respectively, \n                            their closures are equivalent links if and only if $\\beta'_{m}$ can be obtained from  $\\beta_{n}$ by applying \n                            a sequence of the following operations:\n                        </p>\n\n                        <ol>\n                            <li>conjugating $\\beta_{n}$ in $B_{n}$;</li>\n                            <li>replacing $\\beta_{n}$ by $\\beta_n\\sigma_{n}^{\\pm 1} \\in B_{n+1}$;</li>\n                            <li>the inverse of the previous operation (if $\\beta_{n}=\\beta_{n-1}\\sigma_{n}^{\\pm 1}$ with $\\beta_{n-1} \\in B_{n-1}$, replace $\\beta_{n}$ with $\\beta_{n-1}$).</li>\n                        </ol>\n                        \n                        <p>\n                            In Markov's theorem, we learn about two new moves that can be applied to braids to obtain different braids \n                            with equivalent closures. The first comes in part 1: conjugation. Conjugation is an operation applied to \n                            the braid word where the beginning of the word is multiplied by $\\sigma_{j}$, and the end of the word by \n                            $\\sigma_{j}^{-1}$, or vice-versa. In the closure we are only adding a Reidemeister II move, so we are not \n                            changing the resulting knot type (see <a href=\"figure9\">Figure 9</a>). \n                        </p>\n\n                        <figure>\n                            <img src=\"blog_files/braids/Figure9.png\" alt=\"Example of conjugation.\" width=\"100%\" height=\"80%\" id=\"figure9\">\n                            <figcaption text-align=center><b>Figure 9:</b> If we were to connect this braid into a knot and move the bottom crossing near \n                                the top crossing, we would see that it is simply a pair of crossings that can be removed by a Reidemeister II move.</figcaption>\n                        </figure>\n\n                        <p>\n                            The second move comes in part 2: stabilization. Stabilization involves adding a new strand and a single \n                            crossing to a braid as illustrated in <a href=\"figure10\">Figure 10</a>. This is performed by taking the braid word $w$ that corresponds \n                            to an $n$-string braid, adding a strand to make it an $(n+1)$-string braid, and then adding $\\sigma_{n}$ or \n                            $\\sigma_{n}^{-1}$ to the beginning or end of the word $w$. Destabilization is the opposite of stabilization: simply \n                            remove a string and a crossing from a braid as shown in <a href=\"figure11\">Figure 11</a> below.\n                        </p>\n\n                        <figure>\n                            <img src=\"blog_files/braids/Figure10.png\" alt=\"Stabilization.\" width=\"100%\" height=\"80%\" id=\"figure10\">\n                            <figcaption text-align=center><b>Figure 10:</b> Through adding a single strand and crossing $\\sigma^{\\pm 1}_{n}$ \n                                at the end of the braid representation, we obtain a different braid with equivalent closure.</figcaption>\n                        </figure>\n\n                        <figure>\n                            <img src=\"blog_files/braids/Figure11.png\" alt=\"Destabilization.\" width=\"100%\" height=\"80%\" id=\"figure11\">\n                            <figcaption text-align=center><b>Figure 11:</b> Through removing a single strand and crossing $\\sigma^{\\pm 1}_{n}$ \n                                at the end of the braid representation, we obtain a different braid with equivalent closure.</figcaption>\n                        </figure>\n\n\n                        <h4>Conclusion</h4>\n\n                        <p>\n                            In this blog post, we discussed the basics of braids and introduced Alexander's theorem, and Markov's Theorem. \n                            Alexander's theorem which states that every knot or link can be expressed as the closure of a braid,\n                            and Markov's theorem which states that two braids are equivalent if and only if their diagrams are similar through\n                            a sequence of conjugations, stabilizations, and destabilizations. This is a precursor for the next blog post\n                            which will be about slice surfaces.\n                        </p>\n\n                        <h4>Citations</h4>\n\n                        <ol>\n                            <span id=\"adams\">[1]</span> Collin C. Adams, <em>The Knot Book</em>. American Mathematical Society, 2004. ISBN: 978-0821836781.\n                        </ol>\n                        <ol\n                            <span id=\"birman\">[2]</span> Joan S Birman. <em>Braids, links, and mapping class groups</em>. 82. Princeton University Press, 1974.\n                        </ol>   \n\n                            <!-- Add more list items as needed -->\n                        </ol>\n\n                        <!-- <p>\n                            <a id=\"adams\"></a>Adams, C. C. (1994). The Knot Book: An Elementary Introduction to the Mathematical Theory of Knots. W. H. Freeman and Company. -->",
      "quote": "Braids, bouffants, and balayage. Everything a girl wants to hear",
      "quoteAuthor": "The Internet",
      "category": "Math & ML"
    },
    {
      "slug": "convex-linear-opt-cvxpy",
      "title": "Convex Linear Optimization Using CVXPY",
      "date": "2023-02-24",
      "description": "Convex linear optimization is a wonderful tool that allows businesses and people to find optimal solutions to their problems. This post gives a short tutorial on how to us the python convex optimization package CVXPY.",
      "content": "<p>CVXPY is a library in Python that contains a set of classes and functions designed for solving convex optimization problems. There are many different convex optimization problems we can solve using CVXPY, but today we are going to look at linear convex optimization problems.</p>\n                        \n                        <h4>Getting started</h4>\n                        <p>A linear convex optimization problem (sometimes called linear programming) is a linear constrained optimization problem. This means that the problem can be stated in a way similar to</p>\n                        <img src=\"blog_files/cvxpy_intro/convex_standard.png\">\n                        <figcaption>This is the standard form of a linear convex optimization problem.</figcaption>\n                        <p>CVXPY accepts $\\leq$, $\\geq$, and $=$ in its constraints as long as the equations satisfy the convexity requirements. In our CVXPY problems, CVXPY accepts NumPy arrays and SciPy sparse matrices as constraints, but the variable x must be a CVXPY Variable.</p>\n                        \n                        <h4>Simple Example</h4>\n                        <p>Let's do a simple example to see how this works. Consider the following problem:</p>\n                        <img src=\"blog_files/cvxpy_intro/convex_simple.png\">\n                        <p>How would we represent this using CVXPY? Here is the code to represent this problem. I will present the code and then break down each line.</p>\n                        <pre>\n                            <code class=\"language-python\">\n# Import necessary libraries\nimport cvxpy as cp\n\n# Initialize the variables\nx1, x2 = cp.Variable(nonneg=True), cp.Variable(nonneg=True)\n\n# Create objective function\nobjective = cp.Minimize(-4*x1 - 5*x2)\n\n# Initialize constraints (must be a list)\nconstraints = [x1 + 2*x2 <= 3,\n                2*x1 + x2 == 3]\n\n# Create our problem\nproblem = cp.Problem(objective, constraints)\n\n# Solve our problem\nproblem.solve()\n\n# Find out what the optimal values of x1 and x2 are, and what the answer\n    # our optimization problem is\nprint(problem.value)\nprint(x1.value, x2.value)\n                            </code>\n                        </pre>\n                        <p>There are more optimal ways to do the code above, and we will go over those potential improvements. Now, let's break down each line.</p>\n                        \n                        <p>First, we initialize the variables.</p>\n                        <pre>\n                            <code class=\"language-python\">\n# Initialize the variables\nx1, x2 = cp.Variable(nonneg=True), cp.Variable(nonneg=True)\n                            </code>\n                        </pre>\n\n                        <p>Here we create the CVXPY Variables <code>x1</code> and <code>x2</code>. These are the variables we use to solve our problem. From our optimization constraints, we learn that both <code>x1</code> and <code>x2</code> must be greater than or equal to 0. So, by adding <code>nonneg=True</code> into the <code>cp.Variable()</code> declaration, CVXPY knows that these variables must be greater than or equal to 0. You could omit this function argument and instead put <code>x1 >= 0, x2 >= 0 </code>in the constraints list. Either way works. If, in your optimization problem, the variables <code>x1</code> and <code>x2</code> can take on negative values, then omit <code>nonneg=True</code> and do not include anything in your constraints list.</p>\n                        <p>Next, we create our objective function.</p>\n                        <pre>\n                            <code class=\"language-python\">\n# Create objective function\nobjective = cp.Minimize(-4*x1 - 5*x2)\n                            </code>\n                        </pre>\n                        <p>Creating the objective function in CVXPY is simple. If you wish to minimize your objective function, simply do <code>cp.Minimize()</code> with the problem inside the function--using the CVXPY variables you have already declared. If your problem involves maximization, simply do <code>cp.Maximize()</code> with the problem inside the function\u2014again, using the CVXPY variables you have already declared.</p>\n                        <p>Moving on, we have our constraints.</p>\n                        <pre>\n                            <code class=\"language-python\">\n# Initialize constraints (must be a list)\nconstraints = [x1 + 2*x2 <= 3,\n                2*x1 + x2 == 3]\n                            </code>\n                        </pre>\n                        <p>The very first thing to mention is that our constraints must be in list form! CVXPY will not work otherwise. Even if you have only one constraint, that single constraint must be in a list. Otherwise, creating our constraints is simply encoding our constraints in Python!</p>\n                        <p>Now we create our problem.</p>\n                        <pre>\n                            <code class=\"language-python\">\n# Create our problem\nproblem = cp.Problem(objective, constraints)\n                            </code>\n                        </pre>\n                        <p>With this step, use <code>cp.Problem()</code> and put in the objective of the convex problem as the first argument, and the constraints as the second argument. Of course, if you desired, you could skip the steps of defining the objective function and constraints and do all of this inside <code>cp.Problem()</code>.</p>\n                        <p>Once we have our problem created, we solve our problem!</p>\n                        <pre>\n                            <code class=\"language-python\">\n# Solve our problem\nproblem.solve()\n                            </code>\n                        </pre>\n                        <p>I feel that this step is self-explanatory. Just type the above!</p>\n                        <p>That is it! If you find the run the code, you get:</p>\n                        <pre>\n                            <code class=\"language-python\">\nproblem.value -> -8.999999999850528\nx1.value -> 1.\nx2.value -> 1.\n                            </code>\n                        </pre>\n                        <p>And it is that easy! Now, let's do it with a different, more difficult example.</p>\n\n                        <h4>More Difficult Example</h4>\n                        <p>Consider the following network:</p>\n                        <img src=\"blog_files/cvxpy_intro/graph_network.png\">\n                        <p>If we represent the edges between the nodes and their associated weights, we have:</p>\n                        <pre>\n                            <code class=\"language-python\">\nFrom  To  Weight\nA     B     2\nA     D     5\nB     C     5\nB     D     2\nB     E     7\nB     F     9\nC     F     2\nD     E     4\nE     F     3\n                            </code>\n                        </pre>\n                        <p>Assume that the supply (or demand, depending on the sign) at the nodes is</p>\n                        <p>bA = 10, bB = 1, bC = -2, bD = -3, bE = 4, bF = -10</p>\n                        <p>and that the capacity of each edge is bounded by 6. Using this information, we can build a linear optimization problem whose solution we can find using CVXPY and will tell us the cheapest flow in this network with the above constraints. The first thing we need to do is build our linear optimization problem.</p>\n                        <p>We know that we want to find the most efficient way to distribute our supplies. There are 9 different routes to take (represented in the 9 different from-to pairs above), so we know that we will need 9 CVXPY variables (or, rather, one variable with 9 spots). If A is the matrix that represented the edges connecting different nodes, x is the vector representing the values we are trying to find, b represents the supply/demand of the nodes, and cost represents the weight of the edges in the from-to pairs above, we can structure our linear optimization problem as follows:</p>\n                        <img src=\"blog_files/cvxpy_intro/graph_problem.png\">\n                        <p>In Python (and using NumPy), we can represent these variables as:</p>\n                        <pre>\n                            <code class=\"language-python\">\nimport numpy as np\n\n# How much it costs to go around the edges of our network \n  # (the weight of each edge)\ncost = np.array([2, 5, 5, 2, 7, 9, 2, 4, 3])\n\n\n# Our matrix, representing the connections between the nodes.\n  # 1 for an edge going from the node, -1 for an edge going into the node.\n  # The rows represent the nodes, and the columns represent the connections\n    # associated with that node.\n  # So row 0 is node A, row 1 is node B, etc, and column 0 is the connection\n    # from A to B, column 1 is the connection from A to D, etc.\nA = np.array([[ 1,  1,  0,  0,  0,  0,  0,  0,  0],\n              [-1,  0,  1,  1,  1,  1,  0,  0,  0],\n              [ 0,  0, -1,  0,  0,  0,  1,  0,  0],\n              [ 0, -1,  0, -1,  0,  0,  0,  1,  0],\n              [ 0,  0,  0,  0, -1,  0,  0, -1,  1],\n              [ 0,  0,  0,  0,  0, -1, -1,  0, -1]])\n\n# Our b vector, which contains the information for bA to bF as seen above.\nb = np.array([10, 1, -2, -3, 4, -10])\n                            </code>\n                        </pre>\n                        <p>Now, in order to represent x, we will need to initialize a CVXPY variable.</p>\n                        <pre>\n                            <code class=\"language-python\">\n# Instead of initializing x1, x2, ..., x9, we can instead initialize\n    # x to be a vector with 9 entries.\nx = cp.Variable((9), nonneg=True)\n                            </code>\n                        </pre>\n\n                        <p>Now that we have all of our variables ready and declared, we need to create our objective function,</p>\n                        <pre>\n                            <code class=\"language-python\">\n# Similiar to NumPy, represent matrix multiplication in CVXPY with @\nobjective = cp.Minimize(cost.T@x)\n                            </code>\n                        </pre>\n                        <p>and our constraints</p>\n                        <pre>\n                            <code class=\"language-python\">\n# Remember: Constraints must be in list form!\n    # Additionally, since we said in the creation of the x-variable that\n        # it must be non-negative, the only constraint we need to include is that\n        # each value of x cannot be greater than 6.\nconstraints = [A@x == b,\n               x <= 6]\n                            </code>\n                        </pre>\n                        <p>With our variables, objective function, and constraints all defined, it is time to solve our convex linear optimization problem using CVXPY! Try to do it on your own before looking at the provided code.</p>\n                        <pre>\n                            <code class=\"language-python\">\nimport cvxpy as cp\nimport numpy as np\n\n# How much it costs to go around the edges of our network \n    # (the weight of each edge)\ncost = np.array([2, 5, 5, 2, 7, 9, 2, 4, 3])\n\n# Our matrix, representing the connections between the nodes.\n    # 1 for an edge going from the node, -1 for an edge going into the node.\n    # The rows represent the nodes, and the columns represent the connections\n    # associated with that node.\n    # So row 0 is node A, row 1 is node B, etc, and column 0 is the connection\n    # from A to B, column 1 is the connection from A to D, etc.\nA = np.array([[ 1,  1,  0,  0,  0,  0,  0,  0,  0],\n                [-1,  0,  1,  1,  1,  1,  0,  0,  0],\n                [ 0,  0, -1,  0,  0,  0,  1,  0,  0],\n                [ 0, -1,  0, -1,  0,  0,  0,  1,  0],\n                [ 0,  0,  0,  0, -1,  0,  0, -1,  1],\n                [ 0,  0,  0,  0,  0, -1, -1,  0, -1]])\n\n# Our b vector, which contains the information for bA to bF as seen above.\nb = np.array([10, 1, -2, -3, 4, -10])\n\n# Instead of initializing x1, x2, ..., x9, we can instead initialize\n    # x to be a vector with 9 entries.\nx = cp.Variable((9), nonneg=True)\n\n# Similiar to NumPy, represent matrix multiplication in CVXPY with @\nobjective = cp.Minimize(cost.T@x)\n\n# Remember: Constraints must be in list form!\n    # Additionally, since we said in the creation of the x-variable that\n    # it must be non-negative, the only constraint we need to include is that\n    # each value of x cannot be greater than 6.\nconstraints = [A@x == b,\n                x <= 6]\n\n# Create and solve our convex linear optimization problem.\nproblem = cp.Problem(objective, constraints)\nproblem.solve()\n\n# See what our solution is:\nprint(problem.value)\nprint(x.value)\n                            </code>\n                        </pre>\n                        <p>The solution is:</p>\n                        <pre>\n                            <code class=\"language-python\">\nproblem.value -> 98.00000002103977\nx.value       -> [5.99999999 4.00000001 6.         0.31411362 0.00000001 0.68588636 4.         1.31411363 5.31411364]\n\n# After rounding...\nx.value       -> [6, 4, 6, 0, 0, 1, 4, 1, 5]\n                            </code>\n                        </pre>\n                        \n                        <h4>Conclusion</h4>\n                        <p>And there you have it! CVXPY is a great library that makes solving convex optimization problems a breeze! You can solve more than linear optimization problems, so keep searching to learn more!</p>\n                        \n                        <h4>Additional Resources</h4>\n                        <p>If you are interested in some additional resources, I would recommend the book <a href=\"https://web.stanford.edu/~boyd/cvxbook/\">\"Convex Optimization\"</a> by <a href=\"https://web.stanford.edu/~boyd/\">Stephen Boyd</a> and <a href=\"http://www.seas.ucla.edu/~vandenbe/\">Lieven Vandenberghe</a>. It is a great, indepth look into the world of convex optimization, both linear and nonlinear.</p>\n                        <p>The book has lots of practice problems to try out, and even has a <a href=\"https://egrcc.github.io/docs/math/cvxbook-solutions.pdf\">solution manual</a>. (If you are looking for a free pdf copy, you can find it <a href=\"https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf\">here</a>.</p>",
      "quote": "...in fact, the great watershed in optimization isn't between linearity and nonlinearity, but convexity and nonconvexity.",
      "quoteAuthor": "R. Tyrrell Rockafellar",
      "category": "Math & ML"
    },
    {
      "slug": "decision-tress",
      "title": "Decision Trees for Classification",
      "date": "2024-01-10",
      "description": "",
      "content": "<p>In the vast world of machine learning, decision trees are like the Swiss Army knives - \n                            versatile and surprisingly straightforward. These nifty tools mimic the way humans make decisions, \n                            breaking down big problems into smaller, more manageable chunks.\n                            In this blog post, we break down how decision trees work for classification, how the Gini index plays a cruitial role in constructing\n                            decision trees, and we work through an example of building a decision tree using the Gini index.</p>\n\n                        <figure>\n                            <img src=\"blog_files/decision_trees/futuristic_decision_tree.png\" alt=\"Futuristic Decision Tree.\" width=\"90%\" height=\"90%\">\n                            <figcaption text-align=center>We will create a decision tree as cool as this one!</figcaption>\n                        </figure>\n                        \n                        <p></p>\n\n                        <h4>Classification Trees</h4>\n\n                        <p>\n                            At the heart of it, a classification tree is some classifier $f: \\mathscr{X}\\to\\mathscr{Y}$ that takes in some input $x\\in\\mathscr{X}$ \n                            and outputs a class $y\\in\\mathscr{Y}$. In order to arrive at $y$, the data is passed through a series of if-else decision nodes which, when visualized,\n                            look similar to the branches of a tree. Ultimately, when data is passed through the tree and goes through enough branches,\n                             it ends up at a leaf node, which is the class that is output.\n                        </p>\n\n                        <p>\n                            One of the benefits of decision trees is that they are pretty inexpensive to construct, and incredibly easy to interpret! Additionally, they work for\n                            categorical or numerical data, can handle missing data with easy, and can handle irrelevant features. However, they are prone to overfitting, and\n                            are not very robust. This is where random forests come in, but that is a topic for another blog post.\n                        </p>\n                        \n                        <h4>\n                            Building a Decision Tree - Theory\n                        </h4>\n\n                        <p>\n                            Constructing a decision tree that is optimal for a given dataset is a computationally expensive task (in fact, it is an NP-hard problem). However, the\n                            greedy algorithm we will discuss here is a good approximation of the optimal decision tree and actually works really well in practice.\n                        </p>\n\n                        <p>\n                            The process for using the greedy algorithm to build a decision tree is a recursive process. \n                            We start with the root node and use the Gini index to determine which feature to split on. Once that is done, do that for each branch until we reach a leaf node.\n                            Before we work through an example of this recursive process, let's first talk about the Gini index.\n                        </p>\n\n                        <a id=\"gini_index\"></a>\n\n                        <h4>Gini Index</h4>\n\n                        <p>\n                            In order to determine which feature to split on, we need to be able to measure how well a feature splits the data. This means we want to figure out \n                            how much each leaf constributes to the loss function. To do this, we will use the Gini index. The Gini index for approximating the entropy of each leaf is defined as follows:\n                        </p>\n\n                        $$ G_{\\mathbb{D}_{\\ell}} = 1 - \\sum_{c\\in\\mathscr{Y}}\\widehat{\\pi}_{\\ell,c}^2$$\n\n                        <p>\n                            Where $\\ell$ is a given tree leaf, $\\mathbb{D}_{\\ell}$ is the set of training points $(\\mathbf{x}_i, y_i)$ where $\\mathbf{x}_i$\n                            is assigned to leaf $\\ell$, $c\\in\\mathscr{Y}$ are the possible classes, and $\\widehat{\\pi}_{\\ell,c}$ is the proportion of \n                            training points in $\\mathbb{D}_{\\ell}$ that are class $c$, defined as \n                        </p>\n\n                        $$\\widehat{\\pi}_{\\ell,c} = \\frac{1}{N_{\\ell}}\\sum_{(\\mathbf{x}_i, y_i)\\in\\mathbb{D}_{\\ell}}\\mathbb{I}_{(y_i=c)}$$\n\n                        <p>\n                            Where $\\mathbb{I}$ is the indicator function.\n                        </p>\n\n                        <p>\n                            In the case of most classification trees, we only need to consider two classes at a time while looking at a given leaf\n                            $\\ell$: it is either class $c$ or it is not class $c$. Thus, calculating the total cost to the to our classification\n                            function $f_{\\ell}$ on leaf $\\ell$ is given by\n                        </p>\n\n                        $$\\text{cost}_{\\ell} = \\frac{N_+}{N}G_{\\mathbb{D}_{\\ell_+}} + \\frac{N_-}{N}G_{\\mathbb{D}_{\\ell_-}}$$\n\n                        <p>\n                            where $N_{+}$ is the number of points in the 'yes' node, $G_{\\mathbb{D}_{\\ell_+}}$ is the Gini index for the 'yes' node, $N_-$ is the number of points in the 'no' node,\n                             and $G_{\\mathbb{D}_{\\ell_-}}$ is the Gini index for the 'no' node.\n                        </p>\n                        <p>\n                            To see how this works, let's look at an example.\n                        </p>\n\n                        <h4>Building a Decision Tree - Practice</h4>\n\n                        <p>\n                            Let's say we have the following dataset,\n                        </p>\n\n                        <table>\n                            <tr>\n                                <th>Patient ID</th>\n                                <th>Chest Pain</th>\n                                <th>Male</th>\n                                <th>Smokes</th>\n                                <th>Exercises</th>\n                                <th>Heart Attack</th>\n                            </tr>\n                            <tr>\n                                <td>1</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                            </tr>\n                            <tr>\n                                <td>2</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                            </tr>\n                            <tr>\n                                <td>3</td>\n                                <td>No</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                            </tr>\n                            <tr>\n                                <td>4</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                                <td>No</td>\n                            </tr>\n                            <tr>\n                                <td>5</td>\n                                <td>Yes</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                            </tr>\n                            <tr>\n                                <td>6</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                                <td>No</td>\n                            </tr>\n                        </table>\n\n                        <p>\n                            and we want to predict whether or not a patient will have a heart attack. The first thing we need to do is determine what the first split of the tree will be, i.e., what will the root node be.\n                        </p>\n\n                        <p>\n                            Let's first consider the <code>Chest Pain</code> feature. We can split the data into two groups: those who have chest pain and those who do not.\n                        </p>\n\n                        <table>\n                            <tr>\n                                <th>Patient ID</th>\n                                <th>Chest Pain</th>\n                                <th>Male</th>\n                                <th>Smokes</th>\n                                <th>Exercises</th>\n                                <th>Heart Attack</th>\n                            </tr>\n                            <tr class=\"color1\">\n                                <td>1</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                            </tr>\n                            <tr class=\"color1\">\n                                <td>2</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                            </tr>\n                            <tr>\n                                <td>3</td>\n                                <td>No</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                            </tr>\n                            <tr>\n                                <td>4</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                                <td>No</td>\n                            </tr>\n                            <tr class=\"color1\">\n                                <td>5</td>\n                                <td>Yes</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                            </tr>\n                            <tr>\n                                <td>6</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                                <td>No</td>\n                            </tr>\n                        </table>\n\n                        <p>\n                            Above, I have highlighted the rows that have chest pain. We can see that there are $3$ patients who have chest pain, and $3$ patients who do not. Now, we build a mini tree \n                            for each of these groups to help keep track of who did and who did not have a heart attack based off chest pain.\n                        </p>\n\n                        <img src=\"blog_files/decision_trees/chest_pain_1.png\" alt=\"Decision Tree 1\" width=\"80%\" height=\"80%\">\n\n                        <p>\n                            Looking patient 1, we see that they do have chest pain, and did have a heart attack. We take that information, follow the branches of the tree, and end up with the following tree.\n                        </p>\n\n                        <img src=\"blog_files/decision_trees/chest_pain_2.png\" alt=\"Decision Tree 2\" width=\"80%\" height=\"80%\">\n                        \n                        <p>\n                            Looking at patient 2, we again see that they do have chest pain, and did have a heart attack. We take that information, follow the branches of the tree, and end up with the following tree.\n                        </p>\n\n                        <img src=\"blog_files/decision_trees/chest_pain_3.png\" alt=\"Decision Tree 3\" width=\"80%\" height=\"80%\">\n\n                        <p>\n                            Now, moving on to patient 3, we see that they did not have chest pain, and did have a heart attack.\n                            Taking that information, we follow the branches of the tree, and end up with the following tree.\n                        </p>\n\n                        <img src=\"blog_files/decision_trees/chest_pain_4.png\" alt=\"Decision Tree 4\" width=\"80%\" height=\"80%\">\n\n                        <p>\n                            Continuing this process all the way through for each patient, we end up with the following tree.\n                        </p>\n\n                        <img src=\"blog_files/decision_trees/chest_pain_final.png\" alt=\"Decision Tree 5\" width=\"80%\" height=\"80%\">\n\n                        <p>\n                            With these data points sorted out, we now calculate the Gini index for each leaf. For the leaf with chest pain, we have\n                        </p>\n\n                        $$ \\begin{aligned}G_{\\mathbb{D}_{\\ell}} = 1 - \\sum_{c\\in\\{\\text{Yes}, \\text{ No}\\}} \\widehat{\\pi}_{\\ell, c}^2 &= 1 - \\left[\\left(\\frac{3}{3}\\right)^2 + \\left(\\frac{0}{3}\\right)^2 \\right] \\\\\n                        &= 1 - [1^2] \\\\\n                        &= 0\\end{aligned}$$\n\n                        <p>\n                            The first time I saw this solution, I did not understand where these numbers came from, so let's break them down. First, we have\n                            $\\left(\\frac{3}{3}\\right)$. This is the proportion of patients who had chest pain and had a heart attack ($3$ people with chest pain had a heart attack so that is the numerator,\n                            $3$ people had chest pain so that is the denominator). Next, we have $\\left(\\frac{0}{3}\\right)$. This is the proportion of patients who had chest pain and did not have a heart attack\n                            ($0$ people with chest pain did not have a heart attack so that is the numerator, $3$ people had chest pain so that is the denominator). Thus, the Gini index for\n                            the leaf with chest pain is $0$.\n                        </p>\n\n                        <p>\n                            Now, let's calculate the Gini index for the leaf without chest pain. We have\n                        </p>\n\n                        $$ \\begin{aligned}G_{\\mathbb{D}_{\\ell}} = 1 - \\sum_{c\\in\\{\\text{Yes}, \\text{ No}\\}} \\widehat{\\pi}_{\\ell, c}^2 &= 1 - \\left[\\left(\\frac{1}{3}\\right)^2 + \\left(\\frac{2}{3}\\right)^2\\right] \\\\\n                         &= 1 - \\left[\\frac{1}{9} + \\frac{4}{9} \\right] \\\\\n                         &= 1 - \\left[ \\frac{5}{9} \\right] \\\\\n                         &= \\frac{4}{9}\\end{aligned}$$\n\n                        <p>\n                            Thus, the Gini index for the leaf without chest pain is $\\frac{4}{9}$.\n                        </p>\n\n                        <p>\n                            Now that we have the Gini index for each leaf, we can now calculate the total cost to our classification for this split. \n                        </p>\n\n                        $$\\begin{aligned}\\text{cost}_{\\ell} = \\frac{N_+}{N}G_{\\mathbb{D}_{\\ell_+}} + \\frac{N_-}{N}G_{\\mathbb{D}_{\\ell_-}} &= \\left(\\frac{3}{6}\\right)(0) + \\left(\\frac{3}{6}\\right)\\left(\\frac{4}{9}\\right) \\\\\n                        &= (0) + \\left(\\frac{2}{9}\\right) \\\\\n                        &= \\frac{2}{9} \\end{aligned}$$\n\n                        <p>\n                            Thus, we have that the total cost to split our tree on the <code>Chest Pain</code> feature is $\\frac{2}{9}$.\n                        </p>\n\n                        <p>\n                            We now repeat this process for each feature, and choose the feature that has the lowest cost. Consider next the <code>Male</code> feature. Looking at our table we have\n                        </p>\n\n                        <table>\n                            <tr>\n                                <th>Patient ID</th>\n                                <th>Chest Pain</th>\n                                <th>Male</th>\n                                <th>Smokes</th>\n                                <th>Exercises</th>\n                                <th>Heart Attack</th>\n                            </tr>\n                            <tr class=\"color1\">\n                                <td>1</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                            </tr>\n                            <tr class=\"color1\">\n                                <td>2</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                            </tr>\n                            <tr>\n                                <td>3</td>\n                                <td>No</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                            </tr>\n                            <tr class=\"color1\">\n                                <td>4</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                                <td>No</td>\n                            </tr>\n                            <tr>\n                                <td>5</td>\n                                <td>Yes</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                            </tr>\n                            <tr class=\"color1\">\n                                <td>6</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                                <td>No</td>\n                            </tr>\n                        </table>\n\n                        <p>\n                         Building our tree for this feature (filling in all the values), we get\n                        </p>\n\n                        <img src=\"blog_files/decision_trees/male_final.png\" alt=\"Decision Tree 6\" width=\"80%\" height=\"80%\">\n\n                        <p>\n                            Calculating the Gini index for each leaf (using the exact same process as above), we get the Gini index for male is $\\frac{1}{2}$, and\n                            the Gini index for female is $0$.\n                        </p>\n\n                        <p>\n                            Calculating the total cost to our classification for this split, we get (again, using the exact same process as above) $\\frac{1}{3}$. Thus, our total cost to split\n                            our tree on the <code>Male</code> feature is $\\frac{1}{3}$.\n                        </p>\n\n                        <p>\n                            Next, we consider the <code>Smokes</code> feature. Our tables looks like:\n                        </p>\n\n                        <table>\n                            <tr>\n                                <th>Patient ID</th>\n                                <th>Chest Pain</th>\n                                <th>Male</th>\n                                <th>Smokes</th>\n                                <th>Exercises</th>\n                                <th>Heart Attack</th>\n                            </tr>\n                            <tr>\n                                <td>1</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                            </tr>\n                            <tr class=\"color1\">\n                                <td>2</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                            </tr>\n                            <tr class=\"color1\">\n                                <td>3</td>\n                                <td>No</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                            </tr>\n                            <tr>\n                                <td>4</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                                <td>No</td>\n                            </tr>\n                            <tr class=\"color1\">\n                                <td>5</td>\n                                <td>Yes</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                            </tr>\n                            <tr class=\"color1\">\n                                <td>6</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                                <td>No</td>\n                            </tr>\n                        </table>\n                        \n                        <p>\n                            Building our tree for this feature (filling in all the values), we get\n                        </p>\n\n                        <img src=\"blog_files/decision_trees/smokes_final.png\" alt=\"Decision Tree 7\" width=\"80%\" height=\"80%\">\n\n                        <p>\n                            Calculating the Gini index for each leaf we get the Gini index for smokes is $\\frac{3}{8}$, and the Gini index for does not smoke is $\\frac{1}{2}$. Calculating\n                            the total cost to our classification for this split, we get $\\frac{5}{12}$. Thus, our total cost to split our tree on the <code>Smokes</code> feature is $\\frac{5}{12}$.\n                        </p>\n\n                        <p>\n                            Finally, we consider the <code>Exercises</code> feature. Our tables looks like\n                        </p>\n\n                        <table>\n                            <tr>\n                                <th>Patient ID</th>\n                                <th>Chest Pain</th>\n                                <th>Male</th>\n                                <th>Smokes</th>\n                                <th>Exercises</th>\n                                <th>Heart Attack</th>\n                            </tr>\n                            <tr class=\"color1\">\n                                <td>1</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                            </tr>\n                            <tr>\n                                <td>2</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                            </tr>\n                            <tr>\n                                <td>3</td>\n                                <td>No</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                            </tr>\n                            <tr class=\"color1\">\n                                <td>4</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                                <td>No</td>\n                            </tr>\n                            <tr class=\"color1\">\n                                <td>5</td>\n                                <td>Yes</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                            </tr>\n                            <tr class=\"color1\">\n                                <td>6</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                                <td>No</td>\n                            </tr>\n                        </table>\n                        \n                        <p>\n                            Building our tree for this feature (filling in all the values), we get\n                        </p>\n\n                        <img src=\"blog_files/decision_trees/exercises_final.png\" alt=\"Decision Tree 8\" width=\"80%\" height=\"80%\">\n\n                        <p>\n                            Calculating the Gini index for each leaf we get the Gini index for exercises is $\\frac{1}{2}$, and the Gini index for does not exercise is $0$. Calculating\n                            the total cost to our classification for this split, we get $\\frac{1}{3}$. Thus, our total cost to split our tree on the <code>Exercises</code> feature is $\\frac{1}{3}$.\n                        </p>\n\n                        <p>\n                            Comparing our total costs for each feature, we see that the <code>Chest Pain</code> feature has the lowest cost, so we split our tree on that feature. Thus, our root node is the <code>Chest Pain</code> feature.\n                        </p>\n\n                        <p>\n                            Now, we repeat this process for each branch of the tree. Luckily for us, in this case, the split on 'yes' for <code>Chest Pain</code> is a <em>pure</em> split. This means that all the patients who had chest pain had a heart attack. So, if a data point ends up\n                            in this leaf, we will classify the person as having a heart attack. Thus, we do not need to split that branch any further.\n                        </p>\n\n                        <p>\n                            This means that the only branch we need to split is the 'no' branch. Our tree now looks like this:\n                        </p>\n\n                        <img src=\"blog_files/decision_trees/first_split_tree.png\" alt=\"Decision Tree 9\" width=\"80%\" height=\"80%\">\n\n                        <p>\n                            We now begin the same process as splitting the root node. We consider each feature, and calculate the total cost to our classification for each feature.\n                        </p>\n\n                        <p>\n                            We first consider <code>Male</code>. Our table looks like\n                        </p>\n\n                        <table>\n                            <tr>\n                                <th>Patient ID</th>\n                                <th>Chest Pain</th>\n                                <th>Male</th>\n                                <th>Smokes</th>\n                                <th>Exercises</th>\n                                <th>Heart Attack</th>\n                            </tr>\n                            <tr>\n                                <td>1</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                            </tr>\n                            <tr>\n                                <td>2</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                            </tr>\n                            <tr class=\"color1\">\n                                <td>3</td>\n                                <td>No</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                            </tr>\n                            <tr class=\"color2\">\n                                <td>4</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                                <td>No</td>\n                            </tr>\n                            <tr>\n                                <td>5</td>\n                                <td>Yes</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                            </tr>\n                            <tr class=\"color2\">\n                                <td>6</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                                <td>No</td>\n                            </tr>\n                        </table>\n\n                        <p>\n                            Where the green rows are people who do not have chest pain and are male, and the yellow rows are people who do not have chest pain and are female. Looking at our tree, we have\n                        </p>\n\n                        <img src=\"blog_files/decision_trees/chest_male_final.png\" alt=\"Decision Tree 9\" width=\"80%\" height=\"80%\">\n\n                        <p>\n                            We see that both of these leafs are pure splits. A pure split gives a Gini index of 0, and having both leaves have Gini index of $0$ gives a\n                            total cost to our classification of $0$. This is wonderful news and is an ideal split. While we would typically stop here, we will continue on for the sake of the\n                            tutorial.\n                        </p>\n\n                        <p>\n                            Continuing on, we now look at the <code>Smokes</code> feature. Looking at our table, we see\n                        </p>\n\n                        <table>\n                            <tr>\n                                <th>Patient ID</th>\n                                <th>Chest Pain</th>\n                                <th>Male</th>\n                                <th>Smokes</th>\n                                <th>Exercises</th>\n                                <th>Heart Attack</th>\n                            </tr>\n                            <tr>\n                                <td>1</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                            </tr>\n                            <tr>\n                                <td>2</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                            </tr>\n                            <tr class=\"color2\">\n                                <td>3</td>\n                                <td>No</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                            </tr>\n                            <tr class=\"color1\">\n                                <td>4</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                                <td>No</td>\n                            </tr>\n                            <tr>\n                                <td>5</td>\n                                <td>Yes</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                            </tr>\n                            <tr class=\"color2\">\n                                <td>6</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                                <td>No</td>\n                            </tr>\n                        </table>\n\n                        <p>\n                            Where the green rows are people who do not have chest pain and do smoke, and the yellow rows are people who do not have chest pain and do not smoke. \n                            Looking at our tree, we have\n                        </p>\n\n                        <img src=\"blog_files/decision_trees/chest_smokes_final.png\" alt=\"Decision Tree 9\" width=\"80%\" height=\"80%\">\n\n                        <p>\n                            Calculating the Gini index for each leaf we get the Gini index for does not have chest pain and does smoke is $\\frac{1}{2}$, and the Gini index for does not have chest pain\n                            and does not smoke is $0$. Calculating the total cost to our classification for this split, we get $\\frac{1}{3}$. \n                            Thus, our total cost to split our tree on the <code>Smokes</code> feature is $\\frac{1}{6}$.\n                        </p>\n\n                        <p>\n                            Finally, looking <code>Exercises</code> feature, our table is\n                        </p>\n\n                        <table>\n                            <tr>\n                                <th>Patient ID</th>\n                                <th>Chest Pain</th>\n                                <th>Male</th>\n                                <th>Smokes</th>\n                                <th>Exercises</th>\n                                <th>Heart Attack</th>\n                            </tr>\n                            <tr>\n                                <td>1</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                            </tr>\n                            <tr>\n                                <td>2</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                            </tr>\n                            <tr class=\"color1\">\n                                <td>3</td>\n                                <td>No</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                            </tr>\n                            <tr class=\"color2\">\n                                <td>4</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                                <td>No</td>\n                            </tr>\n                            <tr>\n                                <td>5</td>\n                                <td>Yes</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                            </tr>\n                            <tr class=\"color2\">\n                                <td>6</td>\n                                <td>No</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                                <td>Yes</td>\n                                <td>No</td>\n                            </tr>\n                        </table>\n\n                        <p>\n                            Where the green rows are people who do not have chest pain and do exercise, and the yellow rows are people who do not have chest pain and do not exercise. \n                            Looking at our tree, we have\n                        </p>\n\n                        <img src=\"blog_files/decision_trees/chest_exercises_final.png\" alt=\"Decision Tree 9\" width=\"80%\" height=\"80%\">\n\n                        <p>\n                            Parallel to the split made by the <code>Male</code> feature, this split is completely pure, again giving us a total cost to our classification of $0$. This tells us that making a\n                            split on <code>Male</code> or <code>Exercises</code> will yield the same results. Without loss of generality, we decide to split on <code>Exercises</code>. Thus, our final\n                            decision tree is\n                        </p>\n\n                        <img src=\"blog_files/decision_trees/final_tree.png\" alt=\"Final Classification Tree\" width=\"80%\" height=\"80%\">\n\n                        <p>\n                            Thus, if we are given a new piece of data to classify, say\n                        </p>\n\n                        <table>\n                            <tr>\n                                <th>Patient ID</th>\n                                <th>Chest Pain</th>\n                                <th>Male</th>\n                                <th>Smokes</th>\n                                <th>Exercises</th>\n                                <th>Heart Attack</th>\n                            </tr>\n                            <tr>\n                                <td>7</td>\n                                <td>No</td>\n                                <td>No</td>\n                                <td>No</td>\n                                <td>No</td>\n                                <td>?</td>\n                            </tr>\n                        </table>\n\n                        <p>\n                            We can run this individual through our decision tree\n                        </p>\n\n                        <img src=\"blog_files/decision_trees/new_classification.png\" alt=\"Final Classification Tree\" width=\"80%\" height=\"80%\">\n\n                        <p>\n                            And would classify them as having a heart attack!\n                        </p>\n\n                        <h4>Avoiding Overfitting in Decision Trees</h4>\n\n                        <p>\n                            One big thing to point out is that decision trees can <b>easily</b> overfit on the data. In our toy example, the maximum number of samples we had per\n                            leaf is $3$, and one leaf had only $1$ sample in it. This is a dangerous way to classify because that $1$ sample could very easily be an outlier\n                            that would completely throw off future predictions. To avoid overfitting in decision trees, there are a few things we can do.\n                        </p>\n\n                        <p>\n                            One of the first things we can do is set a minimum number of samples per leaf. This means that in order for a split to happen, there must be at least $n$ samples\n                            present. This enables our decision tree to be a little more robust against outliers and avoid overfitting on a rabbit-hole-like part of our data. In our case, if\n                            we set the minimum number of samples per leaf to be $3$, we would have stopped at the <code>Chest Pain</code> split.\n                        </p>\n\n                        <p>\n                            Another way to avoid overfitting is setting a maximum depth of our decision tree. In our toy example, we arrived at pure splits pretty early, but if our data has\n                            more data points and more features, that is not always the case. Allowing the decision tree to continue splitting for as long as it wants can lead to severe overfitting.\n                            Thus, setting a maximum depth will overcome this.\n                        </p>\n\n                        <p>\n                            There are additional ways to avoid overfitting in our decision trees, but we stop here for now.\n                        </p>\n                        \n                        <h4>Quick Example in Python</h4>\n\n                        <p>\n                            While knowing how to build decision trees by hand is nice, you will never actually do that in practice. We will now go over a quick tutorial of building a classification\n                            decision tree using <code>sklearn</code>. The data we will be using to fit this tree is the data we used in this example. We will then plot the tree (visualize its branches\n                            and leaves) and see that we build the exact same tree when we did it by hand!\n                        </p>\n\n                        <p>\n                            Firstly, here are our necessary imports.\n                        </p>\n\n                        <pre>\n<code class=\"language-python\"># Necessary Imports\nimport pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree</code></pre>\n\n                        <p>\n                            Now that we have everything imported, we will want to create our data frame.\n                        </p>\n\n                        <pre>\n<code class=\"language-python\"># Load in our data\ndf = pd.DataFrame({\n    'Patient ID': [1, 2, 3, 4, 5, 6],\n    \"Chest Pain\": [True, True, False, False, True, False],\n    'Male': [True, True, False, True, False, True],\n    'Exercises': [True, False, False, True, True, True],\n    'Heart Attack': [True, True, True, False, True, False]  \n})</code></pre>\n\n                        <p>\n                            Once we have our data loaded in, we now need to create our <code>DecisionTreeClassifier</code>, specifying <code>criterion='gini'</code>\n                            so we insure that this tree will be built the same way we built our tree.\n                        </p>\n\n<pre><code class=\"language-python\"># Create our decision tree model\ndtc = DecisionTreeClassifier(criterion='gini')</code></pre>\n\n                        <p>\n                            Now that this is done, we need to create our <code>X</code> and <code>y</code> dataframes. This is essentially for us to fit our model.\n                        </p>\n\n<pre><code class=\"language-python\"># Separate our data into X and y\nX = df.drop(columns=['Patient ID', 'Heart Attack'])\ny = df['Heart Attack']</code></pre>\n\n                        <p>\n                            With our <code>X</code> and <code>y</code> dataframes created, we can now fit our <code>DecisionTreeClassifier</code>.\n                        </p>\n\n<pre><code class=\"language-python\"># Fit our model\ndtc.fit(X, y)</code></pre>\n\n                        <p>\n                            Our tree is now fit! (Remarkably faster than it took us to 'fit' our tree.)\n                        </p>\n\n                        <p>\n                            With our newly fit tree, we can now use <code>sklearn</code>'s nifty function <code>plot_tree</code> to see what our decision tree looks like!\n                            Running it is quite simple.\n                        </p>\n\n<pre><code class=\"language-python\"># Plot our tree to see what it looks like.\nplot_tree(dtc, feature_names=['Chest Pain', 'Male', 'Exercises'])</code></pre>\n\n                        <p>\n                            This produces this tree!\n                        </p>\n\n                        <img src=\"blog_files/decision_trees/decision_tree_image.png\" alt=\"Final Classification Tree\" width=\"80%\" height=\"80%\">\n\n                        <p>\n                            As we can see, the columns it split on the same features as our tree, and the Gini index for each split is the same as what we calculated! How neat!\n                        </p>\n\n                        <h4>Conclusion</h4>\n                        <p>\n                            In this article, we explored how to build decision trees for classification. We discussed the Gini index, and even created our own decision tree with a toy example!\n                            I hope that through this article, you now understand how to build your own decision trees by hand. Also, I hope you now understand how you might code them up!\n                            If you want to see the code used in this article, you can find it\n                            on my <a href=\"https://github.com/dylanskinner65/dylanskinner65.github.io/blob/main/blog/blog_files/decision_trees/decision_trees.ipynb\" target=\"_blank\" rel=\"noopener noreferrer\">Github</a>.\n                        </p>",
      "quote": "This oak tree and me, we're made of the same stuff.",
      "quoteAuthor": "Carl Sagan",
      "category": "Math & ML"
    },
    {
      "slug": "feature-selection-l1-aic",
      "title": "Feature Selection Using $L^1$ Regularization and Stepwise Feature Removal",
      "date": "2023-12-27",
      "description": "",
      "content": "<p>The goal of machine learning is to find the best possible model for a specific problem. Typically, finding the best\n                            possible model for a specific problem ends up being nothing more than trying a bunch of different models, fiddling about\n                            with different hyperparameters, and seeing which performs the best. One additional tool that a data scientist can use\n                            to boost their model success is feature selection. In this article, we will explore two different ways to performs feature\n                            selection, namely using $L^1$ regularization and stepwise feature removal. We will also discuss how $L^1$ regularization\n                            often out performs stepwise feature removal.</p>\n\n                        <p>\n                            Note: In this article we will be using $L^1$ regularization for linear regression and the Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC)\n                             for stepwise feature removal. If these terms are unfamiliar to you, then I suggest you read my articles on linear regression and $L^1$ regularization found\n                            <a href=\"https://dylanskinner65.github.io/blog/linear_regression.html\">here</a>, and my article on AIC and BIC found <a href=\"https://dylanskinner65.github.io/blog/aic_bic.html\">here</a>.\n                        </p>\n                        \n                        <h4>What Is Feature Selection?</h4>\n\n                        <p>\n                            Before talking about <em>how</em> to perform feature selection, I feel it is important to talk about <em>what</em>\n                            feature selection is. Feature selection is the process of selecting a subset of features from a larger set of features\n                            to use in a model, noting that each of these feature subsets will give you a different model. We can use various measures of\n                            model performance to determine which feature subset is best.\n                        </p>\n                        <p>\n                            If the number of features in a dataset is small, we can simply try all possible subsets of features and choose the one that\n                            gives us the best model performance. However, if the number of features is large, then this becomes computationally infeasible since\n                            the number of feature subsets is $2^F$ where $F$ is the number of features.\n                        </p>\n                        <p>\n                            The ultimate goal of feature selection is to reduce the number of features used in a model (i.e., reduce model complexity),\n                            while still maintaining a high level of model performance. This is important because it can help us avoid overfitting, and can help us\n                            build models that are more interpretable, if that is important to you.\n                        </p>\n\n                        <h4>Example Data For This Tutorial</h4>\n\n                        <p>Before getting into the data, make sure you have the following libraries installed and loaded.</p>\n\n                        <pre>\n<code class=\"language-python\">import pandas as pd\nimport numpy as np\nfrom statsmodels.api import sm</code></pre>\n\n                        <p>\n                            In this tutorial, we will be using the <a href=\"https://archive.ics.uci.edu/dataset/9/auto+mpg\" target=\"_blank\" rel=\"noopener noreferrer\">Auto MPG</a> from the UC Irvine\n                            machine learning repository. This dataset contains information about different cars, such as the number of cylinders, the weight, the horsepower, and we will use it to predict\n                            the miles per gallon (mpg) of the car.\n                        </p>\n\n                        <p>\n                            To load in this data in Python we first install the <code>ucimlrepo</code> library. To do this, first make sure it is installed by running\n                        </p>\n\n                        <pre>\n<code class=\"language-python\">!pip install ucimlrepo</code></pre>\n\n                        <p>\n                            Then, we can load in the data by running\n                        </p>\n\n                        <pre>\n<code class=\"language-python\"># Import the dataset\nfrom ucimlrepo import fetch_ucirepo \n  \n# fetch dataset \nauto_mpg = fetch_ucirepo(id=9) \n  \n# data (as pandas dataframes) \nX = auto_mpg.data.features \ny = auto_mpg.data.targets</code></pre>\n\n                        <p>\n                            By running <code>auto_mpg.variables</code>, we can get a quick look at the variables in this dataset.\n                        </p>\n\n                        <pre>\n<code class=\"language-python\">auto_mpg.variables\n>>>        name     role         type   missing_values  \n0  displacement  Feature   Continuous               no\n1           mpg   Target   Continuous               no\n2     cylinders  Feature      Integer               no\n3    horsepower  Feature   Continuous              yes\n4        weight  Feature   Continuous               no\n5  acceleration  Feature   Continuous               no\n6    model_year  Feature      Integer               no\n7        origin  Feature      Integer               no\n8      car_name       ID  Categorical               no</code></pre>\n\n                        <p>\n                            Note: Running <code>auto_mpg.variables</code> will not give this exact output. I removed some of the columns I deemed unnecessary for the sake of brevity.\n                        </p>\n\n                        <p>\n                            (All of this is taken from the <a href=\"https://archive.ics.uci.edu/dataset/9/auto+mpg\" target=\"_blank\" rel=\"noopener noreferrer\">Import In Python</a> button found on UC Irvine's page\n                            for the auto-mpg dataset.)\n                        </p>\n\n                        <p>\n                            Now that we have the data loaded in, we also need to perform a little bit of feature engineering. Firstly, we notice that the <code>origin</code>\n                            column is an integer, but when you look at the data, it is actually a categorical variable with integer values, specifically taking on the values\n                            of $1$, $2$, and $3$, where $1$ is the US, $2$ is Europe, and $3$ is Asia. It is important that we one-hot encode this to avoid our model taking\n                             the numerical proxy of the origin as anything other that nominal data. Here is the code that will do that.\n                        </p>\n\n                        <pre>\n<code class=\"language-python\"># Change the values of the origin column to represent the country of origin as strings: 'US', 'Europe', and 'Asia'\nX['origin'] = X['origin'].replace({1: 'US', 2: 'Europe', 3: 'Asia'})\n\n# One-hot encode the origin column\nX = pd.get_dummies(X, columns=['origin'], drop_first=True)</code></pre>\n                        \n                        <p>\n                            Remember to use <code>drop_first=True</code> to avoid creating any sort of linear dependence in the data.\n                        </p>\n\n                        <p>\n                            We also see that the <code>horsepower</code> column is missing some values. Running <code>X['horsepower'].isna().sum()</code>, we get that there are\n                            $6$ missing values. Since this is a small number of missing values, we can simply drop these rows. Before doing that, we need to get the rows that have the missing values, so we can\n                            make sure to drop them from $y$. We can do this by running <code>nan_indices = X[X.horsepower.isna()].index</code>. Once that is done, we can drop those rows from $X$ and $y$ by running\n                            <code>X = X.dropna()</code>, and <code>y = y.drop(nan_indices)</code>.\n                        </p>\n\n                        <p>\n                            The model we will be using for this article will be a simple linear regression model from <code>statsmodels</code>. Recall that when using\n                            <code>statsmodels</code> for linear regression, we need to add a constant column manually. We can do this by running <code>X = sm.add_constant(X)</code>.\n                        </p>\n\n                        <p>\n                            Now that any necessary feature engineering is complete, we will now begin feature selection.\n                        </p>\n\n                        <h4>Stepwise Feature Removal</h4>\n\n                        <p>\n                            Stepwise feature removal is a simple, yet effective, way to perform feature selection. The idea is to start with all of the features, \n                            and then remove one feature at a time, each time fitting a model and evaluating the model performance. We then choose the model that\n                            gives us the best performance. To do this, follow these simple steps:\n                        </p>\n\n                        <ol>\n                            <li>Fit a model using all of the features.</li>\n                            <li>Compute the AIC and/or BIC.</li>\n                            <li>Identify a feature with a high p-value and a $95\\%$ confidence interval that contains $0$.</li>\n                            <li>Drop the feature.</li>\n                            <li>Refit the model without the feature and recompute the AIC and/or BIC.</li>\n                            <li>If the AIC and/or BIC has dropped significantly, permanently drop the feature. Otherwise, add the feature back in and go back to step 3.</li>\n                            <li>Repeat steps $3-6$ until the AIC/BIC stops improving or there are no other features with a large p-value.</li>\n                        </ol>\n\n                        <p>\n                            With this process now defined, let's see how we can implement it in Python. First, we need to define a function that will fit a model by doing\n                        </p>\n\n                        <pre>\n<code class=\"language-python\"># Fit the model\nmodel = sm.OLS(y, X).fit()</code></pre>\n\n                        <p>\n                            Once our model is fitted, we can use the <code>.summary()</code> method to get all the information we need to perform steps $3-6$ of stepwise feature removal.\n                            Running <code>model.summary()</code> gives us the following output.\n                        </p>\n                        \n                        <img src=\"blog_files/feature_selection_l1_aic/model_summary_result_1.png\" alt=\"Initial Result of model.summary()\" width=\"100%\" height=\"100%\">\n\n                        <p>\n                            First, note the AIC and BIC. We see that they have values of $2059$ and $2095$, respectively. These are the values we will use to compare models.\n                            Next, looking at the <code>P>|t|</code> column, we see that the <code>origin_Europe</code> feature has a p-value of $0.694$ and contains $0$\n                            in its $95\\%$ confidence interval. This means that we can drop this feature. To do this, we simply run <code>X1 = X.drop('origin_Europe', axis=1)</code>.\n                            (Note, we set this operation equal to <code>X1</code> so we do not overwrite the original <code>X</code> dataframe.)\n                        </p>\n\n                        <p>\n                            Once we have dropped the feature, we can refit the model and call <code>model.summmary()</code> again (remember to use <code>X1</code> instead of <code>X</code>).\n                            This yields\n                        </p>\n\n                        <img src=\"blog_files/feature_selection_l1_aic/model_summary_result_2.png\" alt=\"Second Result of model.summary()\" width=\"100%\" height=\"100%\">\n\n                        <p>\n                            We note that the AIC and BIC have decreased from $2059$ and $2095$ to $2057$ and $2089$, respectively. \n                            This means that we have improved our model by dropping the <code>origin_Europe</code> feature, so we will not add it back in.\n                        </p>\n\n                        <p>\n                            We now select the next feature to drop. Looking at the <code>P>|t|</code> column, we see that the <code>acceleration</code> feature has a p-value of $0.421$ and contains $0$\n                            in its $95\\%$ confidence interval. This means that we can drop this feature. To do this, we simply run <code>X2 = X1.drop('acceleration', axis=1)</code>. Refitting and calling\n                            <code>model.summary()</code> again yields\n                        </p>\n\n                        <img src=\"blog_files/feature_selection_l1_aic/model_summary_result_3.png\" alt=\"Third Result of model.summary()\" width=\"100%\" height=\"100%\">\n\n                        <p>\n                            Since the AIC and BIC have decreased from $2057$ and $2089$ to $2056$ and $2084$, respectively, we have improved our model by dropping the <code>acceleration</code> feature, so we will not add it back in.\n                            However, we notice that the decrease in AIC and BIC is less than the previous decrease. This means that we are not improving our model as much as we were before. This is a sign that we are nearing the end of our feature selection.\n                        </p>\n\n                        <p>\n                            Finally, we select the next feature to drop. Looking at the <code>P>|t|</code> column, we see that the <code>cylinders</code> feature has a p-value of $0.122$ and contains $0$\n                            in its $95\\%$ confidence interval. This means that we can drop this feature. To do this, we simply run <code>X3 = X2.drop('cylinders', axis=1)</code>. Refitting and calling\n                            <code>model.summary()</code> again yields\n                        </p>\n\n                        <img src=\"blog_files/feature_selection_l1_aic/model_summary_result_4.png\" alt=\"Fourth Result of model.summary()\" width=\"100%\" height=\"100%\">\n\n                        <p>\n                            We can see that the AIC has not changed, and the BIC has decreased from $2084$ to $2080$. This is not a very significant change, especially when\n                            you remember that BIC penalizes model complexity more than AIC, so it makes sense that removing a feature (i.e., reducing model complexity) would\n                            have a larger effect on BIC than AIC. This tells us that removing the <code>cylinders</code> feature is up to us. If we want a simpler model, then we can remove it. \n                            If we want a more complex model, then we can keep it. In our case we will keep it.\n                        </p>\n\n                        <p>\n                            Now that we have gone through the process of stepwise feature removal, we can calculate the mean-squared error (MSE) of our final model and compare it with\n                            the MSE of the inital model. To do this, we first create a <code>train_test_split</code> of our data and then fit the initial model and the final model.\n                        </p>\n\n                        <pre>\n<code class=\"language-python\"># Perform train test split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=334)\ny_test = y_test.values\n\n# Train the first model, and get the MSE on the test set\nmodel = sm.OLS(y_train, X_train).fit()\ny_pred = model.predict(X_test).values\nmse = np.mean((y_pred - y_test)**2)\nprint(f'MSE when using all features:         {mse}')\n\n# Now, remove the features we decided weren't useful through stepwise feature removal, and get the MSE on the test set\nX_train = X_train.drop(['origin_Europe', 'acceleration', 'cylinders'], axis=1)\nX_test = X_test.drop(['origin_Europe', 'acceleration', 'cylinders'], axis=1)\nmodel = sm.OLS(y_train, X_train).fit()\ny_pred = model.predict(X_test).values\nmse = np.mean((y_pred - y_test)**2)\nprint(f'MSE when using only useful features: {mse}')</code></pre>\n\n                        <p>\n                            The output of this code is\n                        </p>\n                        <pre>\n<code class=\"language-python\">>>> MSE when using all features:         99.67098477764772\n>>> MSE when using only useful features: 98.94744747911336</code></pre>\n\n                        <p>\n                            Thus, we can see that our final model performs slightly better than our initial model. This is a good sign that our feature selection was successful!\n                        </p>\n\n                        <h4>$L^1$ Regularization Feature Selection</h4>\n\n                        <p>\n                            Now that we have seen how to perform feature selection using stepwise feature removal, let's see how to perform feature selection using $L^1$ regularization.\n                            To do this, we will be using the <code>LassoLarsIC</code> model from <code>sklearn.linear_model</code>, so make sure you import that.\n                        </p>\n\n                        <pre>\n<code class=\"language-python\">from sklearn.linear_model import LassoLarsIC</code></pre>\n\n                        <p>\n                            The <code>LassoLarsIC</code> function is a powerful tool for performing feature selection using $L^1$ regularization. \n                            Recall that $L^1$ regularization introduces a penalty term based on the absolute values of the regression coefficients, \n                            encouraging sparsity in the model by driving some coefficients to exactly zero. <code>LassoLarsIC</code>, short for Least \n                            Angle Regression with AIC/BIC criterion, combines the LARS (Least Angle Regression) algorithm with the AIC or BIC, which, as we know help in \n                            selecting the optimal subset of features by balancing model complexity and goodness of fit. <code>LassoLarsIC</code> sequentially adds features \n                            to the model, monitoring the AIC/BIC at each step, and stops when the chosen criterion is optimized. This makes it an effective method \n                            for automatic feature selection, especially in high-dimensional datasets where the number of features is large relative to the number \n                            of observations.\n                        </p>\n\n                        <p>\n                            Using <code>LassoLarsIC</code> is really quite simple. First, we need to create an instance of the model. We can do this by running\n                        </p>\n\n                        <pre>\n<code class=\"language-python\">lars_model = LassoLarsIC(criterion='bic', normalize=False)</code></pre>\n\n                        <p>\n                            Note that we are choosing to evaluate our model based on the BIC criterion. We could also choose to use the AIC criterion\n                            by simply replacing <code>'bic'</code> with <code>'aic'</code>.\n                        </p>\n\n                        <p>\n                            Next, we fit our model. We will fit it with the same <code>X_train</code> and <code>y_train</code> that we used for stepwise feature removal so we can\n                            compare the results with more accuracy.\n                        </p>\n\n                        <pre>\n<code class=\"language-python\">lars_model.fit(X_train, y_train.values.reshape(1, -1)[0])</code></pre>\n\n                        <p>\n                            We see that instead of using <code>y_train</code> directly, we need to reshape it. This is because <code>y_train</code> is a pandas series \n                            (and therefore a column vector), and <code>LassoLarsIC</code> expects a row vector. Using <code>y_train.reshape(1, -1)[0]</code> will convert\n                            the column vector to a row vector.\n                        </p>\n\n                        <p>\n                            Now that we have fit our model, we can see which values of $\\alpha$ were used and the corresponding BIC values by running\n                        </p>\n\n                        <pre>\n<code class=\"language-python\"># Get the results from the model\nresults = pd.DataFrame(\n    {\n        \"alphas\": lars_model.alphas_,\n        \"BIC criterion\": lars_model.criterion_,\n    }\n).set_index(\"alphas\")\nprint(results)</code></pre>\n\n                        <p>\n                            The output of this code is\n                        </p>\n\n                        <pre>\n<code class=\"language-python\">             BIC criterion\nalphas                    \n5791.332580    3181.170871\n23.835366      1837.591358\n15.655978      1838.232508\n8.145562       1831.269152\n2.384037       1674.376395\n0.663504       1660.903465\n0.295187       1665.666492\n0.232070       1662.522325\n0.188430       1663.228888\n0.033225       1655.087183\n0.000000       1659.478067</code></pre>\n\n                        <p>\n                            Now, in our case we see that the optimal value of $\\alpha$, according to the BIC, is $0.033225$.\n                            We can see which features the model kept by running <code>model.coef_</code>. This gives us\n                        </p>\n\n                        <pre>\n<code class=\"language-python\">lasso_model = Lasso(alpha=0.033225).fit(X_train, y_train.values.reshape(1, -1)[0])\nprint(lars_model.coef_)\n>>> [ 0.02359115, -0.61367486, -0.01471611, -0.00695722,  0.04982891, 0.74492259,  0.        , -2.30763917]</code></pre>\n\n                        <p>\n                            This tells us that the seventh features was dropped (<code>origin_Europe</code>), and the remaining features were kept. If we\n                            compute the MSE of this model and a regular linear regression model, we get\n                        </p>\n\n                        <pre>\n<code class=\"language-python\">from sklearn.linear_model import LinearRegression\nlasso_no_alpha = LinearRegression().fit(X_train, y_train.values.reshape(1, -1)[0])\n\n# Get the MSE on the test set for each model\ny_pred_no_alpha = lasso_no_alpha.predict(X_test)\ny_pred_opt_alpha = lars_model.predict(X_test)\n\nmse_no_alpha = np.mean((y_pred_no_alpha - y_test)**2)\nmse_opt_alpha = np.mean((y_pred_opt_alpha - y_test)**2)\n\nprint(f'MSE when alpha=0:         {mse_no_alpha}')\nprint(f'MSE when alpha=0.033225:  {mse_opt_alpha}')</code></pre>\n\n                        <p>\n                            The output of this code is\n                        </p>\n                        <pre>\n<code class=\"language-python\">>>> MSE when alpha=0:         99.6709847776471\n>>> MSE when alpha=0.033225:  98.75552193526808</code></pre>\n                        \n                        <p>\n                            If we recall, the MSE of our stepwise feature removal model was $98.94744747911336$. This means that our $L^1$ regularization model\n                            performs slightly better than our stepwise feature removal model.\n                        </p>\n\n                        <p>\n                            This brings up an important point. In general, $L^1$ regularization often outperforms stepwise feature removal in linear regression \n                            due to its automatic and continuous shrinkage of less important features. $L^1$ regularization optimizes the model globally, \n                            providing a more robust approach to feature selection by considering all features simultaneously. It effectively reduces overfitting, \n                            handles multicollinearity better, and leads to sparser models with better generalization performance compared to the stepwise feature removal method.\n                        </p>\n\n                        <h4>Conclusion</h4>\n                        <p>\n                            In this article, we explored two different ways to perform feature selection, namely using $L^1$ regularization and stepwise feature removal.\n                            We also discussed how $L^1$ regularization often out performs stepwise feature removal. My hope is that you now understand how you can improve your\n                            machine learning models by performing feature selection, specifically using $L^1$ regularization.\n                            If you want to see the code used in this article, you can find it\n                            on my <a href=\"https://github.com/dylanskinner65/dylanskinner65.github.io/blob/main/blog/blog_files/feature_selection_l1_aic/feature_selection_l1_aic.ipynb\" target=\"_blank\" rel=\"noopener noreferrer\">Github</a>.\n                            If you want to read a paper that me and some of my friend wrote that used $L^1$ regularization for feature selection, you can find it\n                            <a href=\"https://github.com/jeffxhansen/NYC_Taxi_Trip_Duration/blob/main/NYC%20Taxi%20Duration%20Prediction.ipynb\" target=\"_blank\" rel=\"noopener noreferrer\">here</a>.\n\n                            \n                        </p>",
      "quote": "It's better to enjoy life - selection is a temporary thing.",
      "quoteAuthor": "Shreyas Iyer",
      "category": "Math & ML"
    },
    {
      "slug": "gradient-boosting",
      "title": "Gradient Boosting in Machine Learning",
      "date": "2024-01-24",
      "description": "",
      "content": "<p>\n                            Gradient Boosting, a powerful ensemble learning technique, is an important topic in machine learning and is the foundation of several powerful models.\n                            This sophisticated method combines the strengths of decision trees with a meticulous optimization process, leveraging the principles of boosting to \n                            sequentially improve the accuracy of weak learners. In this blog post, we'll discuss the math of gradient boosting, exploring its inner workings and \n                            understanding how it stands out among ensemble methods. We'll also shed light on AdaBoost, a version of gradient boosting, and draw comparisons between gradient boosting\n                            and random forests through a practical Python example. By the end of this journey, you'll gain a deeper insight into the mechanics of gradient boosting and its practical implications.\n                        </p>\n\n                        <figure>\n                            <img src=\"blog_files/gradient_boosting/agif13.gif\" alt=\"Vector field fun.\" width=\"90%\" height=\"90%\">\n                            <figcaption text-align=center>Gif acquired from <a href=\"https://necessarydisorder.wordpress.com/2017/09/04/animated-gifs-from-vector-fields-as-force-fields/\">necessary-disorder tutorials</a>. \n                                Vector fields are a great way to visualize gradients, but I mainly just think this gif is cool.</figcaption>\n                        </figure>\n                        \n                        <p></p>\n\n                        <h4>\n                            Why Boosting\n                        </h4>\n\n                        <p>\n                            In my previous blog post about <a href=\"https://dylanskinner65.github.io/blog/decision_tress.html\" target=\"_blank\" rel=\"noopener noreferrer\">decision trees</a>, I\n                            mentioned the building an optimal decision tree is an NP-hard problem and how using the standard greedy method of building trees can easily overfit the data. One solution\n                            to improving decision trees is by combining bagging and attribute bagging in\n                            <a href=\"https://dylanskinner65.github.io/blog/random_forests.html\" target=\"_blank\" rel=\"noopener noreferrer\">random forests</a>. While incredibly powerful,\n                            random forests can still overfit the data and are not always the best choice for classification tasks. This is where boosting comes in.\n                        </p>\n\n                        <h4>The Basic Idea Behind Boosting</h4>\n\n                        <p>\n                            Let's say we have a set of data $\\mathbb{D} = \\{(\\textbf{x}_1, y_1), \\dots, (\\textbf{x}_N, y_N) \\}$ and a weak learner (function) $f$ that\n                            is built to hopefully produce $f(\\textbf{x}_i) \\approx y_i$ for each data point $\\textbf{x}_i$. However, $f$ is actually not that good and \n                            rarely produces the correct output. One thing we can do is get another weak learner $g_1$ such that $f(\\textbf{x}_i) + g_1(\\textbf{x}_i)$ produces\n                            a better answer $y_i$ than just $f$ alone. This means that $|y_i - f(\\textbf{x}_i) - g_1(\\textbf{x}_i)| < |y_i - f(\\textbf{x}_i)|$.\n                        </p>\n\n                        <p>\n                            For this to be done, after finding $f$, we need to produce a new dataset $\\mathbb{D}_1 = \\{(\\textbf{x}_1, y_1 - f(\\textbf{x}_1)), \\dots, (\\textbf{x}_N, y_N - f(\\textbf{x}_N)) \\}$.\n                            Once that new dataset is created, we can train $g_1$ on $\\mathbb{D}_1$. If $g_1$ is fit well, then $f + g_1$ will be a better approximation of the data $\\mathbb{D}$ than $f$ alone.\n                            We can repeat this process to get $g_2, g_3, \\dots, g_M$ (creating $\\mathbb{D}_2, \\mathbb{D}_3, \\dots, \\mathbb{D}_M$ along the way) to get a final function \n                            $f_M = f + g_1 + g_2 + \\dots + g_M$ that will hopefully produce $f_M(\\textbf{x}_i)=y_i$ for all $i$.\n                        </p>\n\n                        <p>\n                            This is the idea behind boosting. We are trying to boost the performance of a weak learner by adding more weak learners to it that are trained on the residuals\n                            of the previous iteration.\n                        </p>\n\n                        <h4>Gradient Boosting</h4>\n\n                        <p>\n                            To set the scene, let's define a few things. We will let $\\mathscr{X}\\times\\mathscr{Y}$ be a probability space that our data is drawn from. Let\n                            $\\mathscr{L}$ be some fixed loss function, and let $\\mathscr{F}$ be a set of functions $f:\\mathscr{X}\\to\\mathscr{Y}$ that meet our criteria. Our ultimate goal\n                            is to find\n                        </p>\n\n                        $$f^* = \\text{argmin}_{f\\in\\mathscr{F}}\\mathbb{E}_{(\\textbf{x}, y)\\sim\\mathscr{X}\\times\\mathscr{Y}}[\\mathscr{L}(f(\\textbf{x}), y)].$$\n\n                        <p>\n                            Now, this is an unrealistic idea. We cannot possibly expect to find the optimal function $f^*$ considering how large $\\mathscr{F}$ is. \n                            So we need to find a function $f$ that is close to $f^*$, and we will do this through approximation.\n                        </p>\n\n                        <p>\n                            Let's say we have a sample $\\mathbb{D} = \\{(\\textbf{x}_1, y_1), \\dots, (\\textbf{x}_N, y_N) \\}$ drawn from $\\mathscr{X}\\times\\mathscr{Y}$. We can compute\n                            the expectation $\\mathbb{E}_{(\\textbf{x}, y)\\sim\\mathscr{X}\\times\\mathscr{Y}}[\\mathscr{L}(f(\\textbf{x}), y)]$ by taking the average of the loss function\n                            $T(f) = \\frac{1}{N}\\sum_{i=1}^N \\mathscr{L}(f(\\textbf{x}_i), y_i)$. Thus, we have\n                        </p>\n\n                        $$f^* \\approx \\text{argmin}_{f\\in\\mathscr{F}}T(f) = \\text{argmin}_{f\\in\\mathscr{F}}\\frac{1}{N}\\sum_{i=1}^{N}\\mathscr{L}(f(\\textbf{x}_i), y_i).$$\n\n                        <p>\n                            With gradient boosting, what gradient descent does it it takes our current $f_k\\in\\mathscr{F}$ that tries to approximate $f^*$,\n                            and finds the next $f_{k+1}$ by \n                        </p>\n\n                        $$f_{k+1} = f_k - \\alpha_k DT(f_k)^T,$$\n\n                        <p>\n                            where $DT(f_k)$ is the derivative of $T$ with respect to $f_k$, and $\\alpha_k > 0$ is the learning rate. This is the gradient descent step.\n                        </p>\n\n                        <p>\n                            Now, as mentioned above, finding $f^*$ is virtually impossible because $\\mathscr{F}$ is infinite dimensional. However,\n                            since we are only looking in $\\mathscr{F}$ at the points $\\mathbb{D}$, we can reduce the dimensions of $\\mathscr{F}$ from infinite to $N$. So,\n                            if $(\\hat{y}_1, \\dots, \\hat{y}_N) = (f(\\textbf{x}_1), \\dots, f(\\textbf{x}_N))$, $T$ is now a function of $(\\hat{y}_1, \\dots, \\hat{y}_N)$, which \n                            means that $DT$ is a function of $(\\hat{y}_1, \\dots, \\hat{y}_N)$ as well.  Now, $-\\alpha_k DT(f_k)^T$ might not be in $\\mathscr{F}$, \n                            so we simply use some $t_k\\in\\mathscr{F}$ that works as a good approximation for $t_k \\approx - \\alpha_k DT(f_k)^T$.\n                        </p>\n\n                        <p>\n                            In my blog post about random forests, I mentioned how we can use bagging and attribute bagging to create a set of independent decision trees.\n                            With gradient boosting, our $-\\alpha_k DT(f_k)^T$ is not even close to independent.\n                        </p>\n\n                        <h4>AdaBoost</h4>\n\n                        <p>\n                            AdaBoost is an example of using gradient boosting for classification. The idea behind AdaBoost is that we have a set of data $\\mathbb{D} = \\{(\\textbf{x}_1, y_1), \\dots, (\\textbf{x}_N, y_N) \\}$,\n                            where $y_i\\in\\{-1, 1\\}$ (not $\\{0,1\\}$), and a loss function $\\mathscr{L}(f(\\textbf{x}), y) = e^{-yf(\\textbf{x})}$ (which is the exponential loss function).\n                            We can then define $T$ to be (where $f_k(\\textbf{x}_i) = \\hat{y}_i$)\n                        </p>\n\n                        $$T(f_k) = \\frac{1}{N}\\sum_{i=1}^N \\mathscr{L}(f_k(\\textbf{x}_i), y_i) =  \\frac{1}{N}\\sum_{i=1}^N \\mathscr{L}(\\hat{y}_i, y_i) = \\frac{1}{N}\\sum_{i=1}^N \\exp(-\\hat{y}_i y_i)$$\n\n                        <p>\n                            Thus, for whatever our $\\alpha_k$ is, our gradient descent step in the $\\hat{y}_i$ direction will be\n                        </p>\n\n                        $$-\\alpha_k D_{\\hat{y}_i}T = -y_i\\exp(-\\hat{y}_iy_i).$$\n\n                        <p>\n                            Once this step is performed, we find the $t_k \\approx y_i\\exp(-\\widehat{y}_iy_i)$.  We can then create a new tree $t_{k+1}\\mathscr{F}$ based on the data \n                            $\\{(\\textbf{x}_i, y_i^{k+1}) \\}_{i=1}^N = \\{(\\textbf{x}_i, -\\alpha_k D_{\\hat{y}_i}T) \\}_{i=1}^N = \\{(\\textbf{x}_i, y_i\\exp(-\\hat{y}_i^k y_i)) \\}_{i=1}^N$\n                            where $\\hat{y}_i^{k} = f_k(\\textbf{x}_i)$. Now, updating our tree, we have\n                        </p>\n\n                        $$f_{k+1} = f_k + t_{k+1}.$$\n\n                        <p>\n                            We can repeat this process until we have a good approximation of $f^*$.\n                        </p>\n\n                        <h4>Gradient Boosting vs Random Forests in Python</h4>\n\n                        <p>\n                            Now with this understand of gradient boosting and AdaBoost, let's compare gradient boosting to random forests in python. Let's begin by importing the necessary libraries.\n                        </p>\n            \n<pre><code class=\"language-python\">from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier</code></pre>\n\n                        <p>\n                            For this example, we will use the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_olivetti_faces.html#sklearn.datasets.fetch_olivetti_faces\" target=\"_blank\" rel=\"noopener noreferrer\">Olivetti faces dataset</a> from sklearn.\n                            So let's load this dataset in and visualize a few of the faces.\n                        </p>\n                    \n<pre><code class=\"language-python\">from sklearn.datasets import fetch_olivetti_faces\nimport matplotlib.pyplot as plt\n\n# Load in the data\nfaces_X, faces_y = fetch_olivetti_faces(return_X_y=True, shuffle=True, random_state=1)\n\n# Visualize the first 4 faces\nfig, axes = plt.subplots(1, 4, figsize=(10, 5), dpi=100)\nfor i, ax in enumerate(axes):\n    ax.imshow(faces_X[i].reshape(64, 64), cmap='gray')\n    ax.set_title(f'Person {faces_y[i]}')\n    ax.axis('off')\nplt.show()</code></pre>\n\n                        <figure>\n                            <img src=\"blog_files/gradient_boosting/few_faces.png\" alt=\"The first 4 faces in the Olivetti faces dataset.\" width=\"90%\" height=\"90%\">\n                            <figcaption text-align=center>The first 4 faces in our shuffled Olivetti faces dataset.</figcaption>\n                        </figure>\n\n                        <p>\n                            Now that we have our data loaded in, let's split it into training and testing sets.\n                        </p>\n                \n<pre><code class=\"language-python\">from sklearn.model_selection import train_test_split\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(faces_X, faces_y, test_size=0.2, random_state=1)</code></pre>\n\n                        <p>\n                            We can now initialize our two and fit our two models. We will use 100 trees for both models and default parameters otherwise for both models, timing\n                            how long it takes to fit each model. Starting with initialization,\n                        </p>\n\n<pre><code class=\"language-python\"># Initialize the models\ngbc = GradientBoostingClassifier(n_estimators=100, random_state=1)\nrf = RandomForestClassifier(n_estimators=100, random_state=1)</code></pre>\n\n                        <p>\n                            Now, let's fit our models and time how long it takes to fit each model, starting with the gradient boosting model.\n                        </p>\n                \n<pre><code class=\"language-python\">%%timeit\ngbc.fit(X_train, y_train)\n>>> 10min 36s \u00b1 1.89 s per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)</code></pre>\n\n                        <p>\n                            Now, let's fit our random forest model and time how long it takes to fit.\n                        </p>\n\n<pre><code class=\"language-python\">%%timeit\nrf.fit(X_train, y_train)\n>>> 1.36 s \u00b1 4.24 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)</code></pre>\n\n                        <p>\n                            As we can see, the random forest models took <em><b>significantly</b></em> less time to train than the gradient boosting model. It took\n                            88 minutes and 12 seconds to fit the gradient boosted model, and 10.9 seconds to fit the random forest model. \n                            Now, just because training the random forest took less time does not mean the random forest model is better than the gradient boosting model.\n                            So let's see how well each model performs on the test set. We have\n                        </p>\n                    \n<pre><code class=\"language-python\">gbc.score(X_test, y_test)\n>>> 0.5375</code></pre>\n\n<pre><code class=\"language-python\">rf.score(X_test, y_test)\n>>> 0.9125</code></pre>\n\n                        <p>\n                            Now, this is interesting. The random forest model took significantly less time to train, and it performed better on the test set.\n                            This is not to say that gradient boosting is bad. In fact, gradient boosting is a very powerful technique. However, it is important to\n                            understand that gradient boosting is not always the best choice for classification tasks. In this case, random forests outperformed gradient boosting.\n                            In other cases (such as one the fashion mnist dataset), I have seen gradient boosting outperform random forests. Gradient boosting should\n                            simply be another tool in your toolbox that you can use when you need it!\n                        </p>\n\n                        <p>\n                            Also, gradient boosting is pretty obsolete now. It was a great technique when it was first introduced, but now algorithms such as\n                            XGBoost and LightGMB are much better and faster than gradient boosting. I will write a blog post about XGBoost and LightGBM in the future.\n                        </p>\n\n                        <h4>Conclusion</h4>\n                        <p>\n                            In this article, we talked about the math behind gradient boosting and talked a bit about using AdaBoost for classification. \n                            We discussed the why boosting is a good idea, and compared gradient boosting and random forests in Python!\n                            I hope that through this article, you now understand gradient boosting especially because of its importance in laying the foundation\n                            for other boosting algorithms.\n                            If you want to see the code used in this article, you can find it\n                            on my <a href=\"https://github.com/dylanskinner65/dylanskinner65.github.io/blob/main/blog/blog_files/gradient_boosting/gradient_boosting.ipynb\" target=\"_blank\" rel=\"noopener noreferrer\">Github</a>.\n                            I hope you delve deeper into gradient boosting on your data science and machine learning journey!\n                        </p>",
      "quote": "Nature abhors a gradient.",
      "quoteAuthor": "Eric Schneider",
      "category": "Math & ML"
    },
    {
      "slug": "gradient-descent",
      "title": "Gradient Descent",
      "date": "2024-04-26",
      "description": "In this blog post we discuss gradient descent and the math behind several modifications that make it better.",
      "content": "<p>\n                            Gradient descent is one of the key innovations that allows us to optimize functions. It is a simple algorithm with a simple goal: solving\n                            unconstrained optimization problems of the form\n\n                            $$\\text{minimize} \\;f: \\mathbb{R}^n\\to\\mathbb{R}.$$\n                        </p>\n\n                        <p>\n                            In a previous blog post we discuessed <a href=\"https://dylanskinner65.github.io/blog/unconstrained_opt.html\">unconstrained optimization</a> and some of the necessary and sufficient conditions for optimality. \n                            In this blog post, we will make the optimization a little more complicated by taking about gradient descent, Polyak's heavy ball method, and Nesterov's accelerated gradient descent method. Don't be afraid, though! In the words of\n                            one of my college professors, <a href=\"https://science.byu.edu/directory/ben-webb\">Dr. Ben Webb</a>, \"All we're doing is going down hill.\"\n                        </p>\n\n                        <figure>\n                            <img src=\"blog_files/gradient_descent/initial_mountain.jpg\" alt=\"A nice picture of a mountain.\" width=\"100%\" height=\"90%\">\n                            <figcaption text-align=center>\n                                Photo by <a href=\"https://unsplash.com/@sepoys?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash\">Rohit Tandon</a> on <a href=\"https://unsplash.com/photos/aerial-photography-of-mountain-range-covered-with-snow-under-white-and-blue-sky-at-daytime-9wg5jCEPBsw?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash\">Unsplash</a>  \n                            </figcaption>\n                        </figure>\n                        \n                        <p></p>\n\n                        <h4>\n                            The Basics of Gradient Descent\n                        </h4>\n\n                        <p>\n                            If you can recall from your multivariate calculus class (and if you never took multivariate calculus, that's okay), the gradient\n                            $Df(\\textbf{x})^{\\intercal}$ of a function $f$ is a vector that points in the direction of the greatest <em>increase</em> of the function at a point $\\textbf{x}$.\n                            This tells us that the negative gradient $-Df(\\textbf{x})^{\\intercal}$ points in the direction of the greatest <em>decrease</em> of the function at a point $\\textbf{x}$.\n                            This idea of the negative gradient is exactly the intuition behind gradient descent.\n                        </p>\n\n                        <p>\n                            So, we define the basic form of gradient descent to be\n\n                            $$\\textbf{x}_{k+1} = \\textbf{x}_k - \\alpha Df(\\textbf{x}_k)^{\\intercal},$$\n\n                            where $\\textbf{x}_k$ is the current point, $\\textbf{x}_{k+1}$ is the next point, and $\\alpha$ is a step size (or learning rate) that we can adjust.\n                        </p>\n\n                        <p>\n                            This is a very simply algorithm to code up with one simple implementation being as follows:\n                        </p>\n\n<pre><code class=\"language-python\">import numpy as np\nimport scipy.optimize as opt\n\n# Define our gradient descent function\ndef gradient_descent(f, x0, lr=0.1, tol=1e-6, maxiter=10**5):\n    # Get our initial guess\n    x_vals = np.array([x0])\n    x = x0\n    \n    # Iterate until we converge\n    for i in tqdm(range(maxiter)):\n        # Get the gradient\n        grad = opt.approx_fprime(x, f, 1e-6)\n        \n        # Update our guess\n        x = x - lr * grad\n        x_vals = np.vstack((x_vals, x))\n        \n        # Check for convergence\n        if np.linalg.norm(grad) < tol:\n            break\n    return x_vals, i</code></pre>\n\n                        <p>\n                            In this implementation, we use the <code>approx_fprime</code> function from the <code>scipy.optimize</code> package to approximate the gradient of the function $f$ at a point $\\textbf{x}$.\n                            This could also be doing using something like <code>jax</code> or your favorite difference method for calculating derivatives.\n                        </p>\n\n                        <p>\n                            To visualize how this code works, consider the following function:\n                            \n                            $$f(x,y) = -7e^{-\\frac{(x-8)^2 + (y-20)^2}{100}} - 4e^{-\\frac{(x+19)^2 + (y-17)^2}{100}} + 23e^{-\\frac{(x+7)^2}{100} - \\frac{(y+10)^2}{100}}.$$\n                            \n                            This function has two minima. A local minimum at the point $(-19, 17)$, and a global minimum at the point $(8, 20)$. We can visualize this function with the following plot.\n                        </p>\n\n                        <figure>\n                            <img src=\"blog_files/gradient_descent/func1.png\" alt=\"The test function function.\" width=\"100%\" height=\"90%\">\n                            <figcaption text-align=center>\n                                Our test function.\n                            </figcaption>\n                        </figure>\n\n                        <p>\n                            To see how gradient descent works, we can start at a point $(x_0, y_0)$ (which we pick randomly), and then follow the recursive formula until a convergence criterion is met.\n                            In this case, we define the convergence criterion to be when the norm of the gradient is less than $10^{-8}$. We can visualize this process with the following gif.\n                        </p>\n\n                        <figure>\n                            <img src=\"blog_files/gradient_descent/func1.gif\" alt=\"Gif of gradient descent at a random point.\" width=\"100%\" height=\"90%\">\n                        </figure>\n\n                        <p>\n                            In this gif, we are able to see both a strength and a weakness of gradient descent. The strength is that gradient descent is able to\n                            go downhill towards a minimum. The weakness is it finds a <em>local</em> minimum rather than a <em>global</em> minimum. \n                            This is a common problem in optimization and does not have a one-size-fits-all answer. There have been some research into \n                            fixing this problem, which we will now talk about.\n                        </p>\n\n                        <h4>Polyak's Heavy Ball Method</h4>\n\n                        <p>\n                            As seen in the previous section about Gradient Descent, we find the next point in our optimization by taking a step in the direction of the negative gradient.\n                            This gives us a progression of points that should lead us to a minimum. We can use this information and a few ideas from physics to improve our optimization.\n\n                            One idea is to think of a ball rolling down a hill. If the ball is heavy, it will have more momentum and will be able to roll further down the hill. This ball\n                            will have a particular velocity that we can find by looking at the current and previous positions of the ball and how long the point update took. In the case of\n                            gradient descent, since the time it takes to get from one point to the next is simply one time step, we can create an auxiliary variable $\\textbf{v}_k$ that represents\n                            the velocity of our point and is given by\n\n                            $$v_k = \\textbf{x}_{k+1} - \\textbf{x}_k.$$\n\n                            Notice, we find the velocity is always one step behind the current point. This should make sense because it is impossible to tell the velocity of the ball \n                            at the current point if we're not sure where the ball will be next. This is the idea behind Polyak's Heavy Ball Method.\n                        </p>\n\n                        <p>\n                            Differing from the basic gradient descent algorithm, Polyak's Heavy Ball Method is given by\n\n                            $$\\textbf{x}_{k+1} = \\textbf{x}_k - \\alpha Df(\\textbf{x}_k)^{\\intercal} + \\beta(\\textbf{x}_k - \\textbf{x}_{k-1}),$$\n\n                            where $\\beta$ is a <b>momentum</b> parameter that we can adjust. If $\\beta = 0$, this is simply gradient descent. It is common to choose $\\beta\\in(0, 1)$.\n                        </p>\n\n                        <p>\n                            There are two other ways to calculate Polyak's method. The first (which is actually how PyTorch implements it) is to use a dummy variable $\\textbf{u}$, initializing\n                            $\\textbf{u}_0 = \\textbf{x}_0$, and then updating by\n\n                            $$\\begin{aligned}\n                                \\textbf{u}_k &= (1 + \\beta)\\textbf{x}_k - \\beta\\textbf{x}_{k-1} \\\\\n                                \\textbf{x}_{k+1} &= \\textbf{u}_k - \\alpha Df(\\textbf{x}_k)^{\\intercal}.\n                            \\end{aligned}$$\n                        </p>\n\n                        <p>\n                            The second way by initializing the same dummy variable $\\textbf{u}$, but updating by\n\n                            $$\\begin{aligned}\n                                \\textbf{u}_{k+1} &= \\beta\\textbf{u}_k - Df(\\textbf{x}_k)^{\\intercal} \\\\\n                                \\textbf{x}_{k+1} &= \\textbf{x}_k + \\alpha \\textbf{u}_{k+1}.\n                            \\end{aligned}$$\n                        </p>\n\n                        <p>\n                            All three of these are fine implementations. For the purposes of this blog post, we will implement the first method. The code for this implementation is as follows (with the same\n                            imports from above):\n                        </p>\n\n<pre><code class=\"language-python\">def heavy_ball(f, x0, alpha=0.1, beta=0.7, tol=1e-6, maxiter=10**5):\n    # Get our initial guess\n    x_vals = np.array([x0])\n    x = x0\n    x_prev = x0\n    \n    # Iterate until we converge\n    for i in tqdm(range(maxiter)):\n        # Get the gradient\n        grad = opt.approx_fprime(x, f, 1e-6)\n        \n        # Update our guess\n        x_new = x - alpha * grad + beta * (x - x_prev)\n        x_vals = np.vstack((x_vals, x_new))\n        \n        # Check for convergence\n        if np.linalg.norm(x_new - x) < tol:\n            break\n        \n        # Update our points\n        x_prev = x\n        x = x_new\n        \n    return x_vals, i</code></pre>\n\n                        <p>\n                            In this case, we check for convergence by seeing if the norm between the previous and current points is less than some tolerance threshold. Implemented on the same function as above\u2014\n                            though with a different initial point\u2014we get:\n                        </p>\n\n                        <figure>\n                            <img src=\"blog_files/gradient_descent/func2.gif\" alt=\"Gif of Polyak's heavy ball method at a random point.\" width=\"100%\" height=\"90%\">\n                        </figure>\n\n                        <p>\n                            Again, we see that this method finds the local minima instead of the global minima, but it is interesting to note that as the slope of the surface changes, so does the distance between the more recent and current points.\n                            This is a result of the momentum parameter $\\beta$.\n                        </p>\n\n                        <p>\n                            Here is an example of when Polyak's heavy ball method finds the global minima.\n                        </p>\n\n                        <figure>\n                            <img src=\"blog_files/gradient_descent/func2a.gif\" alt=\"Gif of when Polyak's heavy ball method finds the global minima.\" width=\"100%\" height=\"90%\">\n                        </figure>\n\n                        <p>\n                            One interesting thing about Polyak's method is that its convergence rate (for convex functions) is $\\mathcal{O}(1/\\sqrt{\\varepsilon})$, where $\\varepsilon$ is our tolerance threshold; meaning, if we want\n\n                            $$\\lVert \\textbf{x}_k - \\textbf{x}^*\\rVert$$ \\leq \\varepsilon,$$\n\n                            we will need to run Polyak's method for $\\mathcal{O}(1/\\sqrt{\\varepsilon})$ iterations. ($\\textbf{x}^*$ is the true minimum of the function.)\n                        </p>\n\n                        <h4>Nesterov's Accelerated Gradient Descent Method</h4>\n\n                        <p>\n                            Another method that is similar to Polyak's Heavy Ball Method is Nesterov's Accelerated Gradient Descent Method. Nesterov's method is essentially advanced Polyak's method with a twist.\n\n                            A big part of Polyak's method over gradient descent is the idea of momentum. This momentum, however, can hurt us in the end as it can cause us to overshoot the minimum.\n                            Nesterov's method incorperates the idea of damping (or friction) into Newton's law of motion.\n\n                            If we write\n\n                            $$ma = F - cv,$$\n\n                            where $m$ is the mass of the object, $a$ is the acceleration, $F$ is the force, $c$ is the damping coefficient, and $v$ is the velocity. In this, we have that the effective\n                            force helps to decrease the velocity of the object. This allows the weight updates to not slow down in the beginning when the gradient is large. However, as we get closer to the minimum\n                            and the gradient magnitude is smaller, the damping coefficient will help to slow down the weight updates and prevent overshooting.\n                        </p>\n\n                        <p>\n                            To perform Nesterov's method, we can use the following update equations:\n\n                            $$\\begin{aligned}\n                                \\textbf{u}_{k+1} &= \\beta\\textbf{u}_k - Df(\\textbf{x}_k + \\beta\\textbf{u}_k) \\\\\n                                \\textbf{x}_{k+1} &= \\textbf{x}_{k+1} + \\beta(\\textbf{u}_{k+1}).\n                            \\end{aligned}\n                            $$\n\n                            It is important to point out that the only difference between this method and Polyak's method (at least the second alternative implementation) is the terms inside the derivative $Df$. \n                            This main change computes the gradient as if the weights have already moved with the current velocity $\\textbf{u}_k$. It then uses that velocity to update the weights.\n\n                            An alternative way to write this update is\n\n                            $$\\begin{aligned}\n                                \\textbf{u}_{k+1} &= (1 + \\beta)\\textbf{x}_k - \\beta\\textbf{x}_{k-1} \\\\\n                                \\textbf{x}_{k+1} &= \\textbf{u}_{k} - \\alpha Df(\\textbf{u}_{k}).\n                            \\end{aligned}$$\n                        </p>\n\n                        <p>\n                            The first way, implemented in Python, is given by\n                        </p>\n\n<pre><code class=\"language-python\">def nag(f, x0, alpha=0.1, beta=0.7, tol=1e-6, maxiter=10**5):\n    # Get our initial guess\n    x_vals = np.array([x0])\n    x = x0\n    v = np.zeros_like(x0)\n    \n    # Iterate until we converge\n    for i in tqdm(range(maxiter)):\n        # Get the gradient\n        grad = opt.approx_fprime(x - beta*v, f, 1e-6)\n        \n        # Update our guess\n        v_new = beta * v + alpha * grad\n        x_new = x - v_new\n        x_vals = np.vstack((x_vals, x_new))\n        \n        # Check for convergence\n        if np.linalg.norm(x_new - x) < tol:\n            break\n        \n        x = x_new\n        v = v_new\n        \n    return x_vals, i</code></pre>\n\n                        <p>\n                            In this case, <code>v_new</code> is our $\\textbf{u}_k$.\n                        </p>\n\n                        <p>\n                            Applying this to our test function, we get\n                        </p>\n\n                        <figure>\n                            <img src=\"blog_files/gradient_descent/func3.gif\" alt=\"Gif of Nesterov's Accelerated Gradient Descent.\" width=\"100%\" height=\"90%\">\n                        </figure>\n\n                        <p>\n                            Nesterov's method hits the global minimum in this case, but it is very possible for Nesterov's to hit a local minima. \n                        </p>\n\n                        <h4>Other Gradient Descent Methonds</h4>\n\n                        <p>\n                            These methods discussed today are not the only methods for gradient descent. There are many other methods that have been developed over the years. Some of these methods include:\n\n                            <ul>\n                                <li>Adagrad</li>\n                                <li>Adam</li>\n                                <li>Adadelta</li>\n                                <li>RMSprop</li>\n                                <li>LBFGS</li>\n                            </ul>\n\n                            but the current most popular method is Adam. I will touch on Adam in a future blog post.\n                        </p>\n\n                        \n                        <h4>Conclusion</h4>\n\n                        <p>\n                            Gradient descent is a huge, key component of deep learning and optimization. While not perfect, gradient descent does a dang good job for non-convex functions.\n\n                            Some of the main methods we covered today are standard gradient descent, Polyak's Heavy Ball Method, and Nesterov's Accelerated Gradient Descent. These methods all have their strengths and weaknesses, but they all have the same goal: to minimize a function.\n\n                            I hope you enjoyed this blog post. I invite you to check out the <a href=\"https://github.com/dylanskinner65/dylanskinner65.github.io/blob/main/blog/blog_files/gradient_descent/gradient_descent.ipynb\">iPython notebook</a>\n                            I used to write all the methods and create the graphs and animations (it took me a long time to figure out, so just take what I did and don't reinvent the wheel haha).\n                        </p>\n                        \n\n                        <!-- <h4>Citations</h4> -->\n\n                        <!-- <ol>\n                            <span id=\"adams\">[1]</span> Collin C. Adams, <em>The Knot Book</em>. American Mathematical Society, 2004. ISBN: 978-0821836781.\n                        </ol>\n                        <ol\n                            <span id=\"birman\">[2]</span> Joan S Birman. <em>Braids, links, and mapping class groups</em>. 82. Princeton University Press, 1974.\n                        </ol>    -->\n\n                        <!-- <ol>\n                            <span id=\"seifert\">[1]</span> Heinrich Seifert. <em>\u00dcber das Geschlecht von Knoten</em>. In: <em>Mathematische Annalen</em> 110 (1935), pp. 571\u2013592. DOI: 10.1007/BF01448044.\n                        </ol> -->\n\n                        <!-- <p>\n                            <a id=\"adams\"></a>Adams, C. C. (1994). The Knot Book: An Elementary Introduction to the Mathematical Theory of Knots. W. H. Freeman and Company. -->",
      "quote": "All we're doing is going down hill.",
      "quoteAuthor": "Dr. Ben Webb",
      "category": "Math & ML"
    },
    {
      "slug": "intro-to-knots",
      "title": "Introduction to Knot Theory: Knots, Isotopies, and Reidemeister's Theorem",
      "date": "2024-02-14",
      "description": "",
      "content": "<p>\n                           Knot theory is a branch of topology that studies the properties of mathematical knots.\n                           In this blog post, we will briefly explore the basics of knot theory and define some common terms used in the field.\n                           This blog post is a high level introduction to knot theory that is meant to not only be accessible to a general audience,\n                           but also prepare you for more advanced topics in knot theory that I will cover in subsequent blog posts.\n                        </p>\n\n                        <figure>\n                            <img src=\"blog_files/intro_to_knots/first_knot.gif\" alt=\"Fun knot gif.\" width=\"90%\" height=\"90%\">\n                            <figcaption text-align=center>A fun knot. Credit to <a href=\"https://giphy.com/gifs/3d-abstract-math-3oz8xu8AVoL9dShI9G\" target=\"_blank\" rel=\"noopener noreferrer\">Clayton Shonkwiler</a>.</figcaption>\n                        </figure>\n                        \n                        <p></p>\n\n                        <h4>\n                            Introduction to Knot Theory\n                        </h4>\n\n                        <p>\n                            When a non-mathematician thinks of a knot, they typically think about taking a piece of string and \n                            tying the two ends together in some specific way. In mathematics, knots are thought of differently. \n                            Instead of taking a piece of string and tying the two ends together, a mathematical <em>knot</em> can \n                            be thought of by  taking the two ends of a necklace, tying a knot in the middle of the necklace, and \n                            then clasping the two ends together. This results in a knotted loop where the only way to untangle \n                            the knot in the middle of the necklace is to cut the necklace and remove the knot. Put another way, \n                            a knot is a knotted loop of string except that we think of the string as having no thickness, and \n                            its cross-section being a single point <a href=\"#adams\">[1]</a>. Similarly, a <em>link</em> is a collection of multiple knots linked together.\n                        </p>\n\n                        <figure>\n                            <img src=\"blog_files/intro_to_knots/Figure1.png\" alt=\"Fun knot gif.\" width=\"90%\" height=\"90%\">\n                            <figcaption text-align=center><b>Figure 1:</b> Three of the most basic knots that are used. On the left we have the unknot, \n                                the middle is $3_1$ (the trefoil knot), and the right is $4_{1}$ (the figure 8 knot).</figcaption>\n                        </figure>\n\n                        <p>\n                            An important idea in knot theory is that there is no distinction between any particular configuration of a knot. \n                            In other words, if you take a knot and deform it in some way without passing it through itself or by cutting \n                            the string, the resulting knot is thought of as being the same as the original one. But, one problem arises \n                            from this idea. How can we tell when two different-looking knots are equivalent?\n                        </p>\n\n                        <p>\n                            Before looking at the mathematical equivalence of knots, it is important to understand a few key terms. \n                            The first is <em>projection</em>. Knots live in three-dimensional space. However, it is often convenient \n                            to describe them using two-dimensional pictures. These two-dimensional pictures are obtained by projecting \n                            the three-dimensional knot onto a two-dimensional plane. If you were to take two 'different' projections \n                            of the same knot to a plane and create a model of one of those projections out of string, you should be \n                            able to rearrange the string into the second projection without cutting the string. This idea of \n                            rearranging the string in 3-dimensional space is something knot theorists call an <em>ambient isotopy</em>. \n                            <em>Ambient</em> refers to the fact that the string is being deformed in three-dimensional space, \n                            and <em>isotopy</em> the deformation of the string. When deforming a knot projection, knot theorists\n                            use the term <em>planar isotopy</em>, with <em>planar</em> referring to the fact that the knot is only \n                            being deformed within the projection plane.\n                        </p>\n\n                        <figure>\n                            <img src=\"blog_files/intro_to_knots/Figure2.png\" alt=\"Fun knot gif.\" width=\"90%\" height=\"90%\">\n                            <figcaption text-align=center><b>Figure 2:</b> Here we have the knot $5_1$ in two different configurations. \n                                While these knots have been stretched in different ways, they are clearly \n                                ambient isotopic and their projections are planar isotopic.</figcaption>\n                        </figure>\n\n                        <p>\n                            This idea of isotopy is important in determining the equivalence of knots. Although two knots are considered \n                            equivalent if you can change one knot into the other by stretching it and moving it around without tearing \n                            it or causing the knot to intersect with itself, it is not immediately clear how this idea of 3-dimensional \n                            equivalence translates to 2-dimensional projections of the knot. In fact, it is a theorem that two knots are \n                            equivalent if and only if the projection of one knot can be transformed into that of another knot through a \n                            finite sequence of Reidemeister moves, as defined in Reidemeister's theorem below.\n                        </p>\n\n                        <h4>Reidemeister's Theorem</h4>\n\n                        Reidemeister's theorem states that two links are ambient isotopic if and only if their diagrams can be joined by \n                        a sequence consisting of planar isotopies and the following three Reidemeister moves (see <a href=\"#figure3\">Figure 3</a>):\n\n                        <ul>\n                            <li>Move I: Twist and untwist in either direction.</li>\n                            <li>Move II: Move one strand over another.</li>\n                            <li>Move III: Move a strand completely over or under a crossing.</li>\n                        </ul>\n\n                        <p>For more information about this theorem, you can check out chapter 1.4 in <a href=\"#adams\">[1]</a>.</p>\n\n\n                        <figure>\n                            <img src=\"blog_files/intro_to_knots/Figure3.png\" alt=\"Fun knot gif.\" width=\"90%\" height=\"90%\" id=\"figure3\">\n                            <figcaption text-align=center><b>Figure 3:</b> Move I (Left), Move II (Middle), Move III (Right)</figcaption>\n                        </figure>\n\n                        <p>\n                            Reidemeister moves represent three different changes that can be made to a knot projection which do not change \n                            the equivalence class of the knot itself. The first Reidemeister move (or Reidemeister move I) is done by \n                            twisting or untwisting the knot. This twist will create another crossing but does not change the knot. \n                            Reidemeister move II is done by pushing one strand above or below another strand. This push can be used \n                            to create two new crossings, or remove two existing crossings. Reidemeister move III involves sliding a \n                            strand from one side of a crossing to the other side of that same crossing. This will not change the \n                            number of crossings present.\n                        </p>\n\n                        <p>\n                            These Reidemeister moves are often referred to as twisting (move I), poking (move II), or sliding (move III) \n                            a knot, and represent the only three ways to change the projection of a knot, together with planar isotopy, \n                            without changing the knot itself. These moves are named after Kurt Reidemeister, a German mathematician, \n                            who proved in 1926 that if there are two distinct projections of the same knot, one can be transformed into \n                            the other by a sequence of these three moves and planar isotopy.\n                        </p>\n\n                        <h4>Conclusion</h4>\n\n                        <p>\n                            In this blog post, we have introduced the basics of knot theory and defined some common terms used in the field. \n                            We have also discussed Reidemeister's theorem, which states that two links are ambient isotopic if and only if their \n                            diagrams can be joined by a sequence consisting of planar isotopies and the three Reidemeister moves. In subsequent \n                            blog posts, we will explore more advanced topics in knot theory, including braids, surfaces, Markov's theorem, and\n                            slice surfaces.\n                        </p>\n\n                        <h4>Citations</h4>\n\n                        <ol>\n                            <span href=\"#adams\">[1]</span> Collin C. Adams, <em>The Knot Book</em>. American Mathematical Society, 2004.\n\n                            <!-- Add more list items as needed -->\n                        </ol>\n\n                        <!-- <p>\n                            <a id=\"adams\"></a>Adams, C. C. (1994). The Knot Book: An Elementary Introduction to the Mathematical Theory of Knots. W. H. Freeman and Company. -->",
      "quote": "We learn the rope of life by untying its knots.",
      "quoteAuthor": "Jean Toomer",
      "category": "Math & ML"
    },
    {
      "slug": "kernel-density-estimation",
      "title": "Kernel Density Estimation",
      "date": "2024-02-07",
      "description": "",
      "content": "<p>\n                           In the realm of statistics and data analysis, understanding the distribution of data is paramount. This is where\n                           kernel density estimators (KDEs) come into play! Unlike traditional parametric methods that make assumptions about \n                           the shape of the underlying distribution, KDEs offer a flexible and non-parametric approach to estimating probability density functions.\n                           KDEs allow you to transform a scattered set of data points into a smooth, continuous curve, revealing the inherent structure and tendencies of your dataset.\n                           In this blog post, we delve into the world of KDEs, exploring their principles, applications, and the invaluable insights they offer in uncovering the inherent nature of data.\n                        </p>\n\n                        <figure>\n                            <img src=\"blog_files/kernel_density_estimation/popcorn_image.jpg\" alt=\"Popcorn kernels.\" width=\"90%\" height=\"90%\">\n                            <figcaption text-align=center>Almost like a popcorn kernel.</figcaption>\n                        </figure>\n                        \n                        <p></p>\n\n                        <h4>\n                            The Math Behind Histograms\n                        </h4>\n\n                        <p>\n                            Before getting into KDEs, it's important to first understand histograms and the math behind them.\n                        </p>\n\n                        <p>\n                            Let's say we have a set of data $\\mathbb{D} = \\{x_1, \\dots, x_N\\}$ that we want to visualize. These data points \n                            are drawn from some unknown probability distribution $X$ with probability density function (PDF) $f_X(x)$. We want a way\n                            to both visualize and estimate $f_X(x)$ from our data $\\mathbb{D}$. We can do this by creating a histogram of the data.\n                        </p>\n\n                        <p>\n                            To build a histogram we need a couple things. If we know that our data lives in some interval $I=[a,b]$. We can define the\n                            <em>bin width</em> of the histogram to be $h = \\frac{b-a}{m}$, where $m\\in\\mathbb{Z}^+$ is the number of bins we want in our histogram\n                            (some positive integer). The bin width is exactly what it sounds like it should be: the width of each bin in the histogram. We can now\n                            use this bin width to mathematically define the bins in our histogram.\n                        </p>\n\n                        <p>\n                            We can define the $i$th bin in our histogram to be the interval\n\n                            $$B_i = (a + (i-1)h, a+ih].$$\n\n                            Thus, we can get the starting and ending locations of each bin! Now, we need to figure out how many data points fall into each bin. More specifically\n                            we want to find the percentage of points that fall into each bin.\n                        </p>\n\n                        <p>\n                            We can do this by simply getting the average number of points that fall into each bin, defined as \n\n                            $$\\widehat{p}_i = \\frac{1}{n}\\sum_{m=1}^{n}\\Bbb{I}_{B_i}(x_m)$$\n\n                            where $\\mathbb{I}$ is the indicator function.\n                        </p>\n\n                        <p>\n                            With all of this, we can now define our histogram to be the function\n\n                            $$ \\widehat{f}_n(x) = \\sum_{i=1}^m \\widehat{p}_i\\mathbb{I}_{B_i}(x).$$\n\n                            How fun! I bet you've never thought about histograms this way before!\n                        </p>\n\n                        <p>\n                            One of the main problems with histograms is that they are very sensitive to the bin width $h$. If $h$ is too small, then the histogram will be very\n                            noisy and will not give us a good idea of the underlying distribution. If $h$ is too large, then the histogram will be too smooth and will <em>also</em> not give us\n                            a good idea of the underlying distribution. So, how do we choose the right bin width? One method is by using the <em>Freedman-Diaconis rule</em> which is\n                            defined as\n\n                            $$ \\text{Bin Width} = 2\\frac{\\text{IQR}(x)}{\\sqrt[3]{n}}$$\n\n                            where IQR$(x)$ is the interquartile range of the data $x$ and $n$ is the number of points in our data set.\n\n                            Let's go ahead and visualize this idea!\n                        </p>\n\n                        <p>\n                            Let's start off by generating some data. We will generate data that represents a trimodal distribution. We can do\n                            this by generating data sets three normal distributions with different means and concatenate them together.\n                        </p>\n\n<pre><code class=\"language-python\">import numpy as np\nnp.random.seed(70)  # For reproducibility\nx1 = np.random.normal(1, 1.11, 400) + np.random.beta(2, 5, 400)\nx2 = np.random.normal(10, .5, 400)\nx3 = np.random.normal(5, 1, 400)\nx = np.concatenate((x1, x2, x3))</code></pre>\n\n                        <p>\n                            Before plotting this data, let's first calculate the optimal bin width using the Freedman-Diaconis rule.\n                        </p>\n\n<pre><code class=\"language-python\"># Get optimal binwidth\nopt_bins = 2 * (np.quantile(x, .75) - np.quantile(x, .25))/np.cbrt(len(x))\nopt_bins\n>>> 1.4303876051024769</code></pre>\n\n                        <p>\n                            Now that we have our optimal bin width, we can now calculate the optimal number of bins to use.\n                        </p>\n\n<pre><code class=\"language-python\"># Get optimal number of bins\nopt_bins = round(max(x) - min(x)/int(opt_bin_width))\nopt_bins\n>>> 14</code></pre>\n\n                        <p>\n                            So we know that our optimal number of bins is 14! Let's plot our histogram with a various number of bins\n                            (including 14) and see how that impacts our graphs!\n                        </p>\n\n                        <img src=\"blog_files/kernel_density_estimation/bincomparison.png\" alt=\"Histograms with various bin widths.\" width=\"90%\" height=\"90%\">\n\n                        <p>\n                            As you can see, there is a fine line between having too many and too few bins. Ultimately, the number of bins you choose\n                            is up to you and what you are trying to visualize. However, the Freedman-Diaconis rule is a great place to start!\n                        </p>\n\n\n                        <h4>Kernel Density Estimators</h4>\n\n                        <p>\n                            Histograms are great, but there are some inherent problems with them, with the most obvious being that they are discontinuous.\n                            This is a problem because the underlying data is, more than likely, continuous. Additionally, if we do not know the underlying distribution\n                            of the data, histograms do not give us any information about that distribution in a continuous sense. This is where kernel density estimators\n                            come into play! Let's first start off by defining what a kernel is.\n                        </p>\n\n                        <p>\n                            A nonnegative, integrable function $K:\\mathbb{R}\\to[0,\\infty)$ is called a <em>kernel</em> if\n                            \n                            <ol type=\"i\">\n                                <li>$\\int_{\\mathbb{R}}K(x)dx = 1$</li>\n                                <li>$\\int_{\\mathbb{R}}x K(x)dx = 0$</li>\n                                <li>$\\int_{\\mathbb{R}} x^2 K(x)dx > 0$.</li>\n                            </ol>\n\n                            All this is saying is that the area under the curve of the kernel is equal to 1 (it is a proper pdf), \n                            the mean of the kernel is 0, and the variance of the kernel is positive! It is important to note that\n                            any pdf that has mean 0 and positive variance is kernel. But, not every kernel is continuous!\n                        </p>\n\n                        <p>\n                            There are four main kernels that are used in practice: the uniform kernel, the triangular kernel, the Epanechnikov kernel, and the Gaussian kernel.\n                        </p>\n\n                        <p>\n                            The uniform kernel is given by\n\n                            $$K(x) = \\frac{1}{2}\\mathbb{I}_{[-1,1]}(x),$$\n\n                            the triangular kernel is given by\n\n                            $$K(x) = (1-|x|)\\mathbb{I}_{[-1,1]}(x),$$\n\n                            the Epanechnikov kernel is given by\n\n                            $$K(x) = \\frac{3}{4}(1-x^2)\\mathbb{I}_{[-1,1]}(x),$$\n\n                            and the Gaussian kernel is given by\n\n                            $$K(x) = \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{x^2}{2}}.$$\n\n                            (Note that the Gaussian kernel is simply the pdf of the standard normal distribution with mean $\\mu=0$ and variance $\\sigma^2=1$.)\n                        </p>\n\n                        <p>\n                            Unlike histograms, KDEs are more focused on <em>bandwidth</em> instead of bin width. The bandwidth $h$ is the width of the kernel that is placed at each data point.\n                            So the smaller $h$ is, the more narrow the kernels are and the more \"spiky\" the KDE is. The larger $h$ is, the wider the kernels are and the more \"smooth\" the KDE is.\n                            There is no \"optimal\" bandwidth, but there are some rules of thumb that can be used to choose a bandwidth, two of which are\n                            Scott's rule and Silverman's rule. Scott's rule is given by\n\n                            $$h = \\left(\\frac{6\\sigma}{n}\\right)^{\\frac{1}{5}}$$\n\n                            where $\\sigma$ is the sample standard deviation of the data. Silverman's rule is given by\n\n                            $$h = \\left(\\frac{4\\sigma}{3n}\\right)^{\\frac{1}{5}}.$$\n\n                            Another method for choosing a bandwidth is simply by trial and error.\n                        </p>\n\n                        <p>\n                            Now, kernels and bandwidth are essentially useless without the KDE itself, so let's define that!\n\n                            Let $\\mathbb{D} = \\{x_1, \\dots, x_n\\}$ be a set of data points drawn from some unknown distribution $X$ with pdf $f_X(x)$.\n                            Let $K(x)$ be our chosen kernel and $h$ be our chosen bandwidth. Then, the kernel density estimator $\\widehat{f}_n(x)$ is defined as\n\n                            $$\\widehat{f}_n(x) = \\frac{1}{nh}\\sum_{i=1}^n K\\left(\\frac{x-x_i}{h}\\right).$$\n\n                            Let's go ahead and define and visualize this in Python!\n                        </p>\n\n                        <h1>KDEs in Python</h1>\n\n                        <p>\n                            Let's start off by defining the kernels we will be using.\n                        </p>\n\n<pre><code class=\"language-python\"># Define the four kernels discussed\nuniform = np.vectorize(lambda x: 0.5 * np.where(np.abs(x) <= 1, 1, 0))\ntriangular = np.vectorize(lambda x: (1 - np.abs(x))*np.where(-1 <= x <= 1, 1, 0))\nepanechnikov = np.vectorize(lambda x: (3/4)*(1 - x**2)*np.where(-1 <= x <= 1, 1, 0))\ngaussian = np.vectorize(lambda x: (1/np.sqrt(2*np.pi))*np.exp(-0.5*x**2))</code></pre>\n\n                        <p>\n                            Plotting these, we get\n                        </p>\n\n                        <img src=\"blog_files/kernel_density_estimation/kernels.png\" alt=\"The four kernels discussed.\" width=\"90%\" height=\"90%\">\n\n                        <p>\n                            Now, let's define the KDE function.\n                        </p>\n\n<pre><code class=\"language-python\"># Define the KDE function with the custom kernel\ndef kde(x, data, h, kernel):\n    \"\"\"\n    Compute the Kernel Density Estimate (KDE) at one or multiple points.\n\n    Parameters:\n    - x (float or array-like): The point(s) at which to estimate the KDE.\n    - data (array-like): The dataset used for the KDE estimation.\n    - h (float): The bandwidth, controlling the smoothness of the estimate.\n    - kernel (function): The kernel function used for weighting data points.\n\n    Returns:\n    - float or array-like: The KDE estimate(s) at the specified point(s).\n    \"\"\"\n    # If we are estimate the value of a single point\n    if np.isscalar(x):\n        return np.sum(kernel((x - data) / h)) / (len(data) * h)\n    # If we are estimating the value of multiple points\n    else:\n        return [np.sum(kernel((xi - data) / h), axis=0) / (len(data) * h) for xi in x]</code></pre>\n\n                        <p>\n                            Now that we have all the necessary functions defined, let's go ahead and plot our KDEs!\n                            Let's plot these KDEs with a bandwidth of 0.1, 0.5, 1.5, and 3.0 for each of the four kernels, starting with the uniform kernel.\n                            \n                            (For reference, he is the code we used to generate the plots below. The code is the same for each kernel\n                            except for the kernel function used.)\n                        </p>\n\n<pre><code class=\"language-python\"># Define initial variables\nbandwidths = [0.1, 0.5, 1.5, 3.0]\nx_vals = np.linspace(min(x), max(x), 1200)\n\n# Plot the unform kernel KDE\nfig, ax = plt.subplots(2, 2, figsize=(16, 10), dpi=100)\nfor bw in bandwidths:\n    ax[bandwidths.index(bw) // 2][bandwidths.index(bw) % 2].plot(x_vals, kde(x_vals, x, bw, uniform), 'b', linewidth=2)\n    ax[bandwidths.index(bw) // 2][bandwidths.index(bw) % 2].hist(x, bins=14, color='c', edgecolor='black', linewidth=1.2, density=True)\n    ax[bandwidths.index(bw) // 2][bandwidths.index(bw) % 2].set_title(f'Bandwidth={bw}')\n\nplt.suptitle('Uniform Kernel', fontsize=16)\nplt.tight_layout()\nplt.show()</code></pre>\n\n                        <p>\n                            Plotting the uniform kernel for our generated data, we get\n                        </p>\n\n                        <img src=\"blog_files/kernel_density_estimation/uniform_kernel.png\" alt=\"KDEs with the uniform kernel.\" width=\"95%\" height=\"95%\">\n\n                        <p>\n                            Now, let's plot the triangular kernel. We get\n                        </p>\n\n                        <img src=\"blog_files/kernel_density_estimation/triangular_kernel.png\" alt=\"KDEs with the triangular kernel.\" width=\"95%\" height=\"95%\">\n\n                        <p>\n                            Moving onto the Epanechnikov kernel, we get\n                        </p>\n\n                        <img src=\"blog_files/kernel_density_estimation/epanechnikov_kernel.png\" alt=\"KDEs with the Epanechnikov kernel.\" width=\"95%\" height=\"95%\">\n\n                        <p>\n                            Finally, let's plot the Gaussian kernel. We get\n                        </p>\n\n                        <img src=\"blog_files/kernel_density_estimation/gaussian_kernel.png\" alt=\"KDEs with the Gaussian kernel.\" width=\"95%\" height=\"95%\">\n\n                        <p>\n                            As you can see, the KDEs are very sensitive to the bandwidth, but are not really all that sensitive to the kernel used.\n                            Sure, there are differences between the kernels at the various bandwidths, but buy-and-large they are all very similar.\n                            Thus, when creating a KDE, it is more important to focus on the bandwidth than the kernel used.\n                        </p>\n\n                        <h4>Conclusion</h4>\n\n                        <p>\n                            Congratulations! You've successfully ventured into the realm of kernel density estimators (KDEs). \n                            Now, let's explore how to leverage the power of KDEs in your data analysis journey. KDEs offer a \n                            flexible and non-parametric approach to estimate probability density functions, allowing you to \n                            uncover the inherent structure and tendencies of your dataset in a smooth, continuous manner.\n\n                            In addition to coding up your own KDEs, there are many Python packages that offer KDE functionality,\n                            one of which is Seaborn (you can learn about that <a href=\"https://seaborn.pydata.org/generated/seaborn.kdeplot.html\" target=\"_blank\" rel=\"noopener noreferrer\">here</a>).\n                            Additionally, all code used in this blog post (including creating the plots) can be found \n                            on my <a href=\"https://github.com/dylanskinner65/dylanskinner65.github.io/blob/main/blog/blog_files/kernel_density_estimation/kde.ipynb\" target=\"_blank\" rel=\"noopener noreferrer\">Github</a>\n                        </p>",
      "quote": "Must is a hard nut to crack, but it has a sweet kernel.",
      "quoteAuthor": "Charles Spurgeon",
      "category": "Math & ML"
    },
    {
      "slug": "linear-regression",
      "title": "Linear Regression",
      "date": "2023-04-15",
      "description": "",
      "content": "<h4>Introduction</h4>\n                        <p>Linear regression, a fundamental concept in statistics and machine learning, \n                            serves as a powerful tool for modeling and understanding the relationships between variables.\n                            At its core, linear regression explores the linear association between an independent variable\n                            (or multiple variables) and a dependent variable. This technique provides a systematic way to\n                            quantify and predict the impact of changes in one variable on another, making it a cornerstone \n                            in the toolkit of data analysts and scientists. Whether predicting stock prices, analyzing \n                            economic trends, or understanding the factors influencing house prices, \n                            linear regression is a fundamental tool in statistical modeling. In this blog post, we'll delve into the principles of linear regression,\n                            model accuracy, and regularization.\n                        </p>\n\n                        <h4>What is Regression?</h4>\n\n                        <p>At its core, a <em>regression</em> problem is one where you have random variables $X=(X_1, X_2,\\dots, X_n)\\in\\mathbb{R}^d$,\n                            $Y\\in\\mathbb{R}$, and $\\varepsilon\\in\\mathbb{R}$, with the relationship $Y=f(X)+\\varepsilon$, where $f$ is some unknown function.\n                            The goal of regression is to estimate $f$ using the data $\\{(\\boldsymbol{x}_1, y_1), (\\boldsymbol{x}_2, y_2),\\dots, (\\boldsymbol{x}_n, y_n)\\}$.\n                            In this situation, the variable $Y$ is the <em>dependent</em> or <em>target</em> variable, the variable $X$\n                            is the <em>independent</em> or <em>predictor</em> variable, and the individual components\n                            $X_1, X_2,\\dots, X_n$ of $X$ are the <em>features</em> or <em>factors</em>. The variable $\\varepsilon$ is the <em>error</em> term.\n\n                        </p>\n\n                        <p>In <em>linear regression</em>, we assume that the function $f$ takes the form of some linear or affine function, typically\n                            of the form $Y = \\beta_0 + X^T\\boldsymbol{\\beta} + \\varepsilon$, where $\\boldsymbol{\\beta}\\in\\mathbb{R}^d$, and\n                            $\\beta_0\\in\\mathbb{R}$. This is often reformulated into the form $Y = \\boldsymbol{\\beta}^T\\boldsymbol{x} + \\varepsilon$,\n                            where $\\boldsymbol{x} = (1, X_1, X_2,\\dots, X_n)\\in\\mathbb{R}^{d+1}$, and $\\boldsymbol{\\beta} = (\\beta_0, \\beta_1, \\beta_2,\\dots, \\beta_n)\\in\\mathbb{R}^{d+1}$.\n                        </p>\n\n                        <p>\n                            Here, $\\boldsymbol{\\beta}$ is unknown, so the goal of linear regression is to estimate $\\boldsymbol{\\beta}$ using the data\n                            $\\{(\\boldsymbol{x}_1, y_1), (\\boldsymbol{x}_2, y_2),\\dots, (\\boldsymbol{x}_n, y_n)\\}$. This is done by minimizing the <em>residual sum of squares</em> (RSS),\n                            which is defined as\n\n                            $$RSS= ||\\boldsymbol{y} - \\mathbb{X}\\tilde{\\boldsymbol{\\beta}}||_2^2 =  \\sum_{i=1}^n \\tilde{\\varepsilon}_i^2.$$\n\n                            where $\\tilde{\\varepsilon}_i$ is the <em>residuals</em> defined as the difference between the measured and predicted values, given by\n                            $$\\widetilde{\\varepsilon}_i = y_i - \\tilde{y}_i,$$\n\n                            $\\tilde{y}_i$ is the fitted or predicted values of $y_i$, defined as\n                            $$\\tilde{y}_i = \\boldsymbol{x}_i^T\\tilde{\\boldsymbol{\\beta}} = (\\mathbb{X}\\tilde{\\boldsymbol{\\beta}})_i,$$\n\n                            and $\\mathbb{X}$ is the <em>design matrix</em> defined as\n                            $$\\mathbb{X} = \\begin{bmatrix} \n                                \\boldsymbol{x}_1^T \\\\\n                                \\boldsymbol{x}_2^T \\\\\n                                \\vdots \\\\\n                                \\boldsymbol{x}_n^T \\\\\n                            \\end{bmatrix}\n                            =\n                            \\begin{bmatrix}\n                                1 & x_{11} & x_{12} & \\dots & x_{1d} \\\\\n                                1 & x_{21} & x_{22} & \\dots & x_{2d} \\\\\n                                \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n                                1 & x_{n1} & x_{n2} & \\dots & x_{nd} \\\\\n                                \\end{bmatrix}$$\n                        </p>\n\n                        <p>\n                            It should be known that the residual sum of squares is a pretty good choice for measuring the quality of our model. But,\n                            one of the best ways to estimate what our unknown $\\boldsymbol{\\beta}$ values are is to use the <em>ordinary least-squares (OLS)</em> method.\n                        </p>\n\n                        <p>\n                            The ordinary least-squares method is done by calculating\n\n                            $$\\widehat{\\boldsymbol{\\beta}} = \\text{argmin}_{\\tilde{\\boldsymbol{\\beta}}}||y - \\mathbb{X}\\tilde{\\boldsymbol{\\beta}}||_2^2\n                            = \\text{argmin}_{\\tilde{\\boldsymbol{\\beta}}}\\sum_{i=1}^n \\tilde{\\varepsilon}_i^2,$$\n\n                            and if $\\mathbb{X}$ is of full rank (meaning its columns are linearly independent), then we can use the equation\n                            $$\\mathbb{X}^T\\mathbb{X}\\widehat{\\boldsymbol{\\beta}} = \\mathbb{X}^T\\boldsymbol{y}$$\n                            \n                            to get the solution, given by\n                            $$\\widehat{\\boldsymbol{\\beta}} = (\\mathbb{X}^T\\mathbb{X})^{-1}\\mathbb{X}^T\\boldsymbol{y}.$$\n\n                            Of course, in practice, you would never invert the matrix $\\mathbb{X}^T\\mathbb{X}$ because of its time complexity and stability, but instead use a more efficient method like QR decomposition.\n                        </p>\n\n                        <p>\n                            But what if your matrix $\\mathbb{X}$ is not of full rank? There are two main ways to deal with this, the first one\n                            being to us the SVD of the matrix, which we will now describe.\n                        </p>\n\n                        <p>\n\n                            Recall that if $\\mathbb{X}$ is of rank $r$, then we can write it as \n                            $$\\mathbb{X} = U\\Sigma V^H,$$\n                            \n                            where $U\\in M_{n\\times r}$, $V\\in M_{r\\times d}$, and $\\Sigma\\in M_{r\\times r}.$ Recall that the matrices\n                            $U$ and $V$ are orthonormal, so we can solve the OLS problem by\n\n                            $$\\widehat{\\boldsymbol{\\beta}} = V\\Sigma^{-1}U^T\\boldsymbol{y}.$$\n\n                            Not only will this product a solution in the rank deficient case (please note this is not a unique solution),\n                            but it also has several computational advantages over the method presented for the full-rank case. Thus,\n                            even if you have a full-rank matrix, it can often be helpful to use the SVD method because of its speed.\n                        </p>\n\n                        <h4>Python Example of OLS</h4>\n\n                        <p>Now that we mathematically understand how to solve linear regression problems using \n                            the OLS method, let's see how we can implement this in Python. For this example, we will use randomly generated data.\n                        </p>\n\n                        <p>Let's begin by generating the data. To do that, we can use the following code:</p>\n\n                        <pre>\n                            <code class=\"language-python\">\n# Import NumPy\nimport numpy as np\n\n# Get the random data, first setting a random seed for reproducibility\nnp.random.seed(100)\nn = 50  # Number of points\nX = np.linspace(0, 10, n)  # Generate n points between 0 and 10 for X \nepsilon = np.random.uniform(-5, 3, size=(n, ))  # Noise for y coordinates\ny = 2 + 3*X + epsilon  # Generate y coordinates</code>\n                        </pre>\n\n                        <p>This code produces the following graph</p>\n\n                        <img src=\"blog_files/linear_regression/scatter.png\" />\n\n                        <p>It is pretty easy to see the linear relationship between this data, but there is no clear linear line that best fits this.</p>\n\n                        <p>Now, let's implement the OLS method. To do this, we will use the following code:</p>\n\n                        <pre>\n                            <code class=\"language-python\">\ndef OLS(X, y):\n    \"\"\"\n    Perform Ordinary Least Squares (OLS) linear regression.\n\n    Parameters:\n    - X (numpy.ndarray): Input feature matrix of shape (n_samples, n_features).\n    - y (numpy.ndarray): Target variable vector of shape (n_samples,).\n\n    Returns:\n    - numpy.ndarray: Coefficient vector beta, representing the solution to the OLS regression.\n    \n    This function fits a linear regression model using the Ordinary Least Squares method. It adds a column\n    of ones to the input feature matrix X to account for the intercept term (beta_0). The function then\n    calculates and returns the coefficient vector beta using the OLS formula: beta = (X^T * X)^(-1) * X^T * y.\n    \"\"\"\n    # Add a column of ones to X (account for beta_0)\n    X = np.column_stack((np.ones_like(y), X))\n\n    # Return beta's\n    return np.linalg.solve(X.T @ X, X.T @ y)</code>\n                        </pre>\n                        \n                        <p>Running this code on our generated data, calculating the RSS, and plotting our results, we can visualize how well our OLS line fits the data.</p>\n\n                        <img src=\"blog_files/linear_regression/scatter_ols.png\" />\n\n                        <p>Pretty good!</p>\n\n                        <p>Now, using the same data, let's code up an SVD solver, and see how well it does. We can code up the SVD solver using the following code.</p>\n\n                        <pre>\n                            <code class=\"language-python\">\ndef SVD_OLS(X, y):\n    \"\"\"\n    Perform Ordinary Least Squares (OLS) linear regression using Singular Value Decomposition (SVD).\n\n    Parameters:\n    - X (numpy.ndarray): Input feature matrix of shape (n_samples, n_features).\n    - y (numpy.ndarray): Target variable vector of shape (n_samples,).\n\n    Returns:\n    - numpy.ndarray: Coefficient vector beta, representing the solution to the OLS regression.\n    \n    This function fits a linear regression model using the Ordinary Least Squares method with the help of\n    Singular Value Decomposition (SVD). It adds a column of ones to the input feature matrix X to account\n    for the intercept term (beta_0). The function then computes the SVD of X (X = U * Sigma * V^T), where U and\n    V are orthogonal matrices, and Sigma is a diagonal matrix of singular values. The OLS solution is obtained\n    using the SVD components: beta = V * diag(1/s) * U^T * y, where diag(1/s) represents the inverse of the\n    diagonal matrix of singular values.\n    \"\"\"\n    # Add a column of ones to account for beta_0\n    X = np.column_stack((np.ones_like(y), X))\n\n    # Compute SVD of X\n    U, s, V = np.linalg.svd(X, full_matrices=False)\n\n    # Return OLS using SVD\n    return V.T @ np.diag(1/s) @ U.T @ y</code>\n                        </pre>\n\n                        <p>Running this code on our generated data, calculating the RSS, and plotting our results (along with the results from the OLS method),\n                            we can visualize how well our SVD line fits the data.</p>\n                        \n                        <img src=\"blog_files/linear_regression/scatter_ols_svd.png\" />\n\n                        <p>As you can see, the lines are exactly the same with the exact same RSS score. Thus, either option is viable for our randomly generated data!</p>\n\n\n                        <h4>Regularization</h4>\n\n                        <p>Now that we have a basic understand of doing linear regression, let's explore the method of <em>regularization</em> and how it can improve our models.</p>\n\n\n                        <h5>Ridge Regression</h5>\n                        <p>The first form of regularization we want to look at is <em>Ridge</em> or $L^2$ regularization. Ridge regularization is a simple modification we \n                        can make to our linear regression. We can implement ridge regression by modifying the OLS method as follows:\n                    \n                        $$\\widehat{\\boldsymbol{\\beta}} = \\text{argmin}_{\\boldsymbol{\\beta}}\\frac{1}{n}||y - \\mathbb{X}\\boldsymbol{\\beta}||_2^2 + \\lambda||\\boldsymbol{\\beta}||_2^2$$\n                    \n                        In this case, $\\lambda>0$ is a hyperparameter that you can tweek.</p>\n\n                        <p>In this formula, we include $\\frac{1}{n}$ because the OLS term growing larger with more data, but the regularization term does not. Thus, scaling\n                            by a factor of $\\frac{1}{n}$ ensures that things are kept proportional.\n                        \n                        It is also important to note that if $\\lambda$ is very small, then our minimizer is essentially just the OLS solution and is potentially overfit, and if \n                        $\\lambda$ is very large, then our minimizer is close to $0$ and is potentially underfit. Thus, adding ridge regularization can allow our model to avoid over or underfitting our data.</p>\n\n                        <p>Another benefit of using ridge regression is that even if $\\mathbb{X}^T\\mathbb{X}$ is singular or nearly singular (i.e., not invertible), adding \n                            this regularization term will make it invertible. Thus, ridge regression can be used to solve the problem of $\\mathbb{X}^T\\mathbb{X}$ being singular. We\n                        can find the unique minimizer of a problem using the following formula\n                    \n                        $$\\widehat{\\boldsymbol{\\beta}} = (\\mathbb{X}^T\\mathbb{X} + n\\lambda I)^{-1}\\mathbb{X}^T\\boldsymbol{y}$$\n\n                        where $I$ is simply an $(n\\times n)$ identity matrix.\n                        </p>\n\n                        <p>One important thing to note about ridge regression is that it will rarely zero-out the $\\beta$ coefficients. Instead,\n                            it makes unnecessary coefficients very small. This is nice if we do not care about model interpretability, but\n                            if understand more about the model is important, ridge regression makes it difficult. In this case, we can use <em>Lasso</em> or $L^1$ regularization.\n                        </p>\n\n                        <h5>Lasso Regression</h5>\n                        <p>Lasso regularization is very similar to ridge regularization, but instead of using the $L^2$ norm, we use the $L^1$ norm. Thus, we can implement lasso regression by modifying the OLS method as follows:\n\n                        $$\\widehat{\\boldsymbol{\\beta}} = \\text{argmin}_{\\boldsymbol{\\beta}}\\frac{1}{n}||y - \\mathbb{X}\\boldsymbol{\\beta}||_2^2 + \\lambda||\\boldsymbol{\\beta}||_1$$</p>\n\n                        <p>As mentioned above, lasso regression will zero-out coefficients. This makes lasso regression extremely helpful for feature selection. In fact,\n                            lasso regression often out performs stepwise feature removal!\n\n                            A potential downside of lasso regression is that unlike ridge regression, it is not easily solved using standard techniques. This is because the loss function is not\n                            differentiable at every point. The loss function is convex, however, which means we can use standard convex optimization techniques to solve it.\n                        </p>\n\n                        <h5>Elastic Net Regularization</h5>\n                        <p>A final regularization technique to mention is <em>elastic net</em> regularization. Elastic net regularization is simply a combination of ridge and lasso regularization.\n                        This is beneficial because it combines the benefits of ridge regression (which often out performs lasso regression), but still encourages the sparsity that comes from \n                        lasso regression. Elastic net regularization is given by:\n                    \n                        $$\\widehat{\\boldsymbol{\\beta}} = \\text{argmin}_{\\boldsymbol{\\beta}} \\frac{1}{n}||y - \\mathbb{X}\\boldsymbol{\\beta}||_2^2 + \\lambda_2||\\boldsymbol{\\beta}||_2^2\n                                + \\lambda_1 ||\\boldsymbol{\\beta}||_1,$$\n                            \n                            where $\\lambda_1, \\lambda_2\\geq 0$. We can minimize this loss function using similar techniques for minimizing lasso regression.</p>\n\n\n                        <h4>Conclusion</h4>\n\n                        <p>Linear regression is a simple, yet powerful tool for analyzing data. It is a simple machine learning algorithm, but it is easy to understand and implement, making it a helpful\n                            first algorithm in a data scientist's toolkit.\n\n                            To see the code used in this blog post (including creating the images), checkout the file on my <a href=\"https://github.com/dylanskinner65/dylanskinner65.github.io/blob/main/blog/blog_files/linear_regression/linear_regression.ipynb\" target=\"_blank\" rel=\"noopener noreferrer\">Github</a>.</p>",
      "quote": "\u201cYou can do linear regression without thinking about whether the phenomenon you\u2019re modeling \n                            is actually close to linear. But you shouldn\u2019t.\u201d",
      "quoteAuthor": "Jordan Ellenberg",
      "category": "Math & ML"
    },
    {
      "slug": "mdp-bo",
      "title": "Introduction to Reinforcement Learning: the Markov Decision Process and the Bellman Equation",
      "date": "2024-03-06",
      "description": "",
      "content": "<p>\n                           Reinforcement learning is a type of machine learning that is concerned with how an agent should take actions in an environment in order to \n                           maximize some notion of cumulative reward. In this blog post, we will begin a brief introduction to reinforcement learning by talking about \n                           the Markov decision process (MDP).\n                        </p>\n\n                        <figure>\n                            <img src=\"blog_files/mdp_bo/intro_picture.jpg\" alt=\"A man rock climbing.\" width=\"90%\" height=\"90%\">\n                            <figcaption text-align=center>Photo by <a href=\"https://unsplash.com/@neom?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash\">NEOM</a> on <a href=\"https://unsplash.com/photos/a-man-climbing-up-the-side-of-a-cliff-xhMz5xIbhRg?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash\">Unsplash</a>\n                            </figcaption>\n                        </figure>\n                        \n                        <p></p>\n\n                        <h4>\n                            What is Reinforcement Learning?\n                        </h4>\n\n                        <p>\n                            Reinforcement learning (RL) is a branch of machine learning that allows an agent to interact with an environment \n                            that it is placed in and to learn from the results of its interactions. When the agent is placed into an environment \n                            it is given a set of actions that it is allowed to take, which is how it interacts with the environment. \n                            The goal of reinforcement learning is to train the agent in such a way that it learns to select actions that yield \n                            optimal results given whatever situation it is in. As a simple example, consider the Atari game <em>Breakout</em>. \n                            The goal of the game is to break all the bricks in the level, which is done by using the paddle to hit a ball at \n                            the bricks. Tackling this problem using reinforcement learning, the agent controls the paddle and learns to move \n                            it in the most efficient way to break the bricks.\n                        </p>\n\n                        <figure>\n                            <img src=\"blog_files/mdp_bo/Breakout2600.png\" alt=\"A picture of the Atari game, Breakout.\" width=\"90%\" height=\"90%\" id=\"figure1\">\n                            <figcaption text-align=center><b>Figure 1:</b> The Atari 2600 version of Breakout. </figcaption>\n                        </figure>\n\n                        <p>\n                            When an agent begins training, it is passed a starting state $s_{0}$ from the environment. \n                            The agent looks at this state, thinks about what it knows (which in the beginning is nothing), and selects an action $a_{0}$. \n                            This action is then sent back to the environment, analyzed, and assigned a reward $r_0$ based on the effect of the action \n                            on the environment. The environment then creates the next state $s_1$, and sends it and the reward $r_0$ back to the agent. \n                            This cyclical pattern occurs until the agent achieves the goal, fails, or some pre-determined number of time steps is reached. \n                            We can often model this situation as a <em>Markov Decision Process (MDP)</em>\n                        </p>\n\n                        <h4>The Markov Decision Process</h4>\n\n                        <p>\n                            An MDP is defined as $(S, A, R, \\mathbb{P}, \\gamma)$, where \n                            \n                            <ul>\n                                <li>$S$ is the set of states,</li>\n                                <li>$A$ is the set of actions,</li>\n                                <li>$R$ is the distribution of rewards,</li>\n                                <li>$\\mathbb{P}$ is the transition probabilities,</li>\n                                <li>$\\gamma$ is the discount factor.</li>\n                            </ul> \n                            Reinforcement learning agents must learn decision making strategies not only in situations where actions \n                            create immediate rewards, but actions which impact rewards far into the future. In an MDP the current \n                            state $s_t$ tells us everything we need to know about the environment we are working in (this is called the <em>Markov property</em>). \n                            This is beneficial because there is no risk of filling up memory, but can be detrimental because all \n                            information about the past is essentially forgotten.\n                        </p>\n\n                        <p>\n                            The goal of the agent utilizing the MDP is to pick an action to maximize the reward. To do this, \n                            the agent uses a policy $\\pi$ which is a function that maps $S$ to $A$, represented as $\\pi: S \\to A$ \n                            (in some cases it is more useful to think of $A$ as a probability distribution across all actions, \n                            conditioned on the current state $s_t$). This function will pick an action based on the state the agent is in. \n                            Our hope is to find the best policy $\\pi^{*}$ that will maximize the cumulative possible reward for the agent \n                            $\\sum_{t=0}^{T}\\gamma^{t}r_{t}$ (here the discount factor $\\gamma$ is included so future rewards are not \n                            considered as heavily as current rewards). But this is a difficult task.\n                        </p>\n\n                        <p>\n                            Through the work of researchers many different algorithms\u2014both classical and ones that rely on deep learning\u2014\n                            have been created to aid in finding $\\pi^{*}$ in the most efficient way possible. \n                        </p>\n\n                        \n                        <h4>The Bellman Equation</h4>\n                        \n                        <p>\n                            In addition to the MDP, the <em>Bellman Optimiality Principle</em> is another key concept in reinforcement learning. Essentially,\n                            the Bellman optimality principle states that an optimal policy has the property that whatever the initial state and initial decision are,\n                            the remaining decisions must constitute an optimal policy with regard to the state resulting from the first decision. Meaning, for example,\n                            if the shortest path between Salt Lake City, UT and San Francisco, CA is through Elko, NV, then the shortest path between Elko and San Francisco\n                            must be through the shortest path between Salt Lake City and San Francisco. This principle is used to derive the Bellman equation, which is\n                            typically represented as:\n\n                            $$V(s) = \\text{max}_a\\left(R(s,a) + \\gamma V(s')\\right),$$\n\n                            where \n\n                            <ul>\n                                <li>$V(s)$ represents the 'value' of being in our current state,</li>\n                                <li>$R(s,a)$ is the reward we will get for taking action $a$ and being in state $s$,</li>\n                                <li>$\\gamma$ is the discount factor, and</li>\n                                <li>V(s') represents the the value of being in the next state $s'$.</li>\n                            </ul>\n\n                            The $\\text{max}_a$ tells us that the value of our current state is ultimately determined by the action that will yield the highest combination\n                            of immediate reward and future value. Note: that does not necessarily mean that the action with the highest immediate reward is the best action to take,\n                            but instead some combination of immediate reward and future value.\n\n                            Now, because there are typically multiple next states, the above equation is modified to\n\n                            $$V(s) = \\text{max}_a\\left(R(s,a) + \\gamma \\sum_{s'}p(s', a, s)V(s')\\right)$$\n\n                            This modification tells us that when figuring out the value of our current state, we must consider the value of all possible next states, and the probability\n                            of transitioning to those gates given our current state and action.\n                        </p>\n\n                        <p>\n                            To calculate $V(s)$, we use dynamic programming to recursively calculate the value of each state. These results are often stored in a table for\n                            easy lookup. If using the Bellman equation, we can let our agent run through the environment and update the value of each state as it goes. Our hope is that\n                            eventually the value of each state will converge to the true value of the state, and we can use this information to determine the best action to take in each state.\n                            (This is often called <em>value iteration</em>.)\n                        </p>\n\n                        <h4>MDP vs. Bellman</h4>\n\n                        <p>\n                            When people talk about reinforcement learning, they will often mention the MDP and/or the Bellman equation and optimality principle. So,\n                            let's compare them real quick.\n                        </p>\n\n                        <p>\n                            With the MDP, we have a memory benefit by not keeping track of all the previous states, but that can sometimes pose a \n                            problem because we lose all information about the past. The Bellman equation, on the other hand, allows us to know\n                            the value of each state, which is useful for determining the best action to take given whatever state we are in. However, \n                            not only can the Bellman equation be computationally expensive, but it also requires us to know the transition probabilities, which\n                            might not be known. Additionally, it is sometimes bold to assume that the state space is finite or can be discretized in a\n                            way that can be represented in a table.\n                        </p>\n\n                        <h4>Conclusion</h4>\n\n                        <p>\n                            In this blog post, we discussed the basics of the Markov decision process and the Bellman optimality principle and Bellman equation. \n                            These are fundamental concepts in reinforcement learning, and understanding them is crucial to understanding how reinforcement learning works.\n                            In future blog posts, we will discuss more advanced concepts in reinforcement learning, such as one of the optimal ways to calculate\n                            $\\pi^*$.\n                        \n                        </p>\n\n                        <!-- <h4>Citations</h4> -->\n\n                        <!-- <ol>\n                            <span id=\"adams\">[1]</span> Collin C. Adams, <em>The Knot Book</em>. American Mathematical Society, 2004. ISBN: 978-0821836781.\n                        </ol>\n                        <ol\n                            <span id=\"birman\">[2]</span> Joan S Birman. <em>Braids, links, and mapping class groups</em>. 82. Princeton University Press, 1974.\n                        </ol>    -->\n\n                        <!-- <ol>\n                            <span id=\"seifert\">[1]</span> Heinrich Seifert. <em>\u00dcber das Geschlecht von Knoten</em>. In: <em>Mathematische Annalen</em> 110 (1935), pp. 571\u2013592. DOI: 10.1007/BF01448044.\n                        </ol> -->\n\n                        <!-- <p>\n                            <a id=\"adams\"></a>Adams, C. C. (1994). The Knot Book: An Elementary Introduction to the Mathematical Theory of Knots. W. H. Freeman and Company. -->",
      "quote": "Nobody can point to the fourth dimension, yet it is all around us.",
      "quoteAuthor": "Rudy Rucker",
      "category": "Math & ML"
    },
    {
      "slug": "mle",
      "title": "Maximum Likelihood Estimation",
      "date": "2024-03-20",
      "description": "In this blog post we talk about maximum likelihood estimation and its importance in probability.",
      "content": "<p>\n                            Maximum likelihood estimation (MLE) stands as a cornerstone in the realm of statistical inference, \n                            offering a powerful method for estimating the parameters of a probability distribution. Rooted in \n                            the principle of finding the most probable values for the parameters given observed data, MLE \n                            provides a systematic framework for making inferences about unknown quantities. Whether in fields \n                            like economics, biology, or engineering, where uncertainty reigns, understanding and applying MLE \n                            empowers researchers and practitioners to glean insights from data and make informed decisions.\n                        </p>\n\n                        <p>\n                            In this blog post, we will explore the concept of MLE, its mathematical underpinnings, and consider a few\n                            examples.\n                        </p>\n\n                        <figure>\n                            <img src=\"blog_files/mle/estimation_image.jpg\" alt=\"A river.\" width=\"90%\" height=\"90%\">\n                            <figcaption text-align=center>Photo by <a href=\"https://unsplash.com/@solenfeyissa?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash\">Solen Feyissa</a> on <a href=\"https://unsplash.com/photos/a-black-background-with-multicolored-lines-in-the-dark-8z1SGcgkOiA?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash\">Unsplash</a>\n                            </figcaption>\n                        </figure>\n                        \n                        <p></p>\n\n                        <h4>\n                            What is Maximum Likelihood Estimation?\n                        </h4>\n\n                        <p>\n                            Maximum likelihood estimation is an estimation method used to find the parameter values of a probability\n                            distribution that maximize the likelihood of the observed data. In other words, MLE seeks to find the most\n                            probable values for the parameters of a distribution given the observed data. Consider the following definition.\n                        </p>\n\n                        <p>\n                            Let $X_1, X_2, \\ldots, X_n$ be a random sample from a discrete distribution $X$ with probability mass function (p.m.f.) $g(x,\\theta)$\n                            (or a random sample from a continuous distribution $X$ with probability density function (p.d.f.) $f(x,\\theta)$), \n                            where $\\theta$ is the parameter of the distribution. If $\\textbf{x} = (x_1, \\dots, x_n)$ is a draw from the sample, we define the joint probability\n                            to be\n\n                            $$L(\\theta) = P(X_1 = x_1, \\dots, X_n = x_n) = \\prod_{i=1}^n P(X_i=x_i) = \\prod_{i=1}^n g(x_i, \\theta) $$\n\n                            in the discrete case, or \n\n                            $$ \\prod_{i=1}^n P(X_i=x_i) = \\prod_{i=1}^n f(x_i, \\theta)$$\n\n                            in the continuous case. The function $L(\\theta)$ is called the <em>likelihood of</em> $\\mathit{\\theta}$\n\n                            We defined the point $\\widehat{\\theta}$ that maximizes $L(\\theta)$ as the <em>maximum likelihood estimate of</em> $\\mathit{\\theta}$. If we find a \n                            an estimator $\\widehat{\\theta}(X_1,\\dots, X_n)$ whose estimate $\\widehat{\\theta}(x_1,\\dots,x_n)$ is the maximum likelihood estimate of $\\theta$, \n                            then $\\widehat{\\theta}$ is called a <em>maximum likelihood estimator</em> of $\\theta$.\n                        </p>\n\n                        <p>\n                            With those definitions in mind, let's do some examples to illustrate the concept.\n                        </p>\n\n                        <h4>MLE Examples</h4>\n\n                        <p>\n\n                        For our first example, let's consider a simple case where we have a random sample from a Bernoulli distribution. Recall that the p.m.f.\n                        for the Bernoulli distribution is given by\n\n                        $$g(x,\\theta) = \\theta^x(1-\\theta)^{1-x}$$\n\n                        where $x \\in \\{0,1\\}$ and $\\theta \\in [0,1]$, with $\\theta$ unknown. If $\\textbf{x} = (x_1, \\dots, x_n)$ is a draw from our distribution,\n                        then the likelihood of $\\theta$ is given by\n\n                        $$L(\\theta) = \\prod_{i=1}^n \\theta^{x_i}(1-\\theta)^{1-x_i}.$$\n\n                        By the nature of products, this is equivalent to\n\n                        $$L(\\theta) = \\theta^{\\sum_{i=1}^n x_i}(1-\\theta)^{n-\\sum_{i=1}^n x_i}.$$\n\n                        Working with $\\sum_{i=1}^n x_i$ will be a little hairy, so define $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i$. Then we have\n\n                        $$L(\\theta) = \\theta^{n\\bar{x}}(1-\\theta)^{n-n\\bar{x}}.$$\n\n                        To find the maximum likelihood estimate of $\\theta$, we take the derivative of $L(\\theta)$ with respect to $\\theta$, set it equal to zero, and solve for $\\theta$. \n                        Doing this we get\n\n                        $$\\begin{aligned}\n                        \\frac{dL(\\widehat{\\theta})}{d\\theta} = n\\bar{x}\\widehat{\\theta}^{n\\bar{x}-1}(1-\\widehat{\\theta})^{n-n\\bar{x}} - (n-n\\bar{x})\\widehat{\\theta}^{n\\bar{x}}(1-\\widehat{\\theta})^{n-n\\bar{x}-1} &= 0 \\\\\n                        \\widehat{\\theta}^{n\\bar{x}-1}(1-\\widehat{\\theta})^{n-n\\bar{x}-1}\\left(n\\bar{x}(1-\\widehat{\\theta}) - (n-n\\bar{x})\\widehat{\\theta}\\right) &= 0 \\\\\n                        \\end{aligned}$$\n\n                        Dividing both sides by the part outside the parenthesis gives us\n\n                        $$\\begin{aligned}\n                            n\\bar{x}(1-\\widehat{\\theta}) - (n-n\\bar{x})\\widehat{\\theta} &= 0 \\\\\n                            n\\bar{x}(1-\\widehat{\\theta}) &= (n-n\\bar{x})\\widehat{\\theta} \\quad\\quad\\text{(divide by $n$)} \\\\\n                            \\bar{x}(1-\\widehat{\\theta}) &= (1-\\bar{x})\\widehat{\\theta} \\quad\\quad\\;\\;\\;\\text{(distribute)} \\\\\n                            \\bar{x} - \\bar{x}\\widehat{\\theta} &= \\widehat{\\theta} - \\bar{x}\\widehat{\\theta} \\\\\n                            \\bar{x} &= \\widehat{\\theta}\n                        \\end{aligned}$$\n\n                        Recall that $\\bar{x}$ is the sample mean of our data. Thus, the maximum likelihood estimate of $\\theta$ is the sample mean of our data. \n                        \n                        This should make sense to us. We estimate the probability of success in a Bernoulli trial by getting the average number of successes in our sample.\n                        </p>\n\n                        <p>\n                            Now, one thing to note is that the math does not always work out this nice. However, there is a way around that!\n                        </p>\n\n                        <h4>Log-Likelihood!</h4>\n\n                        <p>\n                            The idea of log-likelihood is to take the natural logarithm of the likelihood function and solve for the maximum likelihood estimate of our\n                            parameter. It is important to note that this will still give us the same answer. Since the logarithm is a monotonicly increasing function (always increasing)\n                            and the likelihood function is nonnegative, the log-likelihood $\\ell(\\theta) = \\log L(\\theta)$ will achieve its maximum at the same value of $\\theta$ as the likelihood function. \n                        </p>\n\n                        <p>\n                            Let's see this in action, first with the Bernoulli distribution. We have\n\n                            $$\\ell(\\theta) = \\log L(\\theta) = \\log\\left( \\prod_{i=1}^n \\theta^{x_i}(1-\\theta)^{1-x_i} \\right).$$\n\n                            Recalling that $\\log(ab) = \\log a + \\log b$, we can rewrite the above as\n\n                            $$\\begin{aligned}\n                            \\ell(\\theta) = \\log\\left( \\prod_{i=1}^n \\theta^{x_i}(1-\\theta)^{1-x_i} \\right) &= \\log\\left(\\theta^{n\\bar{x}}(1-\\theta)^{n-n\\bar{x}} \\right) \\\\\n                            &= \\log(\\theta^{n\\bar{x}}) + \\log((1-\\theta)^{n-n\\bar{x}}) \\\\\n                            &= n\\bar{x}\\log(\\theta) + (n-n\\bar{x})\\log(1-\\theta).\n                            \\end{aligned}$$\n\n                            Taking the derivative of this with respect to $\\theta$ and setting it equal to zero gives us\n\n                            $$\\begin{aligned}\n                            \\frac{d\\ell(\\widehat{\\theta})}{d\\theta} = \\frac{n\\bar{x}}{\\widehat{\\theta}} - \\frac{n-n\\bar{x}}{1-\\widehat{\\theta}} &= 0 \\\\\n                            \\frac{n\\bar{x}}{\\widehat{\\theta}} &= \\frac{n-n\\bar{x}}{1-\\widehat{\\theta}}  \\quad\\quad(\\text{divide by } n)\\\\\n                            \\frac{\\bar{x}}{\\widehat{\\theta}} &= \\frac{1-\\bar{x}}{1-\\widehat{\\theta}} \\\\\n                            \\bar{x}(1-\\widehat{\\theta}) &= \\widehat{\\theta}(1-\\bar{x}) \\\\\n                            \\bar{x} - \\bar{x}\\widehat{\\theta} &= \\widehat{\\theta} - \\bar{x}\\widehat{\\theta} \\\\\n                            \\bar{x} &= \\widehat{\\theta}.\n                            \\end{aligned}$$\n\n                            We get the same answer as before! This is a general result, and we can use the log-likelihood to find the maximum likelihood estimate of any parameter.\n                        </p>\n\n                        <h4>Why is MLE Useful?</h4>\n\n                        <p>\n                            Earlier I mentioned that MLE is a powerful method for estimating the parameters of a probability distribution. But truthfully this is an understatement.\n                            MLE is the best method for estimating the parameters of a probability distribution. The principle of MLE is used as a foundation\n                            for many machine learning algorithms and statistical methods. \n                        </p>\n\n                        <p>\n                            Truthfully, the question should not be when is MLE useful, but when is it not useful.\n                        </p>\n\n                        \n                        <h4>Conclusion</h4>\n\n                        <p>\n                            Maximum likelihood estimation is a powerful method for estimating the parameters of a probability distribution. \n                            It provides a systematic framework for making inferences about unknown quantities, and is used as a foundation for many machine learning algorithms and statistical methods.\n                            In this blog post, we explored the concept of MLE, its mathematical underpinnings, and considered an example with the Bernoulli distribution. Stay tuned as \n                            we talk about different important ideas that use MLE!\n                        </p>\n                        \n\n                        <!-- <h4>Citations</h4> -->\n\n                        <!-- <ol>\n                            <span id=\"adams\">[1]</span> Collin C. Adams, <em>The Knot Book</em>. American Mathematical Society, 2004. ISBN: 978-0821836781.\n                        </ol>\n                        <ol\n                            <span id=\"birman\">[2]</span> Joan S Birman. <em>Braids, links, and mapping class groups</em>. 82. Princeton University Press, 1974.\n                        </ol>    -->\n\n                        <!-- <ol>\n                            <span id=\"seifert\">[1]</span> Heinrich Seifert. <em>\u00dcber das Geschlecht von Knoten</em>. In: <em>Mathematische Annalen</em> 110 (1935), pp. 571\u2013592. DOI: 10.1007/BF01448044.\n                        </ol> -->\n\n                        <!-- <p>\n                            <a id=\"adams\"></a>Adams, C. C. (1994). The Knot Book: An Elementary Introduction to the Mathematical Theory of Knots. W. H. Freeman and Company. -->",
      "quote": "And when is there time to remember, to sift, to weigh, to estimate, to total?",
      "quoteAuthor": "Tillie Olson",
      "category": "Math & ML"
    },
    {
      "slug": "one-factor-anova-r",
      "title": "Creating One-Factor ANOVA Tables in R",
      "date": "2022-04-16",
      "description": "ANOVA tables can be very useful to have, but rather difficult to create. This blog post gives an easy tutorial on creating ANOVA tables in R without using any extra packages.",
      "content": "<h4>Getting started</h4>\n                        <p>Creating ANOVA tables can help us better understand the relationship and interactions between variables. However, creating the ANOVA table can pose some difficulties. The purpose of this blog post is to help you create one-factor ANOVA tables in R quickly, easily, and without any extra packages! </p>\n                        <p><bold>This article assumes you already know what ANOVA tables are. I will not be going into any of the math behind ANOVA tables.</bold></p>\n                        <h4>Motivating Examples</h4>\n                        <p>I want to start this off with an example from the article <a href=\"https://www.sciencedirect.com/science/article/pii/S0925857411000802\">\u201cNutrient Deprivation Improves Field Performance of Woody Seedlings in a Degraded Semi-arid Shrubland\u201d (R. Trubata, J. Cortina, and A. Vilagrosaa, Ecological Engineering, 2011:1164\u20131173)</a>. One part of this article looks at the effect of three different types of fertilizers on the height of a specific Mediterranean tree species. One experiment the group conducts takes three samples of 10 different trees, with each sample being grown with a different fertilizer. One group\u2014the control group\u2014was grown with a standard fertilizer, a second group was grown with a fertilizer that contained half the nutrients of the standard fertilizer, and the third group was grown using a commercial slow-release fertilizer. After one year, the heights of the trees were measured and are listed in the following table.</p>\n                        <img src=\"blog_files/creating_anova/tree_height_picture.png\" />\n                        <figcaption>Data table courtesy of <a href=\"https://www.mheducation.com/highered/product/statistics-engineers-scientists-navidi/M9781259717604.html\">William Navidi, \u201cStatistics for Engineers and Scientists,\u201d 5th edition (page 688\u20139)</a>.</figcaption>\n                        <p>This leads us to our research question: does fertilizer type affect the height of these Mediterranean trees?</p>\n                        <p>One statistical tool we can use to analyze the data and figure out the answer to our research question is called <bold>An</bold>alysis <bold>o</bold>f <bold>Va</bold>riance (ANOVA). The idea being to use an ANOVA test is to compare the variance in means of level (fertilizer type), to the variance that occurs by change (variance of errors). Now, you can compute this ANOVA table by hand, but it is long, tedious, and simply unnessary. Instead, we can use R!</p>\n                        \n                        <h4>Calculating ANOVA Tables in R</h4>\n                        <p>Everything I am doing in the following steps can be done in the terminal of R.</p>\n                        <ol>\n                            <li>The first thing we need to do is read in our data.</li>\n                            <pre>\n                                <code class=\"language-R\">\nControl <- c(17.9, 12.2, 14.9, 13.8, 26.1, 15.4, 20.3, 16.9, 20.8, 14.8)\nDeficient <- c(7.0, 6.9, 13.3, 11.1, 11.0, 16.5, 12.7, 12.4, 17.1, 9.0)\nSlow_release <- c(19.8, 20.3, 16.1, 17.9, 12.4, 12.5, 17.4, 19.9, 27.3, 14.4)\n                                </code>\n                            </pre>\n\n                            <li>Next, we want to put these variables into a dataframe</li>\n                            <code class=\"language-R\">height_dataset <- data.frame(Control, Deficient, Slow_release)</code>\n                            <p>This will give us a nice data frame that looks like this:</p>\n                            <img src=\"blog_files/creating_anova/height_dataset.png\" />\n\n                            <li>To help prepare the data for the ANOVA table, we will want to stack the data together using the <code class=\"language-R\">stack()</code> function</li>\n                            <code class=\"language-R\">stack_height_dataset <- stack(height_dataset)</code>\n                            <p>When we stack our dataset together, we see that our data frame turns into a two column frame where the first column has the heights, and the second column as the three types of fertilizer. But, the column headers do not fully represent this.</p>\n                            <img src=\"blog_files/creating_anova/stack_height_dataset.png\"/>\n                            <figcaption>This data frame has 30 rows, but these 21 should help you get the idea.</figcaption>\n\n                            <li>To make things easier for later, rename the column headers to accurately represent what the columns are showing.</li>\n                            <code class=\"language-R\">names(stack_height_dataset) <- c(\"Height\", \"Method\")</code>\n                            <p>If we look at our data frame again, we will see that the column headers are renamed and accurately represent what we are looking at.</p>\n                            <img src=\"blog_files/creating_anova/stack_height_dataset_renamed.png\"/>\n                            <figcaption>We see that the column headers are renamed.</figcaption>\n\n                            <li>With our renamed column headers, we will now want to turn our data frame into a linear model using the <code class=\"language-R\">lm()</code> function in R.</li>\n                            <code class=\"language-R\">height_fit <- lm(Height~Method, data=stack_height_dataset)</code>\n                            <p>If you look at our newly created height_fit linear model, we see a few weird things. Do not worry too much about this. This is all information that our next step requires.</p>\n                            <img src=\"blog_files/creating_anova/height_fit.png\"/>\n                            <figcaption>There is a lot of information here, but do not worry about it if you are new! It is not super important for you to understand right now.</figcaption>\n\n                            <li>Final step! All of our data preparation has brought us up to this point! All we need to do is pass our <code class=\"language-R\">height_fit</code> linear model into the <code class=\"language-R\">anova()</code> function that R provides.</li>\n                            <code class=\"language-R\">anova(height_fit)</code>\n                            <p>When you run this function, R will automatically output the ANOVA table will all the information we need!</p>\n                            <pre>\n                                <code class=\"language-R\">\nAnalysis of Variance Table\n\nResponse: Height\n        Df Sum Sq Mean Sq F value  Pr(>F)   \nMethod     2 229.74 114.870  7.0587 0.00342 **\nResiduals 27 439.39  16.274                   \n---\nSignif. codes:  0 \u2018***\u2019 0.001 \u2018**\u2019 0.01 \u2018*\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1\n                                </code>\n                            </pre>\n                        </ol>\n                            \n                        <h4>Conclusion</h4>\n                        <p>That is it! If you recall our original research question, we wanted to know if fertilizer type impacts the height of these trees. If you recall from your statistics class, we compare the p-value with our significance value, $\\alpha$. In this case, we used a significance $\\alpha$ = 0.05 (which is pretty standard). Looking at our ANOVA table, we see that our p-value = 0.00342. Comparing our p-value and our significance level, we see that p-value = 0.00342 < 0.05 = $\\alpha$, so we reject the null hypothesis and conclude that the fertilizer does have an impact on the height of these Mediterranean trees!</p>\n                        <p>(If you are interested in finding the critical value of our experiment to compare with the F-value found on our ANOVA table, use the R function <code>qf()</code>. All you need is a significance level (1-$\\alpha$), df1, and df2 where df1 is the degrees of freedom of the Method, and df2 is the degrees of freedom of the Residuals (simply look at the Df column of our ANOVA table).)</p>\n                        <p>And that is it! Creating One-Factor ANOVA tables in R is super easy! Make sure to check out the original study that inspired our motivating example, and the textbook that helped provide the data table for our experiment!</p>\n\n                        <h4>Example Fully Worked Out</h4>\n                        <p>Here is a picture of the full code in an R console.</p>\n                        <img href=\"blog_files/creating_anova/full_example_terminal.png\" />\n                        <figcaption>Here is a picture of the full code in an R console.</figcaption>\n                        <p>Here is a code block of the full code in an R console.</p>\n                        <pre>\n                            <code class=\"language-R\">\n> Control <- c(17.9, 12.2, 14.9, 13.8, 26.1, 15.4, 20.3, 16.9, 20.8, 14.8)\n> Deficient <- c(7.0, 6.9, 13.3, 11.1, 11.0, 16.5, 12.7, 12.4, 17.1, 9.0)\n> Slow_release <- c(19.8, 20.3, 16.1, 17.9, 12.4, 12.5, 17.4, 19.9, 27.3, 14.4)\n> \n> height_dataset <- data.frame(Control, Deficient, Slow_release)\n> View(height_dataset)\n> stack_height_dataset <- stack(height_dataset)\n> \n> names(stack_height_dataset) <- c(\"Height\", \"Method\")\n>\n> height_fit <- lm(Height~Method, data=stack_height_dataset)\n>\n> anova(height_fit)\nAnalysis of Variance Table\nResponse: Height\n        Df Sum Sq Mean Sq F value  Pr(>F)   \nMethod     2 229.74 114.870  7.0587 0.00342 **\nResiduals 27 439.39  16.274                   \n---\nSignif. codes:  0 \u2018***\u2019 0.001 \u2018**\u2019 0.01 \u2018*\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1\n                            </code>\n                        </pre>",
      "quote": "\u201d 'interaction' in contingency tables enjoys only a few of the fortuitously simple properties of interactions in the analysis of variance.\u201d",
      "quoteAuthor": "John Darroch",
      "category": "Math & ML"
    },
    {
      "slug": "pandas-thebasics",
      "title": "Pandas: The Basics",
      "date": "2023-04-15",
      "description": "",
      "content": "<h4>Introduction</h4>\n                        <p>While preparing for my summer internship, I decided to learn more about the Python package \n                            <a href=\"https://pandas.pydata.org\" target=\"_blank\" rel=\"noopener noreferrer\">Pandas</a>.\n                            Pandas is a very powerful tool for data analysis and manipulation. It is built on top of NumPy,\n                            and is one of the most popular Python packages for data science. I have used Pandas in the past,\n                            but I wanted to learn more about it and get a better understanding of how it works. This post\n                            will cover the basics of Pandas, including data structures, reading in data, and manipulating data.\n                        </p>\n\n                        <h4>Pandas Data Structures</h4>\n\n                        <p>Pandas has two main data structures: Series and DataFrames. A Series is a one-dimensional array\n                            of indexed data (very similar to a NumPy ndarray). A DataFrame is a two-dimensional array of indexed data. A DataFrame is essentially\n                            a collection of Series objects. The index of a Series or DataFrame is a set of labels that allow\n                            you to access the data. The index can be a list of integers, strings, or dates. The data in a Series\n                            or DataFrame can be any NumPy data type. The following code shows how to create a Series and a DataFrame.\n                        </p>\n\n                        <pre>\n                            <code class=\"language-python\">\n# Import the pandas package\nimport pandas as pd\n\n# Create a Series of flowers with their heights and colors\nheights = pd.Series([3, 2, 4, 1], index=['sunflower', 'rose', 'tulip', 'daisy'])\n\ncolors = pd.Series(['yellow', 'red', 'pink', 'white'], index=['sunflower', 'rose', 'tulip', 'daisy'])\n\n# Create a DataFrame of flowers with their heights and colors\nflowers = pd.DataFrame({'height': heights, 'color': colors})\n                            </code>\n                        </pre>\n\n                        <p>As you can see, we create a Series by doing <code class=\"language-python\">pd.Series()</code>. We pass in a list of data and a list of index labels.\n                            One thing to note is that the argument <code>index</code> is optional. You can omit that and get the same results. One we have our Series, we can now\n                            create our DataFrame.\n                            We can do this by passing in a dictionary of Series objects. The keys of the dictionary are the column names, and the values are the Series.\n                            We can also create a DataFrame by passing in a list of lists. The first list is the column names, and the rest of the lists are the rows of data.\n                        </p>\n                        \n                        <p>\n                            If we were to print out our Series and DataFrame, we would get the following:\n                        </p>\n                        <pre>\n                            <code class=\"language-python\">\nprint(heights)\n>>> sunflower    3\n>>> rose         2\n>>> tulip        4\n>>> daisy        1\n>>> dtype: int64\n\nprint(colors)\n>>> sunflower    yellow\n>>> rose            red\n>>> tulip          pink\n>>> daisy         white\n>>> dtype: object\n\nprint(flowers)\n>>>              height   color\n>>> sunflower       3     yellow\n>>> rose            2     red\n>>> tulip           4     pink\n>>> daisy           1     white\n                            </code>\n                        </pre>\n\n                        <p>From this we can see that <code>pd.DataFrame</code> lines up our information nicely. If one element of our Series\n                            is not present in the other Series, Pandas will fill that value with <code>NaN</code>.\n                        </p>\n\n                        <p>Once you have all your information in a DataFrame, if you want to get the information as a NumPy array, you can simply call\n                            <code>flowers.values</code>. This will return a two-dimensional array of the data. If you want to get the index labels, you can call\n                            <code>flowers.index</code>. This will return a list of the index labels. If you want to get the column names, you can call\n                            <code>flowers.columns</code>. This will return a list of the column names, as seen below.\n                        </p>\n                        \n                        <pre>\n                            <code class=\"language-python\">\nprint(flowers.values)\n>>>[[3 'yellow']\n>>> [2 'red']\n>>> [4 'pink']\n>>> [1 'white']]\n\nprint(flowers.index)\n>>> Index(['sunflower', 'rose', 'tulip', 'daisy'], dtype='object')\n\nprint(flowers.columns)\n>>> Index(['height', 'color'], dtype='object')\n                            </code>\n                        </pre>\n                        \n                        <h4>Reading in Data</h4>\n\n                        <p>One very nice thing about Pandas is that it has functions that make reading in data very easy. Pandas can read in data from\n                            CSV files, Excel files, HTML tables, JSON files, SQL databases, and more. For this post, we will focus on reading in CSV files.\n                            To read in a CSV file, we can use the <code>pd.read_csv()</code> function. This function has many arguments, but the only required\n                            argument is the file path. Another potentially helpful argument is <code>index_col</code>. This argument allows you to specify which\n                            column you want to use as the index. If you do not specify this argument, Pandas will create a new index for you.\n                        </p>\n\n                        <p>Say you have the following CSV file called <code>nhl_teams.csv</code>:</p>\n\n                        <table class=\"center\">\n                            <tr>\n                                <th style=\"text-align:center\">Team</th>\n                                <th style=\"text-align:center\">Stadium Name</th>           \n                                <th style=\"text-align:center\">Stanley Cups Won</th>                       \n                            </tr>\n                            <tr>\n                                <td style=\"text-align:center\">Dallas Stars</td>\n                                <td style=\"text-align:center\">American Airlines Center</td>\n                                <td style=\"text-align:center\">1</td>\n                            </tr>\n                            <tr>\n                                <td style=\"text-align:center\">New Jersey Devils</td>\n                                <td style=\"text-align:center\">Prudential Center</td>\n                                <td style=\"text-align:center\">3</td>\n                            </tr>\n                            <tr>\n                                <td style=\"text-align:center\">Arizona Coyotes</td>\n                                <td style=\"text-align:center\">Gila River Arena</td>\n                                <td style=\"text-align:center\">0</td>\n                            </tr>\n                            <tr>\n                                <td style=\"text-align:center\">Carolina Hurricanes</td>\n                                <td style=\"text-align:center\">PNC Arena</td>\n                                <td style=\"text-align:center\">1</td>\n                            </tr>\n                            <tr>\n                                <td style=\"text-align:center\">Seattle Kraken</td>\n                                <td style=\"text-align:center\">Climate Pledge Arena</td>\n                                <td style=\"text-align:center\">0</td>\n                            </tr>\n                        </table>\n\n                        <p>You could read the data in without the index tag:</p>\n\n                        <pre>\n                            <code class=\"language-python\">\n# Read in nhl_teams.csv (without index_col)\nnhl_teams = pd.read_csv('nhl_teams.csv')\n\n# Print nhl_teams\nprint(nhl_teams)\n>>>             Team\t        Stadium\t               Stanley Cups\n>>> 0\tDallas Stars\t     American Airlines Center\t  1\n>>> 1\tNew Jersey Devils    Prudential Center\t          3\n>>> 2\tArizona Coyotes\t     Gila River Arena\t          0\n>>> 3\tCarolina Hurricanes  PNC Arena\t                  1\n>>> 4\tSeattle Kraken\t     Climate Pledge Arena\t  0\n\n                            </code>\n                        </pre>\n\n                        <p>\n                            But, if you read it in with the index tag set to <code>Team\"</code>, you would get the following:\n                        </p>\n\n                        <pre>\n                            <code class=\"language-python\">\n# Read in nhl_teams.csv (with index_col)\nnhl_teams = pd.read_csv('nhl_teams.csv', index_col='Team')\n\n# Print nhl_teams\nprint(nhl_teams)\n>>>                                 Stadium             Stanley Cups\n>>>     Team                                                       \n>>> Dallas Stars         American Airlines Center             1\n>>> New Jersey Devils           Prudential Center             3\n>>> Arizona Coyotes              Gila River Arena             0\n>>> Carolina Hurricanes                 PNC Arena             1\n>>> Seattle Kraken           Climate Pledge Arena             0\n                            </code>\n                        </pre>\n\n                        <p>Our data frame now only has Stadium and Stanley Cups as columns, while the team names\n                            are the indices. This is a much more useful way to organize the data as we will see in\n                            the next section.\n                        </p>\n\n                        <h4>Manipulating Data</h4>\n\n                        <p>Now that we have our data in a DataFrame, we can manipulate it in many different ways. We can\n                            add columns, remove columns, add rows, remove rows, and more. We can also perform calculations\n                            on the data.\n                        </p>\n\n                        <p>Generally, we want to access the data in our Series or DataFrame through the  methods <code>loc</code>\n                        and <code>iloc</code>. <code>loc</code> is used to access data by label, while <code>iloc</code> is used to access data by\n                        integer position. For example, if we wanted to access the data for the Dallas Stars, we could use either\n                        <code>loc</code> or <code>iloc</code>:     \n                        </p>\n\n                        <pre>\n                            <code class=\"language-python\">\n# Accessing information about the Dallas Stars using loc\nprint(nhl_teams.loc['Dallas Stars'])\n>>> Stadium         American Airlines Center\n>>> Stanley Cups                           1\n>>> Name: Dallas Stars, dtype: object\n\n# Accessing information about the Dallas Stars using iloc\nprint(nhl_teams.iloc[0])\n>>> Stadium         American Airlines Center\n>>> Stanley Cups                           1\n>>> Name: Dallas Stars, dtype: object\n                            </code>\n                        </pre>\n\n                        <p>\n                            The information presented using both methods is the same. The only difference\n                            is that <code>loc</code> uses the label <code>Dallas Stars</code> to access the data, while <code>iloc</code> uses the integer\n                            position <code>0</code> to access the data. \n                        </p>\n\n                        <p>\n                            You can also look up multiple rows at once using <code>loc</code> and <code>iloc</code>. For example, if we wanted to\n                            look up the data for the Dallas Stars and the New Jersey Devils, we could use either <code>loc</code> or <code>iloc</code>:\n                        </p>\n\n                        <pre>\n                            <code class=\"language-python\">\n# Accessing information about the Dallas Stars and New Jersey Devils using loc\nprint(nhl_teams.loc[['Dallas Stars', 'New Jersey Devils']])\n>>>                                Stadium       Stanley Cups\n>>> Team                                                     \n>>> Dallas Stars       American Airlines Center             1\n>>> New Jersey Devils         Prudential Center             3                           \n\n# Accessing information about the Dallas Stars and New Jersey Devils using iloc\nprint(nhl_teams.iloc[[0, 1]])\n>>>                                 Stadium      Stanley Cups\n>>> Team                                                     \n>>> Dallas Stars       American Airlines Center             1\n>>> New Jersey Devils         Prudential Center             3\n                            </code>\n                        </pre>\n\n                        <p>Note the double brackets <code>[[]]</code> in this segment.\n                            But, similar to the previous code block, the results are the same.</p>\n\n                        <p>If you want to look up a specific column, insidee the brackets, you can add the name of the column\n                            you want to look up. For example, if we wanted to look up the stadium for the Dallas Stars, we could\n                            use either <code>loc</code> or <code>iloc</code>:\n                         </p>\n\n                        <pre>\n                            <code class=\"language-python\">\n# Accessing the stadium for the Dallas Stars using loc\nprint(nhl_teams.loc['Dallas Stars', 'Stadium'])\n>>> American Airlines Center\n\n# Accessing the stadium for the Dallas Stars using iloc\nprint(nhl_teams.iloc[0, 0])\n>>> American Airlines Center\n                            </code>\n                        </pre>\n                        \n                        <p>\n                            Now let's say you have a new column, Current Wins, you want to add to the DataFrame. \n                            With Pandas, this is super easy!\n                        </p>\n\n                        <pre>\n                            <code class=\"language-python\">\n# Add a new column to the DataFrame\nnhl_teams['Current Wins'] = [1, 2, 3, 4, 5]\n\n# Print the DataFrame to see the new column\nprint(nhl_teams)\n>>>                                 Stadium     Stanley Cups  Current Wins\n>>> Team                                                                     \n>>> Dallas Stars        American Airlines Center           1         1\n>>> New Jersey Devils          Prudential Center           3         2\n>>> Arizona Coyotes             Gila River Arena           0         3\n>>> Carolina Hurricanes                PNC Arena           1         4\n>>> Seattle Kraken          Climate Pledge Arena           0         5\n                            </code>\n                        </pre>\n                    \n                    <p>Since Pandas is build on top of NumPy and utilized NumPy arrays, we can also perform calculations on the data in a way similar to NumPy.\n                        For example, if we wanted to add the number of Stanley Cups and Current wins together\n                        and triple the number of Stanley Cups for each team, we could do:\n                    </p>\n                    \n                    <pre>\n                        <code class=\"language-python\">\n# Add the number of Stanley Cups and Current Wins together\nnhl_teams['Stanley Cups'] + nhl_teams['Current Wins']\n>>> Team\n>>> Dallas Stars           2\n>>> New Jersey Devils      5\n>>> Arizona Coyotes        3\n>>> Carolina Hurricanes    5\n>>> Seattle Kraken         5\n>>> dtype: int64\n\n# Triple the number of Stanely Cups\nnhl_teams['Stanley Cups'] * 3\n>>> Team\n>>> Dallas Stars           3\n>>> New Jersey Devils      9\n>>> Arizona Coyotes        0\n>>> Carolina Hurricanes    3\n>>> Seattle Kraken         0\n>>> Name: Stanley Cups, dtype: int64\n                        </code>\n                    </pre>\n\n                    <p>There are many more ways to manipulate the data in a DataFrame. Most other methods are very similar to NumPy arrays. For\n                        example you can use <code>abs()</code> to get the absolute value, <code>sum()</code> to get the sum of the data, <code>mean()</code> \n                        to get the mean of the data.\n                        Two particularly useful methods are <code>idxmax()</code> and <code>idxmin()</code>. These methods return the index of the maximum and minimum values in the data.\n                    </p>\n\n                    <p>\n                        In addition to these methods, there is also <code>describe()</code> which gives basic summary statistics\n                        about the data in the DataFrame. In our example we have:\n                    </p>\n\n                    <pre>\n                        <code class=\"language-python\">\n# Print the summary statistics for the DataFrame\nprint(nhl_teams.describe())\n>>>        Stanley Cups  Current Wins\n>>> count      5.000000      5.000000\n>>> mean       1.000000      3.000000\n>>> std        1.224745      1.581139\n>>> min        0.000000      1.000000\n>>> 25%        0.000000      2.000000\n>>> 50%        1.000000      3.000000\n>>> 75%        1.000000      4.000000\n>>> max        3.000000      5.000000                    \n                        </code>\n                    </pre>\n\n                    <p>This information is not particularly exciting or useful in our toy example, but can be very useful when doing\n                        beginning stages data exploration.\n                    </p>\n\n                    <p>\n                        The final piece of data manipulation I want to talk about today is masks. Masks are a way to filter the data in a DataFrame showing\n                        only the information we are looking for. For example, say we want to find the teams with one or more Stanley Cup. We can do this by masking!\n                    </p>\n\n                    <pre>\n                        <code class=\"language-python\">\n# Create the mask to get teams with one or more Stanley Cup.\nmask = nhl_teams['Stanley Cups'] >= 1\n\n# Apply mask to the DataFrame\nprint(nhl_teams[mask])\n>>>                                       Stadium  Stanley Cups  Current Wins\n>>> Team                                                                     \n>>> Dallas Stars         American Airlines Center             1             1\n>>> New Jersey Devils           Prudential Center             3             2\n>>> Carolina Hurricanes                 PNC Arena             1             4\n                        </code>\n                    </pre>\n\n                    <p> This is good, but sometimes this can give us more information than we are looking for.\n                        Say for example we only wanted the team names and current wins of the teams that have won one or more Stanley Cup. We can do this, still with masking,\n                        quite easily. Simply take your DataFrame, apply the mask to it, then, in brackets, put the columns you want to see.\n                    </p>\n\n                    <pre>\n                        <code class=\"language-python\">\n# Apply mask and only get team names and current wins.\nprint(nhl_teams[mask]['Current Wins'])\n>>> Team\n>>> Dallas Stars           1\n>>> New Jersey Devils      2\n>>> Carolina Hurricanes    4\n>>> Name: Current Wins, dtype: int64\n                        </code>\n                    </pre>\n\n                    <h4>Conclusion</h4>\n                    There is WAY more to Pandas than mentioned here, but I hope \n                    this article gave you a taste of the power that Pandas has to offer!\n                    Now that you know a bit about Pandas, I invite you to read the documentation and\n                    try exploring some data for yourself!",
      "quote": "No data is clean, but most is useful.",
      "quoteAuthor": "Dean Abbott",
      "category": "Math & ML"
    },
    {
      "slug": "ppo",
      "title": "Proximal Policy Optimization: A Scenic Tour",
      "date": "2024-03-13",
      "description": "",
      "content": "<p>\n                           Proximal policy optimization (PPO) is a deep reinforcement learning (rl) algorithm that is <em>quite</em> good. In this blog post, we\n                           will dive into PPO, specifically breaking down the cost function (which is rather hairy).\n                        </p>\n\n                        <figure>\n                            <img src=\"blog_files/ppo/river_intro.jpg\" alt=\"A river.\" width=\"90%\" height=\"90%\">\n                            <figcaption text-align=center>Photo by <a href=\"https://unsplash.com/@martinsanchez?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash\">Martin Sanchez</a> on <a href=\"https://unsplash.com/photos/landscape-photography-of-river-between-green-mountains-ycG0A6DlvOk?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash\">Unsplash</a>\n  \n                            </figcaption>\n                            \n                        </figure>\n                        \n                        <p></p>\n\n                        <h4>\n                            What is PPO?\n                        </h4>\n\n                        <p>\n                            If you recall from my previous blog post on the <a href=\"/blog/mdp_bo.html\">Markov Decision Process</a>, \n                            we discussed how an agent interacts with an environment. We also discussed that one of the most active fields\n                            of research in reinforcement learning is how to train an agent to make the best decisions possible; how to find $\\pi^*$. PPO is\n                            one of the many algorithms that has been developed to help us find $\\pi^*$, and it is currently the state-of-the-art!\n                        </p>\n\n                        <p>\n                            PPO is a policy-based algorithm, meaning that it learns a policy $\\pi$ that maps states to actions. The policy is\n                            learned by interacting with the environment and updating the policy based on the rewards received.\n                        </p>\n\n                        <p>\n                            Additionally, PPO seeks to strike a balance between ease of implementation, sample complexity, and ease of tuning; \n                            all of which posed challenges to earlier algorithms. PPO does this \n                            by trying to compute an update at each step that both minimizes the cost function and only slightly deviates from \n                            the previous policy. This ensures that the agent does not take too big of steps and goes off track, but does not \n                            take steps that are too small which may lead to the agent going nowhere.\n                        </p>\n\n                        <h4>The Loss Function</h4>\n\n                        <p>\n                            In order for this to work, the algorithm utilizes two separate policy neural networks\u2014the current policy $\n                            \\pi_{\\theta}(a_{t}|s_{t})$, and the older policy $\\pi_{\\theta_{k}}(a_{t}|s_{t})$\u2014and a rather unique objective function:\n\n $$L^{CLIP}(\\theta) = \\widehat{E}_{t}\\Bigl[\\text{min}\\bigl( r_{t}(\\theta)\\widehat{A}_{t}, \\text{clip}(r_{t}(\\theta), 1 - \\varepsilon, 1+\\varepsilon)\\widehat{A}_{t}\\bigr) \\Bigr],$$\n                        </p>\n\n                        <p>\n                            where\n\n                            <ul>\n                                <li>$\\theta$ is the policy parameter,</li>\n                                <li>$\\widehat{E}_{t}$ is the expected value (calculated by taking the average over a sequence of actions),</li>\n                                <li>$r_{t}(\\theta)$ is the probability ratio, or the ratio of the current policy over the older policy, \n                                    $\\frac{\\pi_{\\theta}(a_{t}|s_{t})}{\\pi_{\\theta_{k}}(a_{t}|s_{t})}$. \n                                    If $r_t(\\theta) > 1$, it indicates that the new policy has a higher probability of selecting \n                                    $a_t$ than the old policy. If it is less than 1, the new policy has a lower probability,</li>\n                                <li>$\\widehat{A}_{t}$ is the estimated advantage at time step $t$, calculated as $\\hat{A_{t}} = R_{t} - V(s_{t})$, where $R_{t}$ \n                                    is the reward from the most recent action, and $V(s_{t})$ is the estimate of return starting from current state $s_t$, and</li>\n                                <li>$\\varepsilon$ is a hyperparameter setting the size of the epsilon-neighborhood for step size.</li>\n                            </ul>\n                        </p>\n\n                        \n                        <p>\n                            One thing that makes this loss function interesting are the components inside of the minimization function. \n                            The first part, $r_{t}(\\theta)\\hat{A_{t}}$, is simply the probability ratio times the advantage. This is \n                            done to determine how much to update the policy for a specific action in a specific state as it quantifies \n                            the advantage of the action $a_t$ taken in state $s_t$ and its relative likelihood under the new and old policies. \n                            The second part, $clip(r_{t}(\\theta), 1-\\epsilon, 1+\\epsilon)\\hat{A_{t}}$, is a little bit different. If our new policy has a \n                            much higher probability of selecting $a_t$ than our old policy, then $r_t(\\theta) \\gg 1$. Similarly, if our new policy has \n                            a much lower probability of selecting $a_t$ than our old policy, we have $r_t(\\theta) \\ll 1$. This can be an issue because \n                            it can cause our algorithm to take a step too far in the wrong direction, potentially ruining its learning. The $\\text{clip}$ \n                            function ensures all of our steps stay in a specified range. More specifically, if \n                            $r_t(\\theta) < 1 - \\epsilon$ or $r_t(\\theta) > 1 + \\epsilon$, the new value of $r_t(\\theta)$ becomes $1 - \\epsilon$ or $1 + \\epsilon$, respectively. </em>\n                        </p>\n\n                        <p>\n                            Once $r_t(\\theta)$ and $clip(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)$ are calculated, $L^{CLIP}$ selects the minimum of the two, \n                            and then finds the expected value of that result, thus allowing us to find the optimal step size.\n                        </p>\n\n                        <h4>The Algorithm</h4>\n\n                        <p>\n                            Let's now discuss how PPO works. The algorithm is quite simple, and can be broken down into the following steps:\n                        </p>\n\n                        <figure>\n                            <img src=\"blog_files/ppo/ppo_algorithm.png\" alt=\"The pseudocode for the PPO algorithm.\" width=\"100%\" height=\"100%\">\n                            <figcaption text-align=center>The pseudocode for the PPO algorithm.\n                            </figcaption>\n                        </figure>\n\n                        <p>\n                            When the PPO runs, it starts off by running the old policy $\\pi_{\\theta_{k}}$, $N$ times for $T$ time steps. \n                            For each $n \\in N$ iterations, the advantage $\\hat{A_{t}}$ is calculated for each $t \\in T$. Once the \n                            $N$ iterations are over, $L^{CLIP}$ is optimized with respect to $\\theta$, and then $\\pi_{\\theta_{k}} \\leftarrow \\pi_{\\theta}$.\n                        </p>\n\n                        <p>\n                            In addition to these two policy networks, PPO also utilizes a value network. \n                            The value network is another neural network model used to estimate the expected \n                            cumulative reward (value) associated with a given state. It is used to assess \n                            the quality of states and provide feedback for policy improvement. There is only \n                            one of this network--not two networks like with the policy network.\n                        </p>\n\n                        <h4>Coding PPO</h4>\n\n                        <p>\n                            One of the best ways to understand an algorithm is to code it. Below is a simple implementation of the above algorithm using Pytorch \n                            (with the above loss function).\n                        </p>\n\n<pre><code language=\"python\">def learn_ppo(optim, policy, value, memory_dataloader, epsilon, policy_epochs):\n    \"\"\"Implement PPO policy and value network updates. Iterate over your entire \n       memory the number of times indicated by policy_epochs (policy_epochs = 5).    \n  \n      Args:\n          optim (Adam): value and policy optimizer\n          policy (PolicyNetwork): Policy Network\n            policy_network = PolicyNetwork(state_size, action_size).cuda()\n          value (ValueNetwork): Value Network\n            value_network = ValueNetwork(state_size).cuda()\n          memory_dataloader (DataLoader): dataloader with (state, action, action_dist, return) tensors\n          epsilon (float): trust region\n          policy_epochs (int): number of times to iterate over all memory\n    \"\"\"\n    # Go through all epochs\n    for epoch in range(policy_epochs):\n      for batch in memory_dataloader:\n        optim.zero_grad()\n        \n        # Get the all variables from memory (by batch)\n        state, action, action_dist, return_v = batch\n\n        # Do state.stack because we have a list of tensors, and we need a tensor of stuff.\n        state = torch.stack(state, dim=1)      \n  \n        state_t       = state.type(torch.FloatTensor).cuda()\n        action_t      = action.type(torch.LongTensor).cuda()\n        action_dist_t = action_dist.type(torch.FloatTensor).cuda()\n        return_v_t    = return_v.type(torch.FloatTensor).cuda()\n  \n        # Calculate advantage \u00c2\n        advantage = (return_v_t - value(state_t).cuda()).detach()\n  \n        # Calculate value loss\n        value_loss = F.mse_loss(return_v_t, value(state_t).squeeze())\n  \n        # Calculate \u03c0(s,a)\n        policy_norm       = action_dist_t.squeeze(1)\n        action_onehot     = F.one_hot(action_t, 13).bool()\n        taken_policy_norm = policy_norm[action_onehot]\n        \n        #Calculate \u03c0'(s,a)\n        policy_prime       = policy(state_t)\n        taken_policy_prime = policy_prime[action_onehot]\n        \n        #Calculate the ratio between \u03c0'(s,a)/\u03c0(s,a)\n        prim_div_norm = taken_policy_prime / taken_policy_norm\n  \n        # Clipping \u03c0'(s,a)/\u03c0(s,a)\n        clip = torch.clip(prim_div_norm, 1-epsilon, 1+epsilon)\n        \n        # Left part of the policy loss (\u03c0'(s,a)/\u03c0(s,a))*\u00c2\n        left_part = prim_div_norm * advantage\n\n        # Right part of the policy loss clip*\u00c2\n        right_part = clip * advantage\n  \n        # Calculating policy loss\n        policy_loss = torch.mean(torch.min(left_part, right_part))\n  \n        # Total loss\n        total_loss = value_loss - policy_loss\n        \n        total_loss.backward()\n        optim.step()</code></pre>\n\n                        <p>\n                            (To see a more complete implementation, check out my \n                            <a href=\"https://github.com/dylanskinner65/DeepRLKnots/blob/master/knot_gpu_Large.py\">Github</a> where I have implemented PPO to solve a 4D topology problem.)\n                        </p>\n\n\n                        <h4>Conclusion</h4>\n\n                        <p>\n                            In this blog post, we discussed the PPO algorithm and its loss function. We also discussed how to implement PPO in Pytorch. PPO\n                            is a powerful algorithm that has been used to solve a variety of problems, from playing video games to solving 4D topology problems!\n                            I encourage you to try implementing PPO in your own projects, and see how it can help you solve your problems. (I also invite you to\n                            check out the original <a href=\"https://arxiv.org/abs/1707.06347\">PPO paper</a> to learn more about the weeds of this algorithm.)   \n                        </p>\n\n                        <!-- <h4>Citations</h4> -->\n\n                        <!-- <ol>\n                            <span id=\"adams\">[1]</span> Collin C. Adams, <em>The Knot Book</em>. American Mathematical Society, 2004. ISBN: 978-0821836781.\n                        </ol>\n                        <ol\n                            <span id=\"birman\">[2]</span> Joan S Birman. <em>Braids, links, and mapping class groups</em>. 82. Princeton University Press, 1974.\n                        </ol>    -->\n\n                        <!-- <ol>\n                            <span id=\"seifert\">[1]</span> Heinrich Seifert. <em>\u00dcber das Geschlecht von Knoten</em>. In: <em>Mathematische Annalen</em> 110 (1935), pp. 571\u2013592. DOI: 10.1007/BF01448044.\n                        </ol> -->\n\n                        <!-- <p>\n                            <a id=\"adams\"></a>Adams, C. C. (1994). The Knot Book: An Elementary Introduction to the Mathematical Theory of Knots. W. H. Freeman and Company. -->",
      "quote": "The biggest obstacle to creativity is breaking through the barrier of disbelief.",
      "quoteAuthor": "Rodney Mullen",
      "category": "Math & ML"
    },
    {
      "slug": "prime-spiral",
      "title": "Prime Spiral in Python",
      "date": "2023-04-15",
      "description": "",
      "content": "<h4>Why I Tried This</h4>\n                        <p>I have always been fascinated with prime numbers. I was born on the 23rd (prime), in 1999 (prime) and have always been interested\n                            in some of the characteristics of primes and the weird things they do. I remember in high school, I\n                            bought a book called 'Prime Obsession' by John Derbyshire. It was a fascinating book about the history of prime numbers and\n                            the people who have studied them. I have always wanted to do something with prime numbers, but I never knew what.\n                        </p>\n\n                        <p>Yesterday, a friend of mine sent me a <a href=\"https://www.tiktok.com/t/ZTR3CuBua/\" target=\"_blank\" rel=\"noopener noreferrer\">video</a> from the 3blue1brown tiktok account.\n                        After struggling mightly to get it to load (turns out you do nothing and it will start by itself), I was captivated. \n                        This video showcased two things I really enjoy: primes, and data visualization.\n                        </p>\n\n                        <p>As soon as I watched this video I knew I wanted to try and create the prime spiral for myself.\n                            Thankfully, 3blue1brown described this phenomenon in enough detail that I knew how to proceed.\n                            It seemed simple enough, so I fired up an iPython notebook in VS code I got to it.\n                        </p>\n\n                        <h4>The Process</h4>\n\n                        <p>The first thing I new I needed to do was find a way to find the \n                            first $n$ primes in a quick manner. I remember doing a <a href=\"https://acme.byu.edu/00000181-49cb-d0ac-abe9-efff44d90000/profiling\" target=\"_blank\" rel=\"noopener noreferrer\">lab</a>\n                            in school where one of the problems was to write a function to find prime numbers as quick as possible.\n                            I went back, copied that code (thanks past me), and used it in my project. After testing it, my function\n                            could find the first 10000 times in about 0.07 seconds, and first 100000 primes in about 1.56 seconds. Sufficently fast for me.\n                        </p>\n\n                        <p>After I got my prime finder up and running, and started working on how I wanted to plot these spirals. I have recently been enjoying the\n                            Plotly library, so I started with that. After a few seconds of Googling, I found that Plotly had a very easy way to plot polar coordinates\n                            using <code>plotly.express.scatter_polar()</code>. I ran my prime finder, and plotted the first 1000 primes. I could see\n                            the spiral, but the graph was not as interactive as I wanted it to be. One of the key features of the 3blue1brown\n                            video is that if you zoomed in or out on the graph, the spiral would change. I could not do that with this plotting method, so\n                            I needed to change something.\n                        </p>\n\n                        <p>\n                            Eventually, after spending a few minutes getting my primes into their polar form, I got something to work using <code>plotly.express.scatter()</code>.\n                            It looked good and I was happy with it. After some reformating of the graph, I got something that looked like this:\n                        </p>\n\n                        <center><iframe src=\"/blog/blog_files/prime_spiral/prime_spiral_graph.html\" height=\"800\" width=\"1200\" frameBorder=\"0\"></iframe></center>\n\n                        <p>Pretty neat! Try zooming in to the middle to see how small the spiral gets.</p>\n\n                        <h4>Conclusion</h4>\n\n                        <p>This was not anything crazy, but something cool that I saw and wanted to share. The code I used to\n                            create this graph and find the primes is located on my <a href=\"https://github.com/dylanskinner65/Fun_math\" target=\"_blank\" rel=\"noopener noreferrer\">Github</a>.</p>",
      "quote": "Mathematicians have tried in vain to this day to discover some order in the sequence of prime numbers, \n                                    and we have reason to believe that it is a mystery into which the human mind will never penetrate.",
      "quoteAuthor": "Leonhard Euler",
      "category": "Math & ML"
    },
    {
      "slug": "random-forests",
      "title": "Random Forests for Classification",
      "date": "2024-01-17",
      "description": "",
      "content": "<p>\n                            Random forests is a powerful machine learning algorithm with a foundation in decision trees.\n                            In this blog post, we will discuss how random forests work for classification, and we will work through an example of building a random forest for classification\n                            using the Python library <code>sklearn</code>.\n                        </p>\n\n                        <figure>\n                            <img src=\"blog_files/random_forests/forest_image.jpg\" alt=\"Cool forest.\" width=\"90%\" height=\"90%\">\n                            <figcaption text-align=center>Photo by <a href=\"https://unsplash.com/@jplenio?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash\">Johannes Plenio</a> on <a href=\"https://unsplash.com/photos/green-grass-field-with-trees-during-daytime-Z6E4rJemy24?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash\">Unsplash</a>\n                            </figcaption>\n                        </figure>\n                        \n                        <p></p>\n\n                        <h4>\n                            TL;DR\n                        </h4>\n\n                        <p>\n                            In a <a href=\"https://dylanskinner65.github.io/blog/decision_tress.html\" target=\"_blank\" rel=\"noopener noreferrer\">previous blog post</a>, we talked\n                            about decision trees. While decision trees are helpful, they are what we call a <em>weak classifier</em>. A weak classifier is a model that often performs slightly \n                            better than random chance. This is not ideal, so we want to find a way to improve our decision trees. This is where random forests come in.\n                            Random forests are an extension of decision trees that help to reduce overfitting. Random forests are a collection of decision trees that are trained on\n                            different subsets of the data. Each tree in a random forest is typically trained on a different subset of the data (<em>bootstrap aggregation</em> or <em>bagging</em>),\n                            and each tree is trained on a different subset of the features (<em>attribute bagging</em>).\n                        </p>\n\n                        <p>\n                            Ultimately, our goal with random forests is to create a set of independent decision trees, creating with both bagging and attribute bagging, that when combined,\n                            create a strong classifier. We will discuss how to do this in more detail below.\n                        </p>\n\n                        <h4>Bootstrap Aggregation or Bagging</h4>\n\n                        <p>\n                            One important aspect of random forests is that we are trying to create a set of independent decision trees. One problem with this, is we are trying to create\n                            this set of independent decision trees from a single dataset. This means that each tree will be trained on the same data, and thus, will be very similar. One of the\n                            ways to encourage independence in our decision trees is through bootstrap aggregation or bagging (the names can be used interchangably).\n                        </p>\n\n                        <p>\n                            Recall from the TLDR section that bagging is when we train each tree on a different subset of the data. The na\u00efve way to do this is to simply\n                            remove $p$ data points from the dataset, and use the remaining $n-p$ points to train our decision tree. (This can actually be very helpful because we can use\n                            the $p$ points not used to train the tree to test the tree and get error estimation.) But finding the optimal $p$ is tricky. If we make $p$ too small, then our trees\n                            will be trained on a majority of the same data and thus be highly correlated. If we make $p$ too large, then our trees will be trained on very small subsets of\n                            the data which will produce trees with high bias on the full training set (i.e., the trees will not generalize well to the full data set). The solution to this problem\n                            is bagging!\n                        </p>\n\n                        <p>\n                            The idea of bagging is repeatively sample $n$ data points from our dataset (with replacement), and training a decision tree based on this sample.\n                            We can then do this processes $B$ times to create $B$ decision trees trained on a different subset of the data. This is bagging!\n                        </p>\n\n                        <p>\n                            Some advantages of bagging are that it reduces variances, is robust against outliers, and is easy to parallelize. One problem it presents, however, is that\n                            it might not use all the data. But, as mentioned above, every data point that is not used to train a tree can be used to test the tree! (This is called <em>out-of-bag</em> or <em>OoB</em> error estimation.)\n                        </p>\n\n                        <p>\n                            In general, the probabilty of a point in our data NOT being selected to be used in a specific tree is $(1- 1/N)^n$, where $n$ is the number\n                            of points we are using to train our tree. If $n=N$ (so the number of points used to train our tree is the size of our dataset), we now have the\n                            probability of a point in our data NOT being selected to be used in a specific tree is\n                        </p>\n\n                        $$\\left(1 - \\frac{1}{N} \\right)^N \\to \\frac{1}{e} \\approx 36\\%$$\n\n                        <p>\n                            as $N\\to\\infty$. Thus, on average, $36\\%$ of our data will not be used in a given tree. This might be confusing. After all, we are taking $N$ points\n                            from our dataset of size $N$ to train the tree. Remember: we are selecting point <em>with</em> replacement. This means that we can select the same point\n                            multiple times! Thus, it is quite likely that some data will be left out of any given tree.\n                        </p>\n\n                        <p>\n                            Note: just because we are creating $B$ decision trees based on different subsets of data, that does not mean our trees are immune from \n                            high correlation. If we have one feature that is a pretty heavy predictor of the target variable, then it is likely that our $B$ trees will all split\n                            on that feature. This can still give us highly correlated trees, which weakens our ensemble classifier.\n                        </p>\n\n                        <p>\n                            This is where attribute bagging comes in.\n                        </p>\n\n                        <h4>Attribute Bagging</h4>\n\n                        <p>\n                            In order to reduce the correlation between our trees, we can train each tree on a different subset of features. This is called attribute bagging.\n                        </p>\n\n                        <p>\n                            Initially proposed in 1996 by <a href=\"https://galton.uchicago.edu/~amit/Papers/shape_rec.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Yali Amit and Donald Geeman</a>,\n                            attribute bagging is very similar to bagging, only instead of randomly selecting $n$ data points from our dataset for each tree, we randomly select $m$ features from our dataset\n                            for each <em>node</em>. Thus, each node in our tree is trained on a different subset of features. This can not only greatly reduce the correlation between our trees, it also \n                            can reduce the computational and temporal cost of building our trees. If we have $M$ features to consider with $M$ being quite later, reducing that to $m < M$ features\n                            allows us to build our trees much faster! \n                        </p>\n\n                        <h4>Random Forest Classifier</h4>\n\n                        <p>\n                            Now that we have discussed bagging and attribute bagging, we can now discuss random forests. Very simply, a random forest is a collection of decision trees\n                            built with both bagging and attribute bagging. The combination of these two ideas allow us to create decision trees that are even more independent of each other, compared\n                            with decision trees built for bagging or attribute bagging alone. Because of this reason, random forests often outperform trees that <em>are</em> built with bagging or \n                            attribute bagging alone.\n                        </p>\n\n                        <p>Here is the main idea of the random forests algorithm:</p>\n\n                        <p>\n                            Start by determining to build $B$ trees and select $m$ features for each node in our tree.\n                            Select $n$ data points from our dataset (with replacement) to train our tree on. Start at the root node. Select $m$ features from our dataset (without replacement) to train our node on.\n                            Split our node on the feature that minimizes our spliting criterion index. Repeat this process for each node until we reach a leaf node. Repeat this process $B$ times.\n                            When we want to predict a new data point, run the data point through each tree in our forest, and take the majority vote of the predictions.\n                        </p>\n\n                        <p>\n                            One very nice thing about decision trees is that both the strength $s$ and average correlation $\\bar{\\rho}$ go down when the number of \n                            features we select at each node $m$ goes down. In this case, we define the average correction to be\n                        </p>\n\n                        $$\\bar{\\rho} = \\frac{\\sum_{i,j}\\text{Cov}(Z_i, Z_j)}{\\sum_{i,j}\\sqrt{\\text{Var}(Z_i)\\text{Var}(Z_j)}}.$$\n\n                        <p>\n                            Where $Z_k$ is tree $k$, and the strength $s$ is defined to be\n                        </p>\n\n                        $$s = \\mathbb{E}[\\bar{Z}],$$\n                        \n                        <p>\n                            where $\\bar{Z} = \\frac{1}{B}\\sum_{i=1}^B Z_i$ (simply the average value of the $Z_i$). This is wonderful news because it tells us that the generalization\n                            error of our random forest classifier is low! We can verify that is true\n                            by using the Breiman bound defined as\n                        </p>\n\n                        $$P(F(X) \\not= Y)\\leq \\bar{\\rho}\\left(\\frac{1 - s^2}{s^2} \\right).$$\n\n                        <h4>Error Estimation Using the Out of Bag Estimate</h4>\n\n                        <p>\n                            As mentioned above, we can use the points <em>not</em> used to create a tree to evaluate how well the tree predicts our data, producing the OoB error estimate.\n                            Let's talk real quick about how we might do this.\n                        </p>\n\n                        <p>\n                            Once we train our random forest algorithm, we get our ensamble classifier $F$. To get our OoB estimate, we start by looking at tree $1$ in the forest. Get every data point\n                            $\\{\\textbf{x}_i\\}_{i=1}^n$ that was <em>not</em> used to train tree $1$. Run each data point through tree $1$ and get the prediction $\\hat{y}_i$ for each data point.\n                            Use those predictions and the actual values $\\{y_i\\}_{i=1}^n$ to calculate the error estimate for tree $1$ using whichever error function you desire. Continue this process\n                            iteratively for each tree in the forest. Once you have the error estimate for each tree, average them together to get the OoB error estimate for the random forest. \n                        </p>\n\n                        <p>\n                            Another way to think about this (with more mathematical notation), if $i$ represents the index of a data point, let OoB$(i)$ represent the set of tree that did not\n                            use data point $\\boldsymbol{x}_i$ to train. We can calculate the error estimate for data point $\\textbf{x}_i$ as\n                        </p>\n\n                        \\[\n                        \\text{OoBErr}(i) = \\frac{1}{|\\text{OoB}(i)|}\\sum_{\\phi\\in\\text{OoB}(i)}\\mathscr{L}(\\phi(\\textbf{x}_i), y_i),\n                        \\]\n\n                        <p>\n                            Where $\\mathscr{L}$ is our loss function. We can then calculate the OoB error estimate for the entire forest as\n                        </p>\n\n                        \\[\n                            \\text{OoBErr} = \\frac{1}{B}\\sum_{i=1}^B\\text{OoBErr}(i).\n                        \\]\n\n                        <p>\n                            Aside from being computational efficient, the OoB error estimate typically has small bias (i.e., the forest has a lower lever of underfitting), and is approximately as accurate as cross\n                            validation while being much faster to compute. This is because using OoB estimates requires us to build one forest, while $k$-fold cross validation requires us to build $k$ forests.\n                        </p>\n\n                        <h4>Downsides to Random Forests</h4>\n\n                        <p>\n                            One of the biggest strengths of decision trees is their interpretability. It is very easy to take a new piece of data to a decision tree, run it through the tree, and understand how it got\n                            to the prediction that it did. This is not the case with random forests. Even though random forests are simply an ensemble of decision trees, because of the use of bagging and attribute bagging\n                            to build those trees, interpretability is not a simple as looking at each tree individually and deducing a prediction from that. This, however, should not be a reason to not use random forests. Random forests are still a very powerful tool despite their lack of interpretability!\n                        </p>\n\n                        <p>\n                            Another downside to random forests is that despite using bagging and attribute bagging to create 'independent' trees, it is still very easy for these trees to overfit the data. This, of course,\n                            is not a problem unique to random forests. To combat this, there are several hyperparameters we can tune to help reduce overfitting. We will discuss these hyperparameters below.\n                        </p>\n\n                        <h4>Reducing Overfitting in Random Forests</h4>\n\n                        <p>\n                            Most of the ways to reduce overfitting in random forests are the same as the ways to reduce overfitting in decision trees. This includes setting a maximum depth of each tree, setting a minimum number of samples per split and/or a minimum\n                            number of samples per leaf. We can choose a maximum number of trees to build, and we can choose a maximum number of features to consider at each node. If time allows, performing a grid search to find the optimal hyperparameters\n                            is a great way to reduce overfitting!\n                        </p>\n\n                        <h4>Building a Random Forest in Python</h4>\n\n\n                        <p>\n                            Now that we have discussed the theory behind random forests, we will now work through an example of building a random forest in Python.\n                            To do this, we will be utilizing the <code>RandomForestClassification</code> function from the <code>sklearn</code> library, using the <code>mnist</code> dataset.\n                        </p>\n\n                        <p>\n                            Start by loading the necessary libraries and the data.\n                        </p>\n\n<pre><code class=\"language-python\">from sklearn.ensemble import RandomForestClassification\nfrom sklearn.datasets import load_digits</code></pre>\n\n                        <p>\n                            We can look at a few of the images in the dataset to get a feel for what we are working with.\n                        </p>\n\n                        <img src=\"blog_files/random_forests/sample_digits.png\" alt=\"MNIST images\" width=\"90%\" height=\"90%\">\n\n                        <p>\n                            The images are quite grainy (they are only $8\\times 8$ pixels), but we can still make out the numbers. Let's see how well a random forest can classify these images.\n                        </p>\n\n                        <p>\n                            First, we need to load in and split our data into training and testing sets. We will use $80\\%$ of the data to train our model, and $20\\%$ to test our model.\n                        </p>\n<pre><code class=\"language-python\"># Load in the data\ndigits_X, digits_y = load_digits(return_X_y=True, as_frame=True)\n\n# Train/test split the data\nX_train, X_test, y_train, y_test = train_test_split(digits_X, digits_y, test_size=0.2, random_state=1)</code></pre>\n\n                        <p>\n                            Now that we have our data, we can build our random forest. Let's load in our classifier. We will just use a basic tree, making sure \n                            we set up our classifier to use bootstrapping and OoB error estimation.\n                        </p>\n\n<pre><code class=\"language-python\"># Load in the classifier\nrf = RandomForestClassifier(random_state=1, bootstrap=True, oob_score=True)</code></pre>\n\n                        <p>\n                            Now that we have our classifier, we can train it on our training data (keeping track of how long it takes to train).\n                        </p>\n                \n<pre><code class=\"language-python\">%%timeit\nrf.fit(X_train, y_train)\n>>> 223 ms \u00b1 679 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)</code></pre>\n\n                        <p>\n                            Now that we have trained our classifier, we can see the <code>oob_score</code> of our classifier.\n                        </p>\n\n<pre><code class=\"language-python\">rf.oob_score_\n>>> 0.9721642310368824</code></pre>\n\n                        <p>\n                            This is a pretty good score! This means that our classifier correctly predicted the class of $97\\%$ of the data points it did not use to train the model.\n                            Let's see how well our classifier does on the test data.\n                        </p>\n\n<pre><code class=\"language-python\">rf.score(X_test, y_test)\n>>> 0.9833333333333333</code></pre>\n\n                        <p>\n                            98.3% accuracy! Not bad! We can visualize the confusion matrix to see how well our classifier did on each class.\n                        </p>\n\n                        <img src=\"blog_files/random_forests/confusion_matrix.png\" alt=\"Confusion matrix\" width=\"90%\" height=\"90%\">\n\n                        <p>\n                            From our confusion matrix, we cannot really see any classes that our classifier struggled with. It appears that \n                            our classifier did incorrectly predict a few $0$'s as $4$'s, and we can see where the comes from. But overall, our classifier did a pretty good job,\n                            especially considering how random forests does not seem like it would work for classifying images!\n                        </p>\n\n                        <h4>Conclusion</h4>\n                        <p>\n                            In this article, we talked about Random Forests, and how they can be used for classification. \n                            We discussed the improvements random forests have over decision trees, including bagging and attribute bagging, and we discussed how to build a random forest\n                            to classify digits in python!\n                            I hope that through this article, you now understand the power of random forests and will consider using them in your next project\n                            before you reach for a neural network.\n                            If you want to see the code used in this article, you can find it\n                            on my <a href=\"https://github.com/dylanskinner65/dylanskinner65.github.io/blob/main/blog/blog_files/random_forests/random_forests.ipynb\" target=\"_blank\" rel=\"noopener noreferrer\">Github</a>.\n                        </p>",
      "quote": "Sadly, it's much easier to create a desert than a forest.",
      "quoteAuthor": "James Lovelock",
      "category": "Math & ML"
    },
    {
      "slug": "slice-surface-genus",
      "title": "Introduction to Knot Theory: Slice Surfaces and Slice Genus",
      "date": "2024-02-28",
      "description": "In this blog post we build upon previous blog posts. We will be dicussing slice surfaces and slice genus.",
      "content": "<p>\n                           Continuing on from my previous <a href=\"braids.html\" target=\"_blank\" rel=\"noopener noreferrer\">blog post</a>, this blog\n                           post focuses on slice surfaces and the slice genus of a knot.\n                        </p>\n\n                        <figure>\n                            <img src=\"blog_files/slice_surface_genus/4d_motion.gif\" alt=\"Fun moving surface.\" width=\"90%\" height=\"90%\">\n                            <figcaption text-align=center>Some fun '4D' motion. Credit to <a href=\"https://www.reddit.com/r/woahdude/comments/15ibll/and_now_a_2d_moving_image_of_a_3d_model_of_4d/\" target=\"_blank\" rel=\"noopener noreferrer\">this reddit user</a>.</figcaption>\n                        </figure>\n                        \n                        <p></p>\n\n                        <h4>\n                            Introduction to Surfaces\n                        </h4>\n\n                        <p>\n                            In the results to follow, one important idea to understand is that of a <em>topological surface</em> (or simply a <em>surface</em>). \n                            A topological surface is a two-dimensional manifold, intuitively representing a flat, rubbery sheet that can be stretched, bent, \n                            and manipulated without tearing or gluing. Surfaces can be <em>orientable</em>, which simply means you can distinguish between the \n                            'front' and 'back' of the surface.\n                        </p>\n\n                        <figure>\n                            <img src=\"blog_files/slice_surface_genus/manifold_examp.png\" alt=\"Example 2d manifold.\" width=\"90%\" height=\"90%\" id=\"figure1\">\n                            <figcaption text-align=center><b>Figure 1:</b> An example of a two-dimensional manifold. I got this image from \n                                <a href=\"https://en.wikipedia.org/wiki/Surface_%28topology%29\" target=\"_blank\" rel=\"noopener noreferrer\">Wikipedia</a>.</figcaption>\n                        </figure>\n\n                        <p>\n                            An important piece of information about an orientable surface is its <em>genus</em>. \n                            The genus of a surface is a fundamental topological invariant describing the shape \n                            and structure of the surface. Intuitively, it can be thought of as the number of \n                            \"handles\" or \"holes\" that a surface possesses (see <a href=\"#figure2\">Figure 2</a>). Every orientable \n                            surface is topologically equivalent some surface with specified genus.\n                        </p>\n\n                        <figure>\n                            <img src=\"blog_files/slice_surface_genus/Figure2.png\" alt=\"Example 2d manifold.\" width=\"100%\" height=\"100%\" id=\"figure2\">\n                            <figcaption text-align=center><b>Figure 2:</b> The left most surface has genus 0 (no holes), \n                                the middle surface has genus 1 (one hole), and the right most surface has genus 2 (two holes).</figcaption>\n                        </figure>\n\n                        <h4>Seifert Surfaces</h4>\n\n                        <p>\n                            If we take an orientable surface and cut a hole in it, we then get a surface with boundary. \n                            Given a knot $K$, we can always find a surface with boundary whose boundary is the knot \n                            $K$ <a href=\"#seifert\">[1]</a>. Such a surface is called a <em>Seifert surface for K</em>. \n                            More precisely, a Seifert surface is an oriented surface associated with a knot or link \n                            on its boundary in three-dimensional space (see <a href=\"#figure3\">Figure 3</a>).\n                        </p>\n\n                        <figure>\n                            <img src=\"blog_files/slice_surface_genus/Figure3.png\" alt=\"Seifert surface.\" width=\"60%\" height=\"90%\" id=\"figure3\">\n                            <figcaption text-align=center><b>Figure 3:</b> This is a Seifert surface. The white and blue represents the surface. You\n                                can see it is orientable because there is a clear distinction between the front and\n                                back of the surface (shown by the two colors). The orange is the knot that is on the\n                                boundary of the surface. (Credit \n                                to <a href=\"https://en.wikipedia.org/wiki/Seifert_surface\" target=\"_blank\" rel=\"noopener noreferrer\">Wikipedia</a>.)</figcaption>\n                            </figcaption>\n                        </figure>\n\n                        <p>\n                            One of the ultimate goals of representing knots in this way is to find the simplest \n                            surface possible to represent a given knot, which is not always obvious. For example,\n                            the minimal genus of a Seifert surface bounded by a certain knot may be 3, but an \n                            explicit minimal genus surface might be difficult to find. Thankfully, the Seifert \n                            genus (which is defined as the minimal genus of a Seifert surface bounded by the knot) \n                            is relatively easy to calculate.\n                        </p>\n\n                        <h4>Slice Surfaces</h4>\n\n                        <p>\n                            Now suppose that you have knot that bounds a surface in $\\mathbb{R}^{3}$ of genus 2, \n                            but would like to construct a surface that it bounds with genus 1. One possible solution \n                            is that instead of requiring the surface to live entirely inside $\\mathbb{R}^{3}$, \n                            we can allow it to dip into $\\mathbb{R}^{4}$. Just as we can think of $\\mathbb{R}^2$ \n                            being the boundary of $\\mathbb{R}^3_+ = \\{(x,y,z) \\in \\mathbb{R}^3 \\, | \\, z \\geq 0 \\}$ \n                            (see <a href=\"#figure4\">Figure 4</a>), we can think of $\\mathbb{R}^3$ as being the boundary of \n                            $\\mathbb{R}^4_+= \\{(x,y,z,t) \\in \\mathbb{R}^3 \\, | \\, t \\geq 0 \\}$.  Given a knot \n                            $K$ in $\\mathbb{R}^3$ we can therefore consider surfaces in $\\mathbb{R}^4_+$ that are bounded by $K$. \n                            These surfaces that dip into $\\mathbb{R}^{4}_+$ are called \\emph{slice surfaces}, and the \\emph{slice genus} \n                            of a knot is the minimal genus of any slice surface you can find for that knot (see <a href=\"#figure5\">Figure 5</a>). Unfortunately, \n                            the slice genus is much more difficult to calculate than the Seifert genus.\n                        </p>\n\n                        <figure>\n                            <img src=\"blog_files/slice_surface_genus/Figure4.png\" alt=\"2D is Boundary of 3D.\" width=\"90%\" height=\"90%\" id=\"figure4\">\n                            <figcaption text-align=center><b>Figure 4:</b> $\\mathbb{R}^{2}$ represents the boundary of $\\mathbb{R}^{3}$, which in this example is a cube.</figcaption>\n                        </figure>\n\n                        <figure>\n                            <img src=\"blog_files/slice_surface_genus/Figure5.png\" alt=\"Dipping into the 4th dimension.\" width=\"90%\" height=\"90%\" id=\"figure5\">\n                            <figcaption text-align=center><b>Figure 5:</b> Here is the trefoil knot in $\\mathbb{R}^3$, and a slice surface it bounds in $\\mathbb{R}^4_+$</figcaption>\n                        </figure>\n\n                        <p>\n                            Since visualizing $\\mathbb{R}^{4}$ is quite difficult, representing slice surfaces can be a bit of a problem. \n                            One way to overcome this is by looking at level sets. Level sets are a way of representing a slice surface in \n                            $\\mathbb{R}^4_+$ by taking a 'slice' out of the surface at various levels and seeing what the surface looks \n                            like at the location of the slice. One dimension lower, taking level sets of a surface in $\\mathbb{R}^3$ yields \n                            a sequence of two-dimensional planes each containing a single slice of the surface. If you take enough of these \n                            level sets you will be able to get the full picture of the surface (see <a href=\"#figure6\">Figure 6</a>).\n                        </p>\n\n                        <figure>\n                            <img src=\"blog_files/slice_surface_genus/Figure6.png\" alt=\"Slice Surfaces.\" width=\"90%\" height=\"90%\" id=\"figure6\">\n                            <figcaption text-align=center><b>Figure 6:</b> The left most part of this figure is the surface inside $\\mathbb{R}^3$. \n                                By taking level sets of this surface, we can obtain an accurate representation of the surface.</figcaption>\n                        </figure>\n\n                        <p>\n                            To take level sets in $\\mathbb{R}^{4}$ we do the exact same thing as in $\\mathbb{R}^{3}$. \n                            The only difference is now the level sets are each 3-dimensional slices with knots inside, \n                            instead of 2-dimensional planes with planar curves.\n                        </p>\n\n                        <p>\n                            When studying the level sets of a slice surface in $\\mathbb{R}^4_+$ there are a finite number of ways the \n                            level sets may change from one picture to another.  If the surface has a \\emph{saddle point} then passing \n                            the saddle point will change the level sets by bringing two nearby strands together and merging them \n                            (see the change that happens between the fourth and fifth diagrams in <a href=\"'$figure7\">Figure 7</a> below). In this context, \n                            we define a saddle point to be a point on the surface where the curvature takes on both positive and \n                            negative values along different directions.  If the surface has a local minimal point, then passing the \n                            mimimal point will result in a single circle being deleted from the level set (in <a href=\"'#figure7\">Figure 7</a>, the two \n                            circles in the final level set sit just above a pair of local maxima, and passing these maximum points \n                            results in the two circles being deleted from the level set). Likewise, if the surface has a local \n                            maximum point, then passing the maximum point results in a single circle being added to the level set.\n                        </p>\n                        \n                        <figure>\n                            <img src=\"blog_files/slice_surface_genus/Figure7.jpeg\" alt=\"Cross sections.\" width=\"90%\" height=\"90%\" id=\"figure7\">\n                            <figcaption text-align=center><b>Figure 7:</b> Here is an example of a surface being represented by cross-sections.</figcaption>\n                        </figure>\n                        \n\n\n                        <h4>Conclusion</h4>\n\n                        <p>\n                            In this blog post, we discussed the basics of Seifert and slice surfaces. \n                            Understanding surfaces, particularly topological surfaces and their properties, \n                            is crucial in various fields, including knot theory. \n                            Seifert surfaces provide a valuable tool for representing knots, offering insights into \n                            their structure and complexity. Moreover, slice surfaces offer another perspective, allowing \n                            us to explore the interaction between knots and higher-dimensional spaces. While calculating \n                            the slice genus may pose challenges, techniques such as examining level sets provide valuable \n                            insights into the nature of slice surfaces. By delving into these concepts, we uncover not only \n                            the intricacies of knots but also deepen our understanding of the underlying mathematical structures that govern them.\n                        \n                        </p>\n\n                        <h4>Citations</h4>\n\n                        <!-- <ol>\n                            <span id=\"adams\">[1]</span> Collin C. Adams, <em>The Knot Book</em>. American Mathematical Society, 2004. ISBN: 978-0821836781.\n                        </ol>\n                        <ol\n                            <span id=\"birman\">[2]</span> Joan S Birman. <em>Braids, links, and mapping class groups</em>. 82. Princeton University Press, 1974.\n                        </ol>    -->\n\n                        <ol>\n                            <span id=\"seifert\">[1]</span> Heinrich Seifert. <em>\u00dcber das Geschlecht von Knoten</em>. In: <em>Mathematische Annalen</em> 110 (1935), pp. 571\u2013592. DOI: 10.1007/BF01448044.\n                            <!-- Add more list items as needed -->\n                        </ol>\n\n                        <!-- <p>\n                            <a id=\"adams\"></a>Adams, C. C. (1994). The Knot Book: An Elementary Introduction to the Mathematical Theory of Knots. W. H. Freeman and Company. -->",
      "quote": "Nobody can point to the fourth dimension, yet it is all around us.",
      "quoteAuthor": "Rudy Rucker",
      "category": "Math & ML"
    },
    {
      "slug": "summer-blog-plans",
      "title": "Where I Want My Blog To Go This Summer",
      "date": "2023-04-07",
      "description": "",
      "content": "<h4>Why I Wanted to Write This Article</h4>\n                        <p> Over the past few years I have spent a lot of time listening to postcasts, reading LinkedIn posts, and watching\n                            videos from successful and influential people in Data Science. These are people like\n                            <a href=\"https://www.linkedin.com/in/keremenko/?originalSubdomain=au\">Kirill Eremenko</a>, \n                            <a href=\"https://karpathy.ai/\">Andrej Karpathy</a>, and <a href=\"https://lexfridman.com/\">Lex Fridman</a>, just to name a few.\n                            The biggest thing I have learned from these data science leaders is their ability to post interesting content. Now,\n                            these three men are VERY intelligent and are on a completely different plane from myself. However,\n                            I feel that one way I can emulate them is by posting what I am learning and working on. I have tried doing that so far,\n                            but my blog posts have been few and far between on random topics that I found interesting in school. I am not saying that is\n                            a bad thing, but I want to do more.</p>\n                        \n                        <h4>My Plan</h4>\n                        <p>I feel that part of the 'problem' with my infrequent blog posts is I am afraid to put anything out there unless I feel very confident\n                            that everything is right and I have had some professional (usually a professor) verify what I am saying. That is something I want to break.\n                            This summer, I want to write about the things I am learning. That is all. I want to write about what I am reading, practicing,\n                            building, and failing at. I do not want these writings to necessary be tutorials or guides (though I hope some are), but rather\n                            my thought process and the things I experience.</p>\n                        <p>For example, this summer I want to learn more about data engineering. I want to learn things about databases (both building and querying),\n                            data collection and aquisition, data processing and cleaning, etc. I have frequently heard (and I am sure you have too) about how\n                            80% of the day for a data scientist is spent cleaning data. I have confidence that my university program will teach me about the\n                            algorithms and the math behind it, but I do not have the same confidence that I will learn about data engineering.\n                        </p>\n                        <p>Right now, my plan is to take the <a href=\"https://www.coursera.org/professional-certificates/ibm-data-engineer\">IBM data engineering course</a>. \n                        Looking at the classes it offers, I feel that my understanding of data engineering will grow and it is something I am interested in. My goal would be\n                        to write at least once a week about the things I am learning in that course. This will help me remember the things I am learning, but also \n                        force me to better understand the material if I am to put it into writing.</p>\n\n                        <h4>My Goals</h4>\n                        <p>With this in mind, I wanted to go over my goals for the summer and the plans I have set for them. I want to put this out \n                            there for the world to see and to hopefully hold me accountable. I am sure that some of these goals and plans will be altered as\n                            time goes on, but I still want to do this.\n                        </p>\n                        <ul>\n                            <li>Learn Data Engineering</li>\n                            Of course I do not mean learn data engineering in its entirety, but I want to gain a better knowledge of SQL, databases (maybe even database management),\n                            data cleaning, data processing and the like. I would also like to learn more about ETL pipelines and how I can leverage them as a future data scientist.\n                            <ul>\n                                <li>How I am going to do this</li>\n                                Like I mentioned above, I want to complete the IMB Coursera course. I would also like to have a personal project\n                                where I would aquire some data, and then use my data engineering skills I am learning to create a database and do all\n                                that sort of stuff.\n                            </ul>\n                            <li>Complete A Big Personal Project</li>\n                            One idea that I have for a personal project is creating an NHL dashboard. My vision for this would be to allow the user\n                            to interactively see just about any information or stat they want to about any player in any year of the NHL, or any team for any year.\n                            A few weeks ago I found the NHL API that has just about anything you could want to know. However, it is all in json form \n                            (which I think is the format of all API's? Not sure, honestly) and so it is confusing for me to work with. I want to \n                            better understand this api and build my dashboard using plotly and dash.\n                            <ul>\n                                <li>How I am going to do this</li>\n                                My plan is to currently work 4 hours a week on my project. Whether this is data scraping from the API, building a database\n                                with all the information I scrape, or learning how to use dash and plotly. I want, by the end of the summer, to have\n                                a working concept.\n                            </ul>\n                            <li>Have A Greater Involvement In The Data Science Community</li>\n                            For the past two months I have been attending the <a href=\"https://www.meetup.com/machine-learning-utah/\">MLOps Meet Up</a> here in Utah.\n                            It has been a great experience and I am excited to keep attending. While I enjoy showing up\n                            and then leaving, I want to play a more active role in the community in whatever\n                            way that means. My plan is to reach out to the founders of the Data Science of Utah\n                            meet up group and see how I can help!\n                        </ul>\n\n                    <h4>Conclusion</h4>\n                    I would say that those are my main goals. Like I mentioned, I might reformulate\n                    these goals and alter them as necessary, but I am excited to see where this goes!",
      "quote": "\u201cPeople with goals succeed because they know where they're going.\u201d",
      "quoteAuthor": "Earl Nightingale",
      "category": "Math & ML"
    },
    {
      "slug": "unconstrained-opt",
      "title": "The Basics of Unconstrained Optimization",
      "date": "2024-03-27",
      "description": "In this blog post we discuss the basics of unconstrained optimization.",
      "content": "<p>\n                            Behind important things like machine learning, finance, and operations research lies an important concept: optimization.\n                            Optimization is the process of finding the best solution to a problem from all possible solutions. In this blog post, we will\n                            discuss the basics of unconstrained optimization, a fundamental concept in optimization theory. We will specifically discuss\n                            a few necessary and sufficient conditions for optimality, and consider a few examples.\n                        </p>\n\n                        <figure>\n                            <img src=\"blog_files/unconstrained_opt/optimization_image.jpg\" alt=\"An optimized pipe.\" width=\"100%\" height=\"90%\">\n                            <figcaption text-align=center>\n                                Photo by <a href=\"https://unsplash.com/@martinadams?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash\">Martin Adams</a> on <a href=\"https://unsplash.com/photos/brown-metal-tower-a_PDPUPuNZ8?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash\">Unsplash</a>\n  \n                            </figcaption>\n                        </figure>\n                        \n                        <p></p>\n\n                        <h4>\n                            Some Important Definitions\n                        </h4>\n\n                        <p>\n                            Throughout this blog post, we will be working with a function $f: \\Omega \\rightarrow \\mathbb{R}$, where $\\Omega\\in\\mathbb{R}^n$ is an open set.\n                        </p>\n\n                        <p>\n                            It is first important to define what it means for a value to be a minimizer. If we have our function $f$ as defined above, then we say a point $\\textbf{x}^*\\in\\Omega$\n                            is a minimizer of $f$ if $f(\\textbf{x}^*) \\leq f(\\textbf{x})$ for all $\\textbf{x}\\in\\Omega$. In this case, we call $f(\\textbf{x}^*)$ the minimum value of $f$.\n                            We currently make no claims about either the uniqueness of the minimizer nor claim is is a global minimizer (but we will discuss this later). In this current set up,\n                            if there are no other constraints on the set of possible minimizers, then we say we are working with unconstrained optimization.\n                        </p>\n\n                        <p>\n                            As mentioned above, it is very possible to have multiple minimizers of a function. It is typical to denote the set of all minimizers of $f$ as\n\n                            $$\\text{argmin}_{\\textbf{x}\\in\\Omega} \\;f(\\textbf{x}) = \\{\\textbf{x}\\in\\Omega \\;|\\; f(\\textbf{x})\\,\\leq\\, f(\\textbf{y})\\quad \\forall\\textbf{y}\\in\\Omega \\}.$$\n\n                            Of course, if this is a set with one element, then we say the minimizer is unique. It should also be mentioned that, by definition, this set will contain only minimizers that yield the\n                            global minimum value. \n                        </p>\n\n                        <h4>\n                            Necessary and Sufficient Conditions for Optimality\n                        </h4>\n\n                        <p>\n                            With these basic definitions and ideas under our belt, we can now discuss some necessary and sufficient conditions for optimality. \n                            We begin with the first order necessary condition for optimality.\n                        </p>\n\n                        <h5>\n                            First-Order Necessary Condition (FONC)\n                        </h5>\n\n                        <p>\n                            If we have our function $f$ as defined above with the additional condition that $f$ is differentiable, then if we know $\\textbf{x}^*$ is a local minimizer of $f$, then $Df(\\textbf{x}^*) = \\mathbf{0}^T$.\n                        </p>\n\n                        <p>\n                            To see this, let's suppose that we have a point $\\textbf{x}^*$ that is a local minimizer of $f$, but $Df(\\textbf{x}^*) \\neq \\mathbf{0}^T$. If this is true, then we can define a unit vector \n                            $\\textbf{q} = -\\frac{Df(\\textbf{x}^*)^T}{\\lVert Df(\\textbf{x}^*)^T}$. This tells us that $Df(\\textbf{x}^*)\\textbf{q} = -\\lVert Df(\\textbf{x}^*)\\rVert$. So, by the non-negative property\n                            of norms, we have that $Df(\\textbf{x}^*)\\textbf{q} = -\\lVert Df(\\textbf{x}^*)\\rVert < 0$. Since $\\textbf{x}^*$ is a local minimizer, this means that we can find a point \n                            $\\textbf{x}^* + \\alpha\\textbf{q}$ such that $f(\\textbf{x}^* + \\alpha\\textbf{q}) < f(\\textbf{x}^*)$. By definition of the derivative, we have\n\n\n                            $$\\begin{aligned}\n                                \\frac{|f(\\textbf{x}^* + t\\textbf{q}) - f(\\textbf{x}^*) - tDf(\\textbf{x}^*)\\textbf{q}|}{\\lVert t\\textbf{q}\\rVert} &= \\frac{f(\\textbf{x}^* + t\\textbf{q}) - f(\\textbf{x}^*) + t\\lVert Df(\\textbf{x}^*)\\rVert}{|t|} \\\\\n                                &= \\frac{f(\\textbf{x}^* + t\\textbf{q}) - f(\\textbf{x}^*) + t\\lVert Df(\\textbf{x}^*)\\rVert}{t} \\\\\n                                &= \\frac{f(\\textbf{x}^* + t\\textbf{q}) - f(\\textbf{x}^*)}{t} + \\lVert Df(\\textbf{x}^*)\\rVert \\geq \\lVert Df(\\textbf{x}^*) \\rVert.\n                            \\end{aligned}$$\n\n                            This implies that \n\n                            $$\\frac{f(\\textbf{x}^* + t\\textbf{q}) - f(\\textbf{x}^*)}{t} \\geq 0$$\n\n                            However, by definition of the derivative, we have\n\n                            $$\\frac{|f(\\textbf{x}^* + t\\textbf{q}) - f(\\textbf{x}^*) - tDf(\\textbf{x}^*)\\textbf{q}|}{\\lVert t\\textbf{q}\\rVert} = 0,$$\n\n                            so we conclude with $0\\geq 0$, which is a contradiction. Thus, we must have that $Df(\\textbf{x}^*) = \\mathbf{0}^T$.\n                        </p>\n\n                        <p>\n                            With this in mind, we know that a point can only be a local minimizer if the derivative of the function at that point is zero. That is not to say that every point where the derivative is zero is a local minimizer,\n                            but a point is certainly not a local minimizer if the derivative is not zero.\n                        </p>\n\n                        <p>\n                            (While the FONC has a very simple statement, it should be noted that it is not always easy to verify $Df(\\textbf{x}) = 0$. This is important to find keep in mind.)\n                        </p>\n\n                        <h5>\n                            Second-Order Necessary Condition (SONC)\n                        </h5>\n\n                        <p>\n                            In addition to the first-order necessary condition, we can also consider the second-order necessary condition. This condition is a little more involved, but it is still important to consider.\n                        </p>\n\n                        <p>\n                            For second-order conditions, we utilize what is called the Hessian or second-derivative matrix. The Hessian of a function $f$ is defined <i>very</i> generally as\n\n                            $$ \\textbf{H}_f = D^2f(\\textbf{x}) = \\begin{bmatrix}\n                            \\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1\\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_1\\partial x_n} \\\\\n                            \\frac{\\partial^2 f}{\\partial x_2\\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_2\\partial x_n} \\\\\n                            \\vdots & \\vdots & \\ddots & \\vdots \\\\\n                            \\frac{\\partial^2 f}{\\partial x_n\\partial x_1} & \\frac{\\partial^2 f}{\\partial x_n\\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_n^2}\n                            \\end{bmatrix}.$$\n\n                            With this in mind, we define the SONC to be if $\\textbf{x}^*$ is a local minimizer of $f$, then the Hessian $D^2f(\\textbf{x}^*)$ is positive semidefinite (also denoted $D^2f(\\textbf{x}^*) \\geq 0$). Now, the\n                            term positive semidefinite might be new to you, but it simply means that all the eigenvalues of the matrix are nonnegative ($\\lambda \\geq 0$ $\\forall\\lambda\\in\\sigma(D^2f(\\textbf{x}^*))$).\n                        </p>\n\n                        <p>\n                            Rather than dive into the proof (which is not very exciting to me), let's instead consider an example. Consider the function $f(x) = -4x^2 - 2y^2$. The first derivative of the function is defined as\n                            $Df(x, y) = \\begin{bmatrix} -8x, & -4y \\end{bmatrix}$. We can easily see that the only place where $Df(x,y) = 0$ is at the origin $(0,0)$. So the origin is the only point that <i>could</i> be a local minimizer (though not guaranteed).\n                            The second derivative of the function is defined as $$D^2f(x, y) = \\begin{bmatrix} -8 & 0 \\\\ 0 & -4 \\end{bmatrix}$$. In this case, the eigenvalues of the Hessian are $-8$ and $-4$, which are both negative. Thus, our\n                            Hessian matrix is negative definite everywhere, so we can conclude the origin is not a local minimizer. This is of course a toy example and can be easily verified by graphing the function (see below). But the idea is important.\n                        </p>\n\n                        <figure>\n                            <img src=\"blog_files/unconstrained_opt/example_plot.png\" alt=\"Plotting our example function.\" width=\"100%\" height=\"90%\">\n                            <figcaption text-align=center>\n                                Plot of the function $f(x,y) = -4x^2 - 2y^2$.\n                            </figcaption>\n                        </figure>\n\n                        <h5>\n                            Second-Order Sufficient Condition (SOSC)\n                        </h5>\n\n                        <p>\n                            Now, up to this point, we have only discussed necessary conditions for optimality. \n                            That is, we have only discussed conditions that, if a point $\\textbf{x}^*$ is a local minimizer, then those conditions are true. \n                            We will now discuss a sufficient conditions for optimality, namely the second-order sufficient condition.\n                        </p>\n                        \n                        <p>\n                            We define the second-order sufficient condition (SOSC) to be\n                            if $\\textbf{x}^*\\in\\Omega$ is a point such that $Df(\\textbf{x}^*) = \\mathbf{0}^T$ and $D^2f(\\textbf{x}^*)$ is positive definite (denoted $D^2f(\\textbf{x}^*) > 0$), then $\\textbf{x}^*$ is a local minimizer of $f$. (This idea\n                            of positive definite is similar to positive semidefinite, but all the eigenvalues are positive.)\n                        </p>\n\n                        <p>\n                            We should note that this sufficiency condition does not say anything about the point $\\textbf{x}^*$ being\n                            a global minimizer. It only says that if the conditions are met, then $\\textbf{x}^*$ is a local minimizer.\n                        </p>\n\n                        <p>\n                            Let's see SOSC in action with a fun example.\n                        </p>\n\n                        <h4>\n                            The Rosenbrock Function\n                        </h4>\n\n                        <p>\n                            We will show the power of the SOSC with the Rosenbrock function. The Rosenbrock function is often used\n                            as a test function for optimization algorithms because of how difficult it is to find the simple minimizer\n                            numerically. This difficulty arises from the fact that the function has a very flat valley that the optimizer\n                            must traverse to find the minimum. We define the Rosenbrock function to be $f(x,y) = (1-x)^2 + 100(y-x^2)^2$.\n                            Here is a plot to visualize it.\n                        </p>\n\n                        <figure>\n                            <img src=\"blog_files/unconstrained_opt/rosenbrock.png\" alt=\"The Rosenbrock function.\" width=\"100%\" height=\"90%\">\n                            <figcaption text-align=center>\n                                The Rosenbrock function.\n                            </figcaption>\n                        </figure>\n\n                        <p>\n                            Let's do some math to get the local minimizer! Recall that SOSC tells us that if $Df(\\textbf{x}^*) = \\mathbf{0}^T$ and $D^2f(\\textbf{x}^*) > 0$, then $\\textbf{x}^*$ is a local minimizer.\n                            So, let's start by getting some derivatives. The first derivative of the Rosenbrock function (and setting it equal to zero) is\n\n                            $$\\begin{aligned}\n                            Df(x, y) &= \\begin{bmatrix} -2(1-x) - 400x(y-x^2), & 200(y-x^2) \\end{bmatrix} \\\\\n                            &= \\begin{bmatrix} -2 + 2x -400xy + 400x^3, & 200y - 200x^2 \\end{bmatrix} = \\begin{bmatrix} 0, & 0 \\end{bmatrix}.\n                            \n                            \\end{aligned}$$\n\n                            Solving the second entry of the vector, we get $y = x^2$. Plugging this into the first entry, we get\n\n                            $$-2(1-x) - 400x({\\color{red}x^2}-x^2) = 0 \\implies -2(1-x) - {\\color{red}0} = 0 \\implies x = 1.$$\n\n                            Since $y = x^2$ and $x=1$, we have $y=1$. Thus, our candidate point (or <em>critical point</em>) is $(1,1)$. Now, let's find the Hessian of the Rosenbrock function. The Hessian is\n\n                            $$D^2f(x, y) = \\begin{bmatrix} 2 - 400y + 1200x^2 & -400x \\\\ -400x & 200 \\end{bmatrix}.$$\n\n                            Evaluating our Hessian at $(1,1)$, we get\n\n                            $$D^2f(1, 1) = \\begin{bmatrix} 2 - 400{\\color{red}(1)} +1200{\\color{red}(1)}^2 & -400{\\color{red}(1)} \\\\ -400{\\color{red}(1)} & 200 \\end{bmatrix} = \\begin{bmatrix} 802 & -400 \\\\ -400 & 200 \\end{bmatrix}.$$\n\n                            We can now check if $D^2f(1,1)$ is positive definite by looking at the eignevalues. Using a computational solver, we get $\\lambda  = 501 \\pm \\sqrt{250601} \\approx \\{1001.6, 0.399 \\}$.\n                            Since both of these are positive always, $D^2f(1,1)$ is positive definite. Thus, by SOSC, we have that $(1,1)$ is a local minimizer of the Rosenbrock function.\n                        </p>\n\n                        <figure>\n                            <img src=\"blog_files/unconstrained_opt/rosen_point.png\" alt=\"The Rosenbrock function and its minimizer.\" width=\"100%\" height=\"90%\">\n                            <figcaption text-align=center>\n                                The Rosenbrock function and its minimizer at $(1,1)$.\n                            </figcaption>\n                        </figure>\n\n                        <p>\n                            This is a very powerful result! Looking at the graph of the Rosenbrock function, it is very difficult to see that $(1,1)$ is a minimizer. But, by using the SOSC, we can see that it is indeed a local minimizer.\n                        </p>\n\n                        \n                        <h4>Conclusion</h4>\n\n                        <p>\n                            Unconstrained optimization is a very important concept in optimization theory. In this blog post, we discussed some necessary and sufficient conditions for optimality. We discussed the first-order necessary condition (FONC),\n                            the second-order necessary condition (SONC), and the second-order sufficient condition (SOSC). We also considered the Rosenbrock function as an example of how to use these conditions.\n                            Unconstrained optimization plays a huge role in many complex fields and is even the backbone of tools like gradient descent.\n                            (If you want to see how I built the plots for this blog post, you can check them out <a href=\"https://github.com/dylanskinner65/dylanskinner65.github.io/blob/main/blog/blog_files/unconstrained_opt/unconstrained_opt.ipynb\"\n                                target=\"_blank\">here</a>.)\n                        </p>\n                        \n\n                        <!-- <h4>Citations</h4> -->\n\n                        <!-- <ol>\n                            <span id=\"adams\">[1]</span> Collin C. Adams, <em>The Knot Book</em>. American Mathematical Society, 2004. ISBN: 978-0821836781.\n                        </ol>\n                        <ol\n                            <span id=\"birman\">[2]</span> Joan S Birman. <em>Braids, links, and mapping class groups</em>. 82. Princeton University Press, 1974.\n                        </ol>    -->\n\n                        <!-- <ol>\n                            <span id=\"seifert\">[1]</span> Heinrich Seifert. <em>\u00dcber das Geschlecht von Knoten</em>. In: <em>Mathematische Annalen</em> 110 (1935), pp. 571\u2013592. DOI: 10.1007/BF01448044.\n                        </ol> -->\n\n                        <!-- <p>\n                            <a id=\"adams\"></a>Adams, C. C. (1994). The Knot Book: An Elementary Introduction to the Mathematical Theory of Knots. W. H. Freeman and Company. -->",
      "quote": "If you optimize everything, you will always be unhappy.",
      "quoteAuthor": "Donald Knuth",
      "category": "Math & ML"
    },
    {
      "slug": "xgboost",
      "title": "A Gentle Introduction and Mathematical Foundations of XGBoost",
      "date": "2024-01-31",
      "description": "",
      "content": "<p>\n                            When it comes to the current machine learning landscape, XGBoost is king. If you look at the tops solutions for just about any Kaggle\n                            competition, the winner will most likely be using XGBoost. XGBoost is a powerful machine learning algorithm that combines ideas from\n                            gradient boosting, random forests, and other important topics. In this blog post, we'll discuss the math behind XGBoost, exploring its inner workings and understanding\n                            how it stands out among other machine learning algorithms. We'll also shed light on the importance of\n                            regularization and draw comparisons between XGBoost, gradient boosting, and random forests through a practical Python example.\n                        </p>\n\n                        <p>\n                            Note, this blog post builds off of my previous blog posts about <a href=\"https://dylanskinner65.github.io/blog/decision_tress.html\" target=\"_blank\" rel=\"noopener noreferrer\">decision trees</a>, <a href=\"https://dylanskinner65.github.io/blog/random_forests.html\" target=\"_blank\" rel=\"noopener noreferrer\">random forests</a>,\n                            and <a href=\"https://dylanskinner65.github.io/blog/gradient_boosting.html\" target=\"_blank\" rel=\"noopener noreferrer\">gradient boosting</a>. If you are unfamiliar with these topics, I recommend reading those blog posts first.\n                        </p>\n\n                        <p>\n                            Additionally, this blog post is <b>very</b> math heavy. If you are more interested in the implementation\n                            of XGBoost and seeing how it compares against random forests and gradient boosted trees, I would\n                            recommend skipping to the <a href=\"#python-example\">Python Example</a> section.\n                        </p>\n\n                        <figure>\n                            <img src=\"blog_files/xgboost/XGBoost_logo.png\" alt=\"XGBoost logo.\" width=\"90%\" height=\"90%\">\n                            <figcaption text-align=center></figcaption>\n                        </figure>\n                        \n                        <p></p>\n\n                        <h4>\n                            Newton Boosted Trees\n                        </h4>\n\n                        <p>\n                            To begin, let's talk about Newton boosted trees. Newton boosted trees are a special case of gradient boosted trees that use Newton's method to\n                            find the optimal weights for each tree. This is the main idea behind XGBoost.\n                        </p>\n\n                        <p>\n                            The main idea of XGBoost is to use the idea of Newton's method to express the objective function $T$ in terms of a sum over the leaves of the trees. Note, \n                            we will actually be getting the quadratic approximation of $T$ instead of the actual $T$ itself. This is because the quadratic approximation of $T$ is much easier to work with.\n                            We will see this come into play, especially in the proofs.\n                        </p>\n\n                        <p>\n                            Recall in my blog post about <a href=\"https://dylanskinner65.github.io/blog/gradient_boosting.html\" target=\"_blank\" rel=\"noopener noreferrer\">gradient boosted trees</a>, we use gradient descent to find the optimal tree $t_k$ that minimizes the loss function $T$. With Newton\n                            boosting, we instead use a quadratic approximation of our objective function $T$ to find the optimal tree $t_k$. Since the quadratic approximation is\n                            a key idea in Newton's method, we call this Newton boosted trees.\n                        </p>\n\n                        <p>\n                            Now, using Newton's method has some pros and cons. One pro is that Newton's method can be much faster than gradient descent. Newton's method converges\n                            quadratically while gradient descent converges linearly, thus a quicker conversion rate. However, Newton's method also\n                            requires computing the Hessian matrix ($2^{\\text{nd}}$ derivative matrix), which can be very expensive ($O(n^3)$, in fact). So, while Newton's method can converge faster, it can also be much expensive to compute,\n                            causing Newton's method to be slower than gradient descent in some cases.\n                        </p>\n\n                        <p>\n                            Since Newton's method is $O(n^3)$ and our parameter space is determined by the values $\\textbf{x}_1, \\dots, \\textbf{x}_N$, Newton's method is not feasible for large datasets.\n                            The nice thing, however, is we can use the idea of approximating the objective with a quadratic function and minimizing that\n                            without ever computing the Hessian! Let's dive into that, reviewing first a few important pieces of terminology.\n                        </p>\n\n                        <h4>Minimizing Quadratics Without the Hessian</h4>\n\n                        <p>\n                            Recall that our updated function is $f_{k+1} = f_k + t_{k+1}$, where $f_k$ is sum of previous trees $\\sum_{i=1}^k t_i$\n                            and $t_{k+1}$ is our new tree. When evalauted at some specific datapoint $\\textbf{x}_i$, we have\n                            $f_{k+1}(\\textbf{x}_i) = \\hat{y}_i^{k+1} = \\hat{y}_i^k + t_{k+1}(\\textbf{x}_i)$. \n                        </p>\n\n                        <p>\n                            To minimize our quadratic function without the Hessian, we need to take the quadratic approximation of our loss function\n                            $\\mathscr{L}((f_k + t_{k+1})(\\textbf{x}_i), y_i)$. The best way to do this is to use a Taylor series expansion around $t_{k+1}(\\textbf{x}_i) = 0$\n                            to get a resulting polynomial of degree 2. This will result in\n                        </p>\n\n                        $$\\small{\\begin{align}\\mathscr{L}(\\hat{y}_i^k + t_{k+1}(\\textbf{x}_i), y_i) \\approx \\mathscr{L}(\\hat{y}_i^k + 0, y_i)(t_{k+1}(\\textbf{x}_i))^0 &+ \\mathscr{L}^{\\prime}(\\hat{y}_i^k + 0, y_i)(t_{k+1}(\\textbf{x}_i))^1 \\\\\n                                                                                      &+ \\frac{\\mathscr{L}^{\\prime\\prime}(\\hat{y}_i^k + 0, y_i)}{2!}(t_{k+1}(\\textbf{x}_i))^2\\end{align}}$$\n\n\n                        <p>\n                            While this looks quite messy, it is simply the Taylor expansion learned about in calc 2. If this is unfamiliar to you,\n                            I recommend checking out <a href=\"https://math.libretexts.org/Bookshelves/Calculus/Calculus_3e_(Apex)/08%3A_Sequences_and_Series/8.08%3A_Taylor_Series\" target=\"_blank\" rel=\"noopener noreferrer\">this blog post</a> on Taylor/Maclaurin series.\n                        </p>\n\n                        <p>\n                            You might also be asking yourself, 'why are we using the second order Taylor series expansion instead of any other order'. That is a valid question!\n                            Ultimately, it comes down to the second order expansion seems to work well and is not too computationally expensive!\n                        </p>\n\n                        <p>\n                            Since we are minimizing this objective function, we do not care about any constant terms, so we can drop them. This leaves us with\n                        </p>\n\n                        $$\\small{\\mathscr{L}^{\\prime}(\\hat{y}_i^k + 0, y_i)(t_{k+1}(\\textbf{x}_i))^1\n                        + \\frac{\\mathscr{L}^{\\prime\\prime}(\\hat{y}_i^k + 0, y_i)}{2!}(t_{k+1}(\\textbf{x}_i))^2}$$\n\n                        <p>\n                            Now, don't let notation fool you. $t_{k+1}(\\textbf{x}_i)$ is a <em>variable</em>, the same as $x$ is a variable in $f(x) = x^2$. \n                        \n                        </p>\n                        \n                        <p>\n                            With this in mind,\n                            instead of computing the Hessian of the objective function $T$ and minimizing that, we instead are trying to find the tree $t_{k+1}$ that takes\n                            in points $\\textbf{x}_1, \\dots, \\textbf{x}_N$ and outputs the values $t_{k+1}(\\textbf{x}_1), \\dots, t_{k+1}(\\textbf{x}_N)$ and now try to minimize\n                        </p>\n\n                        $$\\small{\\begin{align}\\widetilde{T} = \\left[\\sum_{i}^{N} \\mathscr{L}^{\\prime}(\\hat{y}_i^k + 0, y_i)\\cdot(t_{k+1}(\\textbf{x}_i))^1\n                        + \\frac{\\mathscr{L}^{\\prime\\prime}(\\hat{y}_i^k + 0, y_i)}{2!}\\cdot(t_{k+1}(\\textbf{x}_i))^2\\right] + R(t_{k+1})\\end{align}}$$\n\n                        <p>\n                            Where $R(t_{k+1})$ is the contribution of tree $t_{k+1}$ to our regularization term.\n                        </p>\n\n                        <p>\n                            Note, the summation occurs simply because we are doing this for each data point in our dataset. The only jump we made from the previous equation\n                            to this one is the inclusion of more data points. The previous equation is what we are minimizing for one data point, while this equation is what we are minimizing for all data points!\n                        </p>\n                        \n                        <h4>Example of Quadratic Approximation: Exponential Loss</h4>\n\n                        <p>\n                            Let's work through an example of the math shown above using the exponential loss function\n                        </p>\n\n                        $$\\small{\\mathscr{L}(f(\\textbf{x}_i), y_i) = \\exp(-y_i f(\\textbf{x}_i)).}$$\n\n                        <p>\n                            (We will be evaluating each derivative at $0$, so keep that in mind!)\n                        </p>\n\n                        <p>\n                            Replacing $f(\\textbf{x}_i)$ with $f_k(\\textbf{x}_i) + t_{k+1}(\\textbf{x}_i)$, we get\n                        </p>\n\n                        $$\\small{\\begin{align}\\mathscr{L}(f_k(\\textbf{x}_i) + t_{k+1}(\\textbf{x}_i), y_i) &= \\exp(-y_i (f_k(\\textbf{x}_i) + t_{k+1}(\\textbf{x}_i))) \\\\\n                                                                                              &= \\exp(-y_i f_k(\\textbf{x}_i) -y_i t_{k+1}(\\textbf{x}_i))\\end{align}}$$\n\n                        <p>\n                            Before getting the Taylor series expansion, let's first compute the first and second derivatives of $\\mathscr{L}$. The first derivative is given by\n                        </p>\n\n                        $$\\small{\\begin{align} \n                            \\frac{d}{dt_{k+1}}\\mathscr{L}(f_k(\\textbf{x}_i) + t_{k+1}(\\textbf{x}_i), y_i) &= \\exp(-y_i f_k(\\textbf{x}_i) -y_i t_{k+1}(\\textbf{x}_i)) \\\\\n                             &= -y_i\\exp(-y_i f_k(\\textbf{x}_i) -y_i t_{k+1}(\\textbf{x}_i)) \\\\\n                        \\end{align}}$$\n\n                        <p>\n                            Evaluating this derivative at $t_{k+1}(\\textbf{x}_i) = 0$, we get\n                        </p>\n\n                        $$\\small{\\begin{align} \n                            -y_i\\exp(-y_i f_k(\\textbf{x}_i) -y_i t_{k+1}(\\textbf{x}_i))|_{t_{k+1}(\\textbf{x}_i) = 0} &=   -y_i\\exp(-y_i f_k(\\textbf{x}_i) -y_i (0))\\\\\n                             &= -y_i\\exp(-y_i f_k(\\textbf{x}_i)) \\\\\\end{align}}$$\n\n                        \n                        <p>\n                            Similarly, taking the second derivative of $\\mathscr{L}$, we get\n                        </p>\n\n                        $$\\small{\\begin{align} \n                            \\frac{d^2}{dt_{k+1}^2}\\mathscr{L}(f_k(\\textbf{x}_i) + t_{k+1}(\\textbf{x}_i), y_i) &= \\frac{d}{dt_{k+1}}(-y_i\\exp(-y_i f_k(\\textbf{x}_i) -y_i t_{k+1}(\\textbf{x}_i))) \\\\\n                             &= y_i^2\\exp(-y_i f_k(\\textbf{x}_i) -y_i t_{k+1}(\\textbf{x}_i)) \\\\\n                        \\end{align}}$$\n\n                        <p>\n                            Evaluating this derivative at $t_{k+1}(\\textbf{x}_i) = 0$, we get\n                        </p>\n\n                        $$\\small{\\begin{align} \n                            y_i^2\\exp(-y_i f_k(\\textbf{x}_i) -y_i t_{k+1}(\\textbf{x}_i))|_{t_{k+1}(\\textbf{x}_i) = 0} &=   y_i^2\\exp(-y_i f_k(\\textbf{x}_i) -y_i (0))\\\\\n                             &= y_i^2\\exp(-y_i f_k(\\textbf{x}_i)) \\\\\\end{align}}$$\n\n                        <p>\n                            We can now easily express our quadratic approximation of $\\mathscr{L}$ as\n                        </p>\n\n                        $$\\small{\\begin{align}\n                                \\widetilde{T}(t_{k+1}) = \\sum_{i=1}^{N}&-y_i\\exp(-y_i f_k(\\textbf{x}_i))\\cdot(t_{k+1}(\\textbf{x}_i))^1 \\\\\n                                &+\\frac{y_i^2\\exp(-y_i f_k(\\textbf{x}_i))}{2!}\\cdot(t_{k+1}(\\textbf{x}_i))^2 + R(t_{k+1})\n                        \\end{align}}$$\n\n                        <h4>Writing this in Terms of Tree Leaves</h4>\n\n                        <p>\n                            Now, we want to write this in terms of the leaves of the tree. Let's define a few functions real quick.\n                            Let $q:\\mathbb{R}^d\\to L$ be a function that maps a point $\\textbf{x}_i$ to the leaf $L$ that it falls into, and \n                            let $\\textbf{w}:L\\to\\mathbb{R}$ be a function that maps a leaf $L$ to a weight $w$.\n                            With these new functions, we can write $t_{k+1}(\\textbf{x}_i)$ as $\\textbf{w}(q(\\textbf{x}_i))$.\n                        </p>\n\n                        <p>\n                            Now, we can rewrite our quadratic approximation of $\\widetilde{T}$ as\n                        </p>\n\n                        $$\\small{\\begin{align}\n                                \\widetilde{T}(t_{k+1}(\\textbf{x}_i)) = \\sum_{i=1}^{N}&\\mathscr{L}^{\\prime}(\\hat{y}_i^k, y_i)\\cdot w(q(\\textbf{x}_i))^1 \\\\\n                                &+\\frac{\\mathscr{L}^{\\prime\\prime}(\\hat{y}_i^k + 0, y_i)}{2!}\\cdot w(q(\\textbf{x}_i))^2 + R(t_{k+1})\\end{align}}$$\n\n                        <p>\n                            For each of our leaves $\\ell \\in L$, we can define the set $I_{\\ell} = \\{i \\;|\\; q(\\textbf{x}_i = \\ell)\\}$ be the set indices $i$ where $\\textbf{x}_i$ falls into leaf $\\ell$ in tree $t_{k+1}$.\n                            We can now rewrite our quadratic approximation of $\\widetilde{T}$ as\n                        </p>\n\n                        $$\\small{\\begin{align}\n                                    \\widetilde{T}(t_{k+1}(\\textbf{x}_i)) &= \\sum_{i}^{N} \\left[\\mathscr{L}^{\\prime}(\\hat{y}_i^k, y_i)\\cdot w(q(\\textbf{x}_i))^1\n                        + \\frac{1}{2}\\mathscr{L}^{\\prime\\prime}(\\hat{y}_i^k, y_i)\\cdot w(q(\\textbf{x}_i))^2\\right] + \\gamma|L| + \\frac{1}{2}\\lambda\\sum_{\\ell\\in L}w_{\\ell}^2 \\\\\n                          \\end{align}}$$\n\n                        <p>\n                            Where the last two terms come from the regularization term $R(t_{k+1})$. The first term, $\\gamma|L|$, is the cost of adding a new leaf to our tree.\n                            The second term, $\\frac{1}{2}\\lambda\\sum_{\\ell\\in L}w_{\\ell}^2$, is the cost of the weights of our tree. Note, $\\gamma$ and $\\lambda$ are hyperparameters that control the strength of regularization.\n                        </p>\n\n                        <p>\n                            This can \"simplify\", however, into\n                        </p>\n\n                        $$\\small{\\begin{align}\n                                    \\widetilde{T}(t_{k+1}(\\textbf{x}_i)) &= \\sum_{\\ell\\in L} \\left(\\sum_{i\\in I_{\\ell}} \\mathscr{L}^{\\prime}(\\hat{y}_i^k, y_i)\\right)w_{\\ell}\n                        + \\frac{1}{2}\\sum_{\\ell\\in L}\\left(\\sum_{i\\in I_{\\ell}} \\mathscr{L}^{\\prime\\prime}(\\hat{y}_i^k, y_i)\\right)w_{\\ell}^2 + \\gamma|L| + \\frac{1}{2}\\lambda\\sum_{\\ell\\in L}w_{\\ell}^2\n                            \\end{align}}.$$\n                        \n                        <p>\n                            This is because we can group the terms by leaf $\\ell$ and then sum over all the leaves. But, if \n                            we look at the terms inside the parentheses, only one term is dependent on $i$ and the other is not. This means we can rewrite our quadratic approximation of $\\widetilde{T}$ as\n                        </p>\n\n                        $$\\small{\\begin{align}\n                                    \\widetilde{T}(t_{k+1}(\\textbf{x}_i)) &= \\sum_{\\ell\\in L}w_{\\ell} \\left(\\sum_{i\\in I_{\\ell}} \\mathscr{L}^{\\prime}(\\hat{y}_i^k, y_i)\\right)\n                        + \\frac{1}{2}\\sum_{\\ell\\in L}w_{\\ell}^2\\left(\\sum_{i\\in I_{\\ell}} \\mathscr{L}^{\\prime\\prime}(\\hat{y}_i^k, y_i)\\right) + \\gamma|L| + \\frac{1}{2}\\lambda\\sum_{\\ell\\in L}w_{\\ell}^2\n                            \\end{align}}$$\n\n                        <p>\n                            If we let $$F_{\\ell} = \\sum_{i\\in I_{\\ell}} \\mathscr{L}^{\\prime}(\\hat{y}_i^k, y_i)$$ and $$S_{\\ell} = \\sum_{i\\in I_{\\ell}} \\mathscr{L}^{\\prime\\prime}(\\hat{y}_i^k, y_i),$$ we can rewrite our quadratic approximation of $\\widetilde{T}$ as\n                        </p>\n\n                        $$\\small{\\begin{align}\n                                    \\widetilde{T}(t_{k+1}(\\textbf{x}_i)) &= \\sum_{\\ell\\in L}\\left(w_{\\ell} F_{\\ell} + \\frac{1}{2}(H_{\\ell} + \\lambda)w_{\\ell}^2\\right) + \\gamma|L| \\\\\n                                                                         &= \\sum_{\\ell\\in L}\\left(w_{\\ell} F_{\\ell} + \\frac{1}{2}(H_{\\ell} + \\lambda)w_{\\ell}^2 + \\gamma\\right)\n                            \\end{align}}$$\n                        \n                        <h4>Finding the Optimal Weights and Objective</h4>\n\n                        <p>\n                            With this notation down, we now seek to find the optimal weights $\\textbf{w}^*$ that minimize our quadratic approximation of $\\widetilde{T}$ derived above.\n                            We will do this by utilizing the most simple minimization technique: taking the derivative and setting it equal to zero.\n                        </p>\n\n                        <p>\n                            Taking $$ \\small{\\widetilde{T}(t_{k+1}(\\textbf{x}_i)) = \\sum_{\\ell\\in L}\\left(w_{\\ell} F_{\\ell} + \\frac{1}{2}(H_{\\ell} + \\lambda)w_{\\ell}^2\\right) + \\gamma|L|}$$\n                            and taking the derivative with respect to $w_{\\ell}$, we get\n\n                            $$\\small{\\begin{align}\n                                \\frac{d}{dw_{\\ell}}(\\widetilde{T}(t_{k+1}(\\textbf{x}_i))) &= \\frac{d}{dw_{\\ell}}\\left(\\sum_{\\ell\\in L}\\left(w_{\\ell} F_{\\ell} + \\frac{1}{2}(H_{\\ell} + \\lambda)w_{\\ell}^2\\right) + \\gamma|L|\\right) \\\\\n                                                                                          &= F_{\\ell} + (H_{\\ell} + \\lambda)w_{\\ell} \\\\\n                              \\end{align}} $$\n\n                            Setting this equal to zero and solving for $w_{\\ell}$, we get\n\n                            $$\\small{\\begin{align}\n                                F_{\\ell} + (H_{\\ell} + \\lambda)w_{\\ell} &= 0 \\\\\n                                (H_{\\ell} + \\lambda)w_{\\ell} &= -F_{\\ell} \\\\\n                                w_{\\ell} &= \\frac{-F_{\\ell}}{H_{\\ell} + \\lambda} \\\\\n                              \\end{align}} $$\n\n                            This gives us that our optimal weights are given by\n\n                            $$\\small{w_{\\ell}^* = \\frac{-F_{\\ell}}{H_{\\ell} + \\lambda}}.$$\n\n                            Now, we can plug this back into our quadratic approximation of $\\widetilde{T}$ to get\n\n                            $$\\small{\\begin{align}\n                                \\widetilde{T}(t_{k+1}(\\textbf{x}_i)) &= \\sum_{\\ell\\in L}\\left({\\color{red}w_{\\ell}} F_{\\ell} + \\frac{1}{2}(H_{\\ell} + \\lambda){\\color{red}w_{\\ell}}^2\\right) + \\gamma|L| \\\\\n                                                                         &= \\sum_{\\ell\\in L}\\left({\\color{red}\\frac{-F_{\\ell}^2}{H_{\\ell} + \\lambda}} + \\frac{1}{2}(H_{\\ell} + \\lambda)\\left({\\color{red}\\frac{-F_{\\ell}}{H_{\\ell} + \\lambda}}\\right)^2\\right) + \\gamma|L| \\\\\n                                                                         &= \\sum_{\\ell\\in L}\\left(\\frac{-F_{\\ell}^2}{H_{\\ell} + \\lambda} + \\frac{1}{2}\\cancel{(H_{\\ell} + \\lambda)}\\frac{F_{\\ell}^2}{(H_{\\ell} + \\lambda)^{\\cancel{2}}}\\right) + \\gamma|L| \\\\\n                                                                         &= \\sum_{\\ell\\in L}\\left(\\frac{-F_{\\ell}^2}{H_{\\ell} + \\lambda} + \\frac{1}{2}\\frac{F_{\\ell}^2}{H_{\\ell} + \\lambda}\\right) + \\gamma|L| \\\\\n                                                                         &= \\sum_{\\ell\\in L}\\left(\\frac{-F_{\\ell}^2}{2(H_{\\ell} + \\lambda)}\\right) + \\gamma|L| \\\\\n                                                                         &= \\sum_{\\ell\\in L}\\left(\\gamma - \\frac{F_{\\ell}^2}{2(H_{\\ell} + \\lambda)}\\right) \\\\\n                                                                            \\end{align}}$$\n                        \n                            Where the last step comes because  $$\\sum_{\\ell\\in L}\\gamma = \\gamma|L|. $$                                              \n                        </p>\n\n                        <p>\n                            Thus our optimal weights are given by $$\\small{w_{\\ell}^* = \\frac{-F_{\\ell}}{H_{\\ell} + \\lambda}}$$ and our optimal tree is given by\n\n                            $$\\small{\\begin{align}\n                                \\widetilde{T}(t_{k+1}(\\textbf{x}_i)) &= \\sum_{\\ell\\in L}\\left(\\gamma - \\frac{F_{\\ell}^2}{2(H_{\\ell} + \\lambda)}\\right) \\\\\n                                                                            \\end{align}}.$$\n                        </p>\n\n                        <p>\n                            Another interesting thing about this optimal tree is we can see how much each leaf $\\ell\\in L$ contributes to our objective! It is simply the term\n                            inside the summation, $$\\gamma - \\frac{F_{\\ell}^2}{2(H_{\\ell} + \\lambda)}.$$ This is important because it allows us to see not only which leaves are contributing the most to our objective function,\n                            but also what the cost of splitting a leaf is.\n                        </p>\n\n                        <p>\n                            If we were to split leaf $\\ell$, we would have to add two new leaves to our tree. This results in \n\n                            $$\\small{2\\gamma - \\frac{1}{2}\\left(\\frac{F_{\\ell_+}^2}{H_{\\ell_+} + \\lambda} + \\frac{F_{\\ell_-}^2}{H_{\\ell_-} + \\lambda}  \\right)}.$$\n\n                            However, our old leaf does not contribute to the objective anymore, so we subtract that out, resulting in our total cost of splitting leaf $\\ell$ as\n\n                            $$\\small{\\gamma - \\frac{1}{2}\\left(\\frac{F_{\\ell_+}^2}{H_{\\ell_+} + \\lambda} + \\frac{F_{\\ell_-}^2}{H_{\\ell_-} + \\lambda} - \\frac{F_{\\ell}^2}{H_{\\ell}+\\lambda}  \\right)}.$$\n                        </p>\n\n                        <p>\n                            When building our trees in XGBoost, instead of using a cost function such as the Gini index or entropy, we instead us the above cost function!\n                            This cost function has several benefits. Firstly, it will give much faster convergence than gradient descent without using the Hessian (which is a <b>huge</b> plus),\n                            regularization is built into the cost function which reduces overfitting!\n                        </p>\n\n                        <p>\n                            Now that we have a better grip behind the math of XGBoost, let's go through an exmaple implementing XGBoost in Python and compare it to random forests and normal gradient boosting!\n                        </p>\n\n                        <h4>Comparing the Cost Functions of XGBoost, Random Forests, and Gradient Boosted Trees</h4>\n\n                        <p>\n                            Before getting into our python example, let's first look at the cost functions of XGBoost, random forests, and gradient boosted trees.\n\n                            Recall from my blog post about <a href=\"https://dylanskinner65.github.io/blog/decision_trees.html\" target=\"_blank\" rel=\"noopener noreferrer\">decision trees</a> that one way\n                            to build our trees is to use the Gini index as our cost function. The Gini index is a good cost function for building random forests and is given by\n\n                            $$ G_{\\mathbb{D}_{\\ell}} = 1 - \\sum_{c\\in\\mathscr{Y}}\\left(\\frac{1}{N_{\\ell}}\\sum_{(\\mathbf{x}_i, y_i)\\in\\mathbb{D}_{\\ell}}\\mathbb{I}_{(y_i=c)}\\right)^2$$\n                        </p>\n\n                        <p>\n                            Now, the cost function for gradient boosted trees is given by\n\n                            $$f^* \\approx \\text{argmin}_{f\\in\\mathscr{F}}T(f) = \\text{argmin}_{f\\in\\mathscr{F}}\\frac{1}{N}\\sum_{i=1}^{N}\\mathscr{L}(f(\\textbf{x}_i), y_i),$$\n\n                            where we can iteratively find our next tree by\n\n                            $$f_{k+1} = f_k - \\alpha_k DT(f_k)^T.$$\n\n                            Where $DT(f_k)^T$ is the gradient of the loss function $\\mathscr{L}$ evaluated at $f_k$ and $\\alpha_k$ is the learning rate. (For more information, check out\n                            my blog post about <a href=\"https://dylanskinner65.github.io/blog/gradient_boosting.html\" target=\"_blank\" rel=\"noopener noreferrer\">gradient boosted trees</a>).\n\n                            And of course, the cost function for XGBoost is given by\n\n                            $$\\widetilde{T}(t_{k+1}(\\textbf{x}_i)) = \\sum_{\\ell\\in L}\\left(\\gamma - \\frac{F_{\\ell}^2}{2(H_{\\ell} + \\lambda)}\\right).$$\n\n                        </p>\n\n                        <a id=\"python-example\"></a>\n\n\n\n                        <h4>Python Example</h4>\n\n                        <p>\n                            For this example, we will be using the same dataset that we used in my blog post about <a href=\"https://dylanskinner65.github.io/blog/gradient_boosting.html\" target=\"_blank\" rel=\"noopener noreferrer\">gradient boosting</a>,\n                            the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_olivetti_faces.html#sklearn.datasets.fetch_olivetti_faces\" target=\"_blank\" rel=\"noopener noreferrer\">Olivetti Faces data</a>.\n                            Let's start off by importing XGBoost and this dataset.\n                        </p>\n\n<pre><code class=\"language-python\">import xgboost as xgb\nfrom sklearn.datasets import fetch_olivetti_faces</code></pre>\n\n                        <p>\n                            Now, let's load in the data and split it into training and testing sets.\n                        </p>\n\n<pre><code class=\"language-python\"># Import train test split\nfrom sklearn.model_selection import train_test_split\n\n# Load in the data\nfaces_X, faces_y = fetch_olivetti_faces(return_X_y=True, shuffle=True, random_state=1)\n\n# Perform train test split\nX_train, X_test, y_train, y_test = train_test_split(faces_X, faces_y, test_size=0.20, random_state=1)</code></pre>\n\n                        <p>\n                            Now, let's create our XGBoost model. We will use the <code>xgb.XGBClassifier()</code> class to create our model.\n                            We will use the default parameters, but do specify <code>objective=\"multi:softmax\"</code> (because this\n                            is a multiclass classification problem).\n                        </p>\n\n<pre><code class=\"language-python\"># Create the model\nxgb_classifier = xgb.XGBClassifier(objective=\"multi:softmax\", random_state=1)</code></pre>\n\n                        <p>\n                            Just like we did in the <a href=\"https://dylanskinner65.github.io/blog/gradient_boosting.html\" target=\"_blank\" rel=\"noopener noreferrer\">gradient boosting</a> article,\n                            we will fit our model, timing how long it takes to train.\n                        </p>\n\n<pre><code class=\"language-python\">%%timeit\nxgb_classifier.fit(X_train, y_train)\n>>> 15.1 s \u00b1 708 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)</code></pre>\n\n                        <p>\n                            We see it took 2 minutes and 1.1 seconds. Now, we will evaluate our model on the test set.\n                        </p>\n\n<pre><code class=\"language-python\"># Evaluate the model\nxgb_classifier.score(X_test, y_test)\n>>> 0.7625</code></pre>\n\n                        <p>\n                            Looking at a table comparing the results of XGBoost, gradient boosting, and random forests, we have\n                        </p>\n\n                        <table>\n                            <tr>\n                                <th>Model</th>\n                                <th>Time to Train</th>\n                                <th>Accuracy</th>\n                            </tr>\n                            <tr>\n                                <td>XGBoost</td>\n                                <td>2 minutes, 1.1 seconds</td>\n                                <td>76.25%</td>\n                            </tr>\n                            <tr>\n                                <td>Gradient Boosting</td>\n                                <td>88 minutes, 12 seconds</td>\n                                <td>53.75%</td>\n                            </tr>\n                            <tr>\n                                <td>Random Forests</td>\n                                <td>10.9 seconds</td>\n                                <td>91.25%</td>\n                            </tr>\n                        </table>\n\n                        <p>\n                            Clearly Random Forests is the best option, which is a bit of a surprise! This is not always the case.\n                        </p>\n\n                        <p>\n                            In a previous project I participated in where we tried to <a href=\"https://github.com/jeffxhansen/NYC_Taxi_Trip_Duration/blob/main/NYC%20Taxi%20Duration%20Prediction.ipynb\" target=\"_blank\" rel=\"noopener noreferrer\">predict taxi duration times</a>, \n                            we found that XGBoost was the best model and random forests was second best. Thus, it is important to try different models and different model parameters\n                            to find the best model for your data!\n                        </p>\n\n\n                        <h4>Conclusion</h4>\n                        <p>\n                            In this article, we went deep into the math behind newton boosted trees, the backbone of XGBoost. We discussed how to find\n                            the optimal cost function for our trees without the Hessian, and how avoiding overfitting is built into the cost function.\n                            We then went through an example of implementing XGBoost in Python and compared it to gradient boosting and random forests.\n                            I hope that through this article, you now understand XGBoost especially why it is so successful (which most people don't)!\n                            If you want to see the code used in this article, you can find it\n                            on my <a href=\"https://github.com/dylanskinner65/dylanskinner65.github.io/blob/main/blog/blog_files/xgboost/xgboost.ipynb\" target=\"_blank\" rel=\"noopener noreferrer\">Github</a>.\n                            I hope that you choose to try out XGBoost in your next project!\n                        </p>",
      "quote": "XGBoost",
      "quoteAuthor": "Bojan Tunguz",
      "category": "Math & ML"
    }
  ],
  "projects": [
    {
      "slug": "causal-inference",
      "title": "Causal Inference with Graph Neural Networks",
      "date": "2024-04-24",
      "description": "A project where we try to find causal inference in graph neural networks (GNNs).",
      "content": "<p>This project was a joint effort by me, and my friends <a href=\"https://www.linkedin.com/in/jasonwvasquez/\">Jason Vasquez</a>, <a href=\"https://www.linkedin.com/in/gwen-martin-98057b220/\">Gwen Martin</a>,\n                            and <a href=\"https://www.linkedin.com/in/dallinstewart/\">Dallin Stewart</a>. When I say \"we,\" I am talking about all of us together.</p>\n\n                        <p>Neural networks have revolutionized computer science by modeling complex data relationships, yet the need \n                            to elucidate their predictions and understand underlying causality grows increasingly crucial. \n                            This project explores the integration of causal inference, traditionally rooted in statistics and philosophy, \n                            into neural networks, particularly focusing on Graph Neural Networks (GNNs) for causal modeling. \n                            Leveraging GNNs' capabilities in learning graph-structured data representations, we conduct supervised classification \n                            tasks on the LUCAS0 dataset to assess GNNs' ability to capture causal relationships and enhance interpretability and robustness. \n                            Results reveal varying accuracies across GNN architectures, with Transformer convolutional layers showing the highest performance. \n                            Comparisons against Bayesian networks, which exploit causal relationships, demonstrate superior accuracy with less training time. \n                            Moreover, employing \\textbf{do}-calculus exposes GNNs' limitations in discerning precise causal relationships, underscoring \n                            the importance of causal inference for improving neural networks' predictive power. This study advocates for deeper integration \n                            of causal inference into neural network research to foster more interpretable and reliable AI systems, suggesting \n                            future exploration of diverse datasets and model architectures to validate and extend these findings.</p>\n\n\n                        <figure>\n                            <img src=\"projects_files/causal_inference/visualized_graph.jpg\" alt=\"A visualized graph.\" width=\"90%\" height=\"90%\">\n                            <figcaption>This image was found on <a href=\"https://www.reddit.com/r/dataisbeautiful/comments/78vo65/visualizing_neural_networks_as_large_directed/\">Reddit</a>.</figcaption>\n                        </figure>\n                        \n                        <p></p>\n\n                        <h4>\n                            Introduction\n                        </h4>\n\n                        <p>\n                            Neural networks are at the center of computer science research and have revolutionized our ability to model complex relationships with data.\n                            As these networks continue to proliferate in decision-making processes, output explanation and gaining a deeper understanding of the causality behind their predictions become increasingly essential. \n                            The distinction between correlation and causation is not merely philosophical; it creates the foundation for building robust, interpretable, and ethically sound AI systems.\n                        </p>\n\n                        <p>\n                            Causal inference is a field deeply rooted in statistics and philosophy that offers a framework to disentangle cause-and-effect relationships from observed data. \n                            Traditionally, researchers have applied causal inference in fields including epidemiology, economics, and social sciences. \n                            However, integrating these ideas into the domain of neural networks presents a promising frontier with implications ranging from improving model interpretability to enhancing the fairness and accountability of AI systems.\n                        </p>\n\n                        <p>\n                            In this project, we explore the intersection of causal inference and neural networks by focusing on the application of Graph Neural Networks (GNNs) to causal modeling.\n                            We leverage these unique capabilities of GNNs to learn representations of graph-structured data and perform a supervised classification task on the LUCAS0 dataset <a href=\"lucas_dataset\">[7]</a>.\n                            Our goal is to determine if GNNs can capture causal relationships within a graph and provide insights into the interpretability and robustness of these models. \n                            We also use a Bayesian network as a baseline machine learning model that leverages causal relationships to compare against the neural network.\n                        </p>\n\n                        <h4>Related Work</h4>\n\n\n                        <p>\n                            Integrating causal inference with neural networks has garnered significant interest in recent years.\n                            Other studies have proposed several approaches to combine the strengths from these two fields and leverage the expressive power of neural networks to model causal relationships.\n                            Koch et al. discusses ongoing work to extend causal inference to settings where confounding is non-linear, time-varying, or encoded in text, networks, and images <a href=\"koch2023\">[4]</a>. \n                            Furthermore, Yuan et al. compared the performance of CNN's on causal data to previous methods <a href=\"yuan2020\">[10]</a>.\n                        </p>\n\n                        <h4>Graph Neural Networks</h4>\n\n                            <p>\n                                The Graph Neural Networks that Mori et al. <a href=\"gori2005new\">[2]</a>, originally proposed in 2005 are a class of neural networks that operate on graph-structured data. \n                                They have gained significant attention in recent years due to their versatility in handling various types of graph data, including social networks, citation networks, biological networks, and more. \n                            </p>\n\n                            <p>\n                                Unlike traditional neural networks, which operate on grid-like structures such as images or sequences, GNNs' architectures allow \n                                them to capture and leverage the structural information present in graphs. One remarkable feature allows them to learn meaningful \n                                representations of nodes in a graph for various downstream tasks such as node classification, link prediction, and graph classification. \n                                Their ability to capture and model complex relationships in graph data makes them invaluable tools for exploring \n                                and understanding real-world phenomena represented in graph form.\n                            </p>\n\n                            <p>\n                                In this project, we apply GNNs to a supervised classification \n                                task by training it on a labeled, tabular dataset, where we associate each graph instance (or node) with a target label.\n                            </p>\n\n                           <h4>$\\textbf{do}$ Operator</h4>\n\n                           <p>\n                            The $\\textbf{do}$ operator is a way to represent interventions in a causal model, i.e. the effect of an intervention on a variable. For example, consider the following model involving smoking.\n                           </p>\n\n                           <p>\n                            If a person's fingernails $(N)$ have turned yellow, this implies a higher probability that they are a heavy smoker $(S)$ \n                            and hence have a higher probability of developing lung cancer $(C)$. However, simply dyeing someone's fingernails yellow does not impact their probability of developing lung cancer. \n                           </p>\n\n                           <p>\n                            So, in terms of $\\textbf{do}$ calculus, we can denote the process of setting a variable $N$ to have a value $\\textit{yellow}$ by $\\textbf{do}(N = \\textit{yellow})$. \n                            We note that \n\n                            $$P(C \\;|\\;N = \\textit{yellow}) \\neq P(C\\;|\\; \\textbf{do}(N=\\textit{yellow})).$$\n\n                            With this in mind, we now define the $\\textbf{do}$ operator.\n\n                            <div class=\"theorem\">\n                                <p class=\"theorem-title\">Theorem (<a href=\"pearl2009causal\">[6]</a>):</p>\n                                <p>In a causal diagram \\( \\Gamma \\) with nodes \\( X_1, \\dots, X_n \\) and joint distribution \n                                \\( P(X_1, \\dots, X_n) \\), the result of doing \\( X_i = x_i \\) on the joint distribution is:</p>\n                            \n                                \\[\n                                    P(X_1, \\dots, X_n \\;|\\; \\textbf{do}(X_i = x_i)) = \\frac{P(x_1,\\dots,x_n)}{P(x_i\\;|\\; \\textup{par}(x_i))} = \\prod_{j\\neq i}P(x_j\\;|\\; \\textup{par}(x_j)).\n                                \\]\n                            </div>\n\n                            In this equation, $\\text{par}(x_i)$ represents values of the parent nodes of $\\text{PAR}(X_i)$ of $X_i$ in $\\Gamma$.\n                            We call the probabilities on the right hand side of the above equation <i>preintervention</i>. \n                            Preintervention means that we use the probabilities from the original model before \"doing\" $X_i = x_i$.\n                           </p>\n\n                           <p>\n                            The equation above describes how we calculate the probability of several events happening given one event has happened. What if we want to find the probability of a single event happening, \n                            given we do a single event? This scenario leads to the following corollary.\n                           \n\n                           <div class=\"theorem\">\n                            <p class=\"theorem-title\">Corollary:</p>\n                            <p>    If $X$ and $Y$ are random variables in a causal diagram $\\Gamma$ and $\\textup{PAR}(X)$ are the parents of $X$, then\n\n                                \\[P(y\\;|\\;  \\textbf{do}(x)) = \\sum_{\\textup{par}}\\frac{P(x,\\,y,\\,\\textup{par})}{P(x\\;|\\; \\textup{par})},\\]\n                            \n                                where the sum runs over all values $\\textup{par}$ that the variables $\\textup{PAR}(X)$ can take. If $X$ has no parents, then\n                            \n                                \\[P(y\\;|\\; \\textbf{do}(x)) = \\frac{P(x,\\,y)}{P(x)} = P(y\\;|\\; x).\\]\n                        </div>\n\n                            Let us now consider a basic example to see how this works. Consider the following causal diagram in Figure 1.\n                        </p>\n\n                        <figure>\n                            <img src=\"projects_files/causal_inference/figure1.png\" alt=\"Basic directional graph.\" width=\"90%\" height=\"90%\">\n                            <figcaption><b>Figure 1:</b> Basic causal diagram. Note it is in the form of a directed acyclic graph (DAG).</figcaption>\n                        </figure>\n\n                        <p>\n                            In this diagram, nodes $A$ and $C$ are both parents of $B$. So, for any values of $x$ and $b$, \n                            the above Corollary tells us that\n\n                            $$P(X = x \\,|\\, \\textbf{do}(B=b)) = \\sum_{\\text{par}(b)}\\frac{P(x, \\,b, \\,\\text{par}(b))}{P(b\\,|\\, \\text{par}(b))}$$\n\n                            which, written out, is \n\n                            $$\\sum_{\\text{par}(b)}\\frac{P(x, \\,b, \\,\\text{par}(b))}{P(b\\,|\\, \\text{par}(b))} = \\sum_{a}\\sum_{c}\\frac{P(X=x\\,, \\, A=a\\,,\\, B=b \\, ,\\, C=c)}{P(B=b \\;|\\; A=a,\\, C=c)}.$$\n\n                            Since each node only depends conditionally on their parents, we can rewrite the expression as\n\n                            $$\\sum_{a}\\sum_{c}\\frac{P(X=x\\;|\\; A=a\\,,\\, B=b)P(B=b\\;|\\; A=a,\\, C=c)P(A=a)P(C=c)}{P(B=b \\;|\\; A=a,\\, C=c)},$$\n\n                            which simplifies to\n\n                            $$\\sum_{a}\\sum_{c}P(X=x\\;|\\; A=a\\,,\\, B=b)P(A=a)P(C=c).$$\n\n                            Since there is only one instance where we are considering the probability with respect to $c$, we can further reduce this to\n\n                            $$\\sum_{a}P(X=x\\;|\\; A=a\\,,\\, B=b)P(A=a),$$\n\n                            which is our final answer.\n                        </p>\n                            While this introduction to the $\\textbf{do}$ operator might feel a bit abstract, it is the foundation of all current research in causal inference. \n                        </p>\n\n                        <h4>Data</h4>\n\n                        <p>\n                            For our project, we used the LUCAS0 dataset <a href=\"lucas_dataset\">[7]</a>, which is a synthetic toy data set with causal Bayesian networks and binary variables. \n                            The LUCAS0 dataset is a DAG with 11 nodes and 2000 training samples (see Figure 2). \n                        </p>\n\n                        <figure>\n                            <img src=\"projects_files/causal_inference/figure2.png\" alt=\"The dataset.\" width=\"90%\" height=\"90%\">\n                            <figcaption><b>Figure 2:</b> Basic causal diagram. Note it is in the form of a directed acyclic graph (DAG). Our target variable\n                                is shaded in <span style=\"color:pink\"><b>pink</b></span>, and the nodes in <span style=\"color:teal\"><b>teal</b></span> constitute the Markov\n                                blanket of the target variable.</figcaption>\n                        </figure>\n\n                        <p>\n                            We associate each node of the graph with a specific conditional probability that the creators of the dataset used to generate the data (the end of this post).\n                        </p>\n\n\n                        <h4>Experiments and Testing</h4>\n\n                        <p>\n                            Graph Neural Networks and Bayesian Networks perform well when handling complex structured data that consists of underlying causal relationships and do calculus operations. \n                            By examining these models more closely, we can assess their predictive power in capturing these underlying do-calculus operations. \n                            In order to do so, we train these models to predict lung cancer based on the components of the graph provided in Figure 2 and evaluate their accuracy. \n                        </p>\n\n                        <h5>Graph Convolutional Layers</h5>\n\n                        <p>\n                            We begin by training several graph neural networks with five different types of convolutional layers and performing a grid search of learning rates (0.001, 0.005, 0.01, 0.05). \n                            Each model includes a unique convolutional layer variant as at least one of seven convolutional layers. Each layer is followed by a ReLU activation function and dropout regularization, \n                            except the final layer. We describe each of these convolutional layers below.\n                        </p>\n\n                        <p>\n                            The first type of layer we experiment with is a standard graph convolution layer acting as a control for comparison against the other layers. \n                            The other four layer types we test are Chebyshev convolution model, SAGE convolution model, GAT convolution model and a Transformer convolution model. \n                        </p>\n\n                        <p>\n                            <a href=\"Defferrard2017CNNGraphs\">[1]</a> describes a method of using Chebyshev polynomials for rapid filtering in graph neural networks. While previous methods of filtering signals in graphs require expensive computations, \n                            <a href=\"Defferrard2017CNNGraphs\">[1]</a>  implements Chebyshev polynomials in order to approximate kernels. These functions are easy to work with and recursively compute an orthogonal basis efficiently. The resulting combination of these Chebyshev terms can effectively represent the new filter function. \n                        </p>\n\n                        <p>\n                            Researchers created SAGE convolutional layers to generate embeddings using node features such as various attributes, profile information and graph structure <a href=\"Hamilton2018InductiveGraphs\">[3]</a>>. \n                            The algorithm achieves this embedding scheme with a forward propagation algorithm that uses  $K$ parameters of the model to search through the nodes. \n                            It then creates the node's embedding by sampling and aggregating the graph information. This approach provides better generalization to unseen nodes and facilitates node feature learning in a network. \n                            This algorithm is popular because of its robustness to complex node features and information. \n                        </p>\n\n                        <p>\n                            GATConv uses attention mechanisms in order to perform node classification. <a href=\"GraphAttentionNetworks2018\">[9]</a> created this type of specialized attention layer for graph neural networks, \n                            and explained that \"One of the benefits of attention mechanisms is that they allow for dealing with variable sized inputs, focusing on the most relevant parts of the input to make decisions\"  <a href=\"GraphAttentionNetworks2018\">[9]</a>. \n                        </p>\n\n                        <p>\n                            Researchers first adopted Graph Transformer convolution layers in an attempt to combine feature propagation and label propagation as discussed in <a href=\"MaskedLabelPrediction2021\">[8]</a>. \n                            The Transformer Convolution layer relies on a general vanilla structure of a transformer while also accounting for edge features. This layer is similar to the GATConv layer since they both use multi-headed attention.\n                        </p>\n\n                        <h5>Bayesian Network</h5>\n\n                        <p>\n                            To further investigate the ability of neural networks to learn causal relationships, we compared the results of the neural network with a Bayesian network. \n                            A Bayesian network can include causal relationships implicitly in its initialization before it fits the data. \n                            If the Bayesian network performed better than the neural networks, then we can infer that the neural networks are not able to fully grasp causal relationships. \n                            We can also infer that learning causal relationships are essential for higher accuracy when such a dependency exists in the data.\n                        </p>\n\n                        <p>\n                            Bayesian networks, also known as belief networks or directed acyclic graphical models, are probabilistic graphical models that represent a set of variables and their conditional \n                            dependencies via a directed acyclic graph (DAG). Each node in the graph represents a random variable, and the edges between nodes represent probabilistic dependencies, indicating direct influences or causal relationships between variables.\n                        </p>\n\n                        <p>\n                            One of the key features of Bayesian networks is their ability to model uncertainty and reason under ambiguity using Bayesian inference. By leveraging Bayes' theorem, Bayesian networks can update beliefs about variables based on observed evidence and allow for efficient probabilistic reasoning.\n                        </p>\n\n                        <p>\n                            When comparing Bayesian networks with neural networks in learning causal relationships, Bayesian networks offer a structured framework for explicitly modeling causal dependencies between variables. We can encode prior knowledge or assumptions about causal relationships into the network structure to facilitate causal inference and reasoning. Furthermore, Bayesian networks can provide insights into causal mechanisms that may not be fully captured by neural networks alone. This source of insights is particularly salient in tasks where understanding causal relationships is crucial, such as predictive modeling in domains like healthcare or finance.\n                        </p>\n\n                        <h4>Results</h4>\n\n                        <h5>Convolutional Layers and Learning Rate</h5>\n\n                        <p>\n                            While the initial accuracies provided in Table 1 appear relatively high, the performance of these models have several faults. \n                            The total percentage of lung cancer patients in this dataset is $0.7215$, so models with this level of accuracy are likely predicting ``True\" each time rather than learning from the data. \n                            This behavior will cause the model to generalize poorly on other unseen data. With this in mind, both the standard and Chebyshev convolution yield insignificant results. The Transformer convolution provides the most accurate results at the expense of longer training time. \n                        </p>\n\n                        <table border=\"1\">\n                    \n                            <thead>\n                              <tr>\n                                <th rowspan=\"2\">Convolutional Layers</th>\n                                <th colspan=\"4\">Learning Rates</th>\n                              </tr>\n                              <tr>\n                                <th>0.001</th>\n                                <th>0.005</th>\n                                <th>0.01</th>\n                                <th>0.05</th>\n                              </tr>\n                            </thead>\n                            <tbody>\n                              <tr>\n                                <td>Standard Graph Convolution</td>\n                                <td>0.725</td>\n                                <td>0.8325</td>\n                                <td>0.725</td>\n                                <td>0.725</td>\n                              </tr>\n                              <tr>\n                                <td>Chebyshev Convolution</td>\n                                <td>0.721</td>\n                                <td>0.725</td>\n                                <td>0.725</td>\n                                <td>0.658</td>\n                              </tr>\n                              <tr>\n                                <td>SAGE Convolution</td>\n                                <td><b>0.8675</b></td>\n                                <td>0.8575</td>\n                                <td>0.865</td>\n                                <td>0.725</td>\n                              </tr>\n                              <tr>\n                                <td>GAT Convolution</td>\n                                <td>0.8125</td>\n                                <td>0.85</td>\n                                <td>0.855</td>\n                                <td>0.725</td>\n                              </tr>\n                              <tr>\n                                <td>Transformer Convolution</td>\n                                <td>0.8625</td>\n                                <td><b>0.875</b></td>\n                                <td><b>0.8775</b></td>\n                                <td><b>0.8675</b></td>\n                              </tr>\n                            </tbody>\n                            <caption><b>Table 1:</b> Models by Convolutional Layers and Learning Rates</caption>\n                          </table>\n\n                          <p></p>\n\n                          <table border=\"1\">\n                            <thead>\n                              <tr>\n                                <th>Convolutional Layers</th>\n                                <th>Avg Training Time (s)</th>\n                              </tr>\n                            </thead>\n                            <tbody>\n                              <tr>\n                                <td>Standard Graph Convolution</td>\n                                <td>0.98</td>\n                              </tr>\n                              <tr>\n                                <td>Chebyshev Convolution</td>\n                                <td>0.713</td>\n                              </tr>\n                              <tr>\n                                <td>SAGE Convolution</td>\n                                <td>12.457</td>\n                              </tr>\n                              <tr>\n                                <td>GAT Convolution</td>\n                                <td>9.982</td>\n                              </tr>\n                              <tr>\n                                <td>Transformer Convolution</td>\n                                <td>9.189</td>\n                              </tr>\n                            </tbody>\n                            <caption><b>Table 2:</b> Models by Convolutional Layers and Average Training Times</caption>\n                          </table>\n                          \n\n                          <h5>Bayesian Networks</h5>\n\n                          <p>\n                            For the Bayesian network, we initialized the model with the causal relationships on the dataset defined above, split the data randomly into $80\\%$ training data and $20\\%$ test data, \n                            then fit the model on the training data, which took $0.09$ seconds. The model was able to predict cancer with $86.25\\%$ accuracy on the test set, beating every GNN we tested except for the Transformer models. \n                          </p>\n\n\n                          <h5>Evaluating Models with $\\textbf{do}$ Calculus</h5>\n\n                          <p>\n                            Let's consider the following conditional probabilities with $\\textbf{do}$ operators applied:\n\n                            \\[\n                            \\begin{aligned}\n                                P(\\text{LC} = \\text{T} \\;&|\\; \\mathbf{do}(\\text{YF} = \\text{T})), \\\\\n                                P(\\text{LC} = \\text{T} \\;&|\\; \\mathbf{do}(\\text{PP} = \\text{T})), \\\\\n                                P(\\text{LC} = \\text{T} \\;&|\\; \\mathbf{do}(\\text{A} = \\text{T})), \\\\\n                                P(\\text{LC} = \\text{T} \\;&|\\; \\mathbf{do}(\\text{AD} = \\text{T})), \\\\\n                                P(\\text{LC} = \\text{T} \\;&|\\; \\mathbf{do}(\\text{CA} = \\text{T})), \\\\\n                            \\end{aligned}\n                            \\]\n\n                            where LC is lung cancer, YF is yellow fingers, PP is peer pressure, A is anxiety, AD is attention disorder, and CA is car accident. \n                            We note that we chose this probability because it does not involve any variables that are in the Markov blanket of our target variable. \n                            Thus, we should expect that there is very little predictive power in this probability.\n                          </p>\n\n                          <p>\n                            Following the $\\textbf{do}$ calculus algorithm, we write $P(\\text{LC} = \\text{T} \\;|\\; \\textbf{do}(\\text{YF} = \\text{T}))$ (using the corollary) as\n\n                            \\[\n                            \\begin{equation}\n                            P(\\text{LC} = \\text{T} \\;|\\; \\textbf{do}(\\text{YF} = \\text{T})) = \n                            \\sum_{s\\in\\{T, F\\}} \\frac{P(\\text{YF} = \\text{T},\\, \\text{LC} = \\text{T},\\, \\text{S} = s)}{P(\\text{YF} = \\text{T}\\,|\\, \\text{S} = s)},\n                            \\label{yf1}\n                        \\end{equation}\n                            \\]\n\n                            where $\\text{S}$ is smoking. Since the Lung Cancer node has Smoking and Genetics as parents, and Smoking\nhas parent Anxiety and Peer Pressure (see Figure 2), we can write the above equation to\n\n                            \\[  \n                            \\begin{equation}\n                                \\sum_{S, G, A, PP}\\frac{P(\\text{YF} =\\text{T}\\;|\\; \\text{S}) P(\\text{LC} \n                                = \\text{T}\\;|\\; \\text{S},\\, \\text{G}) P(\\text{G}) P(\\text{S}\\;|\\;\\text{A},\\, \\text{PP}) P(\\text{A})P(\\text{PP})}\n                                {P(\\text{YF} = \\text{T}\\;|\\; \\text{S})}.\n                                \\label{yf2}\n                            \\end{equation}\n                            \\]\n\n                            where each $\\text{S},\\text{A}, \\text{G}, \\text{PP}$ is in terms of $\\{T, F\\}$.\n                          </p>\n\n                          <p>\n                            Since not every term is in terms of every variable, we can rewrite the above equation as \n\n                            \\[\n                            \\begin{align*}\n                                &=\\sum_{S} \\frac{P(\\text{YF} = T\\,|\\, S)}{P(\\text{YF} = T\\,|\\, \\text{S})}\n                                    \\Biggl(\\sum_{G} P(\\text{LC} = \\text{T}\\,|\\, \\text{S}, \\text{G})P(\\text{G})\\Biggr)\n                                        \\Biggl(\\sum_{A} P(\\text{A}) \n                                            \\Bigl(\\sum_{\\text{PP}}P(\\text{S}\\,|\\, \\text{A}, \\text{PP})P(\\text{PP})\\Bigr) \n                                        \\Biggr) \\\\\n                                &=\\sum_{S}\\Big(\\sum_{G} P(\\text{LC} = \\text{T}\\,|\\, \\text{S}, \\text{G})P(\\text{G})\\Big)\n                                    \\Biggl(\\sum_{A} P(\\text{A}) \n                                        \\Bigl(\\sum_{\\text{PP}}P(\\text{S}\\,|\\, \\text{A}, \\text{PP})P(\\text{PP})\\Bigr) \n                                    \\Biggr).\\label{yf3}\n                            \\end{align*}\n                            \\]\n\n                            Using the probabilities from Table 5 on the above equation we get\n\n                            \\[\n                            \\begin{equation*}\n                                P(\\text{LC} = \\text{T} \\;|\\; \\textbf{do}(\\text{YF} = \\text{T})) = 0.7363\n                            \\end{equation*}\n                            \\]\n                          </p>\n\n                          <table>\n                          <caption><b>Table 3:</b> The resulting probabilities of the <b>do</b> statements found in the above equation.</caption>\n                          <tr>\n                              <th><i>do Statements</i></th>\n                              <th><i>Probabilities</i></th>\n                          </tr>\n                          <tr>\n                              <td>\\( P(\\text{LC} = \\text{T} \\mid \\mathbf{do}(\\text{A} = \\text{T})) \\)</td>\n                              <td>0.5158</td>\n                          </tr>\n                          <tr>\n                              <td>\\( P(\\text{LC} = \\text{T} \\mid \\mathbf{do}(\\text{S} = \\text{T})) \\)</td>\n                              <td>0.8639</td>\n                          </tr>\n                      </table>\n\n                      <p>\n                        These probabilities are what we expect based on the causal diagram. For example, the probability of cancer given $\\textbf{do}$(YF = T) is hardly higher than the probability of lung cancer on its own. \n                        This helps us understand that while cancer patients are more likely to have yellow fingers, yellow fingers themselves do not cause cancer. However the probability of cancer given $\\textbf{do}$(S = T) is much higher, implying that smoking does cause cancer.\n                      </p>\n\n                      <p>\n                        We compare these probabilities to the probability output by our GNN in Table 4. To calculate these probabilities, we took a data point with False for every feature except for one feature artificially set to True, and then passed this data point through our model. \n                        The discrepancy in the probabilities seems to imply that our neural network was unable to pickup on causality, as it assumed that everyone with yellow fingers has cancer (even absent smoking).\n                      </p>\n\n                      <table>\n                        <caption>The resulting probabilities of the <b>do</b> statements running the data points through the Transformer model with a learning rate of 0.01 as seen above.</caption>\n                        <tr>\n                            <th><i>do Statements</i></th>\n                            <th><i>Probabilities</i></th>\n                        </tr>\n                        <tr>\n                            <td>\\( P(\\text{LC} = \\text{T} \\mid \\mathbf{do}(\\text{YF} = \\text{T})) \\)</td>\n                            <td>1.000</td>\n                        </tr>\n                        <tr>\n                            <td>\\( P(\\text{LC} = \\text{T} \\mid \\mathbf{do}(\\text{A} = \\text{T})) \\)</td>\n                            <td>0.1329</td>\n                        </tr>\n                        <tr>\n                            <td>\\( P(\\text{LC} = \\text{T} \\mid \\mathbf{do}(\\text{S} = \\text{T})) \\)</td>\n                            <td>0.9565</td>\n                        </tr>\n                    </table>\n\n                    <h4>Conclusion</h4>\n\n                    <p>\n                        This project seeks to examine how neural networks can be used to perform do-calculus and causal inference.\n                         Graph neural networks have gained significant popularity in their ability to handle complex relationships and capture meaningful node representations. \n                         These networks achieve successful results in both computational efficiency and accuracy, with a Transformer convolutional layer model leading in performance. \n                         Bayesian networks also prove that a model that understands the causal relationships between the data can outperform most neural networks in much less time. \n                         Furthermore, using $\\textbf{do}$ calculus we conclude that neural networks are unable to pickup on the exact nature of causal relationships. \n                         These results indicate that the field of causal inference is very important in neural networks and the better that neural networks can understand causal relationships, the higher the predictive power.\n                    </p>\n\n                    <p>\n                        We recognize that there is significant room for improvement especially with a topic as complicated as causal inference in deep learning. \n                        For future work, we hope to experiment with a wider variety of datasets to explore how other graphical relationships perform with our models. \n                        Given that our testing is confined to this dataset and lacks sufficient validation across other graphs and model performances, \n                        we suggest that future work couple these results with other reliable studies in making important claims such as lung cancer detection.\n                    </p>\n                    \n\n                        \n\n                        <h4>Citations</h4>\n\n                        <ol>\n                            <li id=\"Defferrard2017CNNGraphs\">Defferrard, Micha\u00ebl, Xavier Bresson, and Pierre Vandergheynst. \"Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering.\" 2017. <a href=\"https://arxiv.org/pdf/1606.09375.pdf\">Paper link</a>.</li>\n\n                            <li id=\"\">Gori, Marco, Gabriele Monfardini, and Franco Scarselli. \"A new model for learning in graph domains.\" <em>Proceedings of the International Joint Conference on Neural Networks</em>, vol. 2, 2005, pp. 729\u2013734. <a href=\"https://doi.org/10.1109/IJCNN.2005.1555942\">DOI: 10.1109/IJCNN.2005.1555942</a>.</li>\n\n                            <li id=\"Hamilton2018InductiveGraphs\">Hamilton, William L., Rex Ying, and Jure Leskovec. \"Inductive Representation Learning on Large Graphs.\" 2018. <a href=\"https://arxiv.org/pdf/1706.02216.pdf\">Paper link</a>.</li>\n\n                            <li id=\"koch2023\">Koch, Bernard, et al. \"Deep Learning of Potential Outcomes.\" <em>CoRR</em>, vol. abs/2110.04442, 2021. <a href=\"https://arxiv.org/abs/2110.04442\">arXiv: 2110.04442</a>.</li>\n\n                            <li id=\"koller2009probabilistic\">Koller, Daphne, and Nir Friedman. <em>Probabilistic Graphical Models: Principles and Techniques</em>. MIT Press, 2009.</li>\n\n                            <li id=\"pearl2009causal\">Pearl, Judea. \"Causal Inference in Statistics: An Overview.\" <em>Statistics Surveys</em>, vol. 3, 2009, pp. 96\u2013146. <a href=\"https://doi.org/10.1214/09-SS057\">DOI: 10.1214/09-SS057</a>.</li>\n                        \n                            <li id=\"lucas_dataset\">Research Group for Lung Cancer Analysis. \"LUCAS (LUng CAncer Simple set) Dataset.\" ETH Zurich Causality and Machine Learning Group, 2020. <a href=\"https://www.causality.inf.ethz.ch/data/LUCAS.html\">Dataset link</a>.</li>\n\n                            <li id=\"MaskedLabelPrediction2021\">Shi, Yunsheng, et al. \"Masked Label Prediction: Unified Message Passing Model for Semi-Supervised Classification.\" 2021. <a href=\"https://arxiv.org/pdf/2009.03509.pdf\">Paper link</a>.</li>\n\n                            <li id=\"GraphAttentionNetworks2018\">Velickovi\u0107, Petar, et al. \"Graph Attention Networks.\" 2018. <a href=\"https://arxiv.org/pdf/1710.10903.pdf\">Paper link</a>.</li>\n\n                            <li id=\"yuan2020\">Yuan, Ye, Xueying Ding, and Ziv Bar-Joseph. \"Causal inference using deep neural networks.\" <em>CoRR</em>, vol. abs/2011.12508, 2020. <a href=\"https://arxiv.org/abs/2011.12508\">arXiv: 2011.12508</a>.</li>\n                        \n                        </ol>\n                    \n\n                        <!-- <ol>\n                            <span id=\"adams\">[1]</span> Collin C. Adams, <em>The Knot Book</em>. American Mathematical Society, 2004. ISBN: 978-0821836781.\n                        </ol>\n                        <ol\n                            <span id=\"birman\">[2]</span> Joan S Birman. <em>Braids, links, and mapping class groups</em>. 82. Princeton University Press, 1974.\n                        </ol>    -->\n\n                        <!-- <ol>\n                            <span id=\"seifert\">[1]</span> Heinrich Seifert. <em>\u00dcber das Geschlecht von Knoten</em>. In: <em>Mathematische Annalen</em> 110 (1935), pp. 571\u2013592. DOI: 10.1007/BF01448044.\n                        </ol> -->\n\n                        <!-- <p>\n                            <a id=\"adams\"></a>Adams, C. C. (1994). The Knot Book: An Elementary Introduction to the Mathematical Theory of Knots. W. H. Freeman and Company. -->",
      "quote": "Correlation does not imply causation.",
      "quoteAuthor": "Everyone at some point",
      "tags": [
        "GNN",
        "Causality",
        "Python"
      ],
      "field": "Structural Causality"
    },
    {
      "slug": "deep-knots-rl",
      "title": "Deep Knots RL",
      "date": "2024-02-29",
      "description": "The code used for my honors thesis project.",
      "content": "<p>\n                            This project is the backbone of my honors thesis.\n                        </p>\n\n                        <p>\n                            My thesis title is: Using Deep Learning Techniques to Find the 4D Slice Genus of a Knot, with abstract:\n                        </p>\n                            \n                        <p>\n                            Deep reinforcement learning (DRL) has proven to be exceptionally effective in addressing challenges related to pattern recognition and problem-solving, particularly in domains where human intuition faces limitations. Within the field of knot theory, a significant obstacle lies in the construction of minimal-genus slice surfaces for knots of varying complexity. This thesis presents a new approach harnessing the capabilities of DRL to address this challenging problem. By employing braid representations of knots, our methodology involves training reinforcement learning agents to generate minimal-genus slice surfaces. The agents achieve this by identifying optimal sequences of braid transformations with respect to a defined objective function.\n                        </p>\n\n                        <p>\n                            Ultimately, we used PPO to try and find the minimal slice genus of a knot. You can find all my code here, including my implementation of PPO! (And if you are interested in reading my thesis, you can find it <a href=\"https://dylanskinner65.github.io/Using%20Deep%20Learning%20Techniques%20to%20Find%20the%204D%20Slice%20Genus%20of%20a%20Kn.pdf\">here</a>!)\n                        </p>\n\n                        <p>To see the code used in this project, visit my <a href=\"https://github.com/dylanskinner65/DeepRLKnots\">Github</a>.</p>\n\n                        <figure>\n                            <img src=\"projects_files/deep_knots_rl/intro_knots.png\" alt=\"Some knots.\" width=\"90%\" height=\"90%\">\n                        </figure>\n                        \n                        <p></p>\n\n                        <h4>Technologies & Tools Used</h4>\n                        <ul>\n                            <li><strong>Python</strong> \u2013 Primary programming language for model development.</li>\n                            <li><strong>Gym (Custom Environment)</strong> \u2013 Created a reinforcement learning environment tailored to the problem.</li>\n                            <li><strong>PyTorch</strong> \u2013 Built and trained custom PPO deep learning models.</li>\n                            <li><strong>TensorBoard</strong> \u2013 Visualized training progress and model performance.</li>\n                            <li><strong>Supercomputer at BYU</strong> \u2013 Leveraged high-performance computing for multi-GPU large-scale training.</li>\n                        </ul>\n                    \n                        <h4>Project Structure</h4>\n                    \n                        <h5>Custom Gym Environment (<code>gym</code> and <code>gym-knot</code>)</h5>\n                        <p>\n                            To train the agents effectively, I implemented a custom <strong>Gym</strong> environment, which allowed seamless interaction with the problem space. \n                            The repository contains multiple files for environment registration, though only one is necessary\u2014something I\u2019d need to revisit for clarity! \n                            If you're interested in custom environments, OpenAI Gym\u2019s documentation is a great resource.\n                        </p>\n                    \n                        <h5>Experimental Results (<code>result_csv</code>)</h5>\n                        <p>\n                            The <code>result_csv</code> directory holds CSV files documenting various training experiments. Each subfolder represents a separate experiment, labeled as <code>sol_n_m</code>, where:\n                        </p>\n                        <ul>\n                            <li><strong>n</strong> = lower bound for the crossing number used in training</li>\n                            <li><strong>m</strong> = upper bound for the crossing number used in training</li>\n                        </ul>\n                        <p>Each CSV file records:</p>\n                        <ul>\n                            <li>The braid attempted</li>\n                            <li>Epoch and step when a solution was found</li>\n                            <li>The reward achieved</li>\n                            <li>The sequence of moves taken by the agent</li>\n                        </ul>\n                        <p>\n                            Experiments were run in parallel across four GPUs, meaning some experiments may have missing results if certain agents failed to solve their assigned problems.\n                        </p>\n                    \n                        <h3>Training Logs (<code>runs</code>)</h3>\n                        <p>\n                            This folder contains <strong>TensorBoard logs</strong>, providing a detailed view of agent performance across different training runs.\n                            Due to the sheer volume of experiments, this directory contains a significant number of log files.\n                        </p>\n                    \n                        <h5>Stable Baselines PPO Experiments (<code>tensorboard_stable/updated_logs</code>)</h5>\n                        <p>\n                            At one point, I experimented with <strong>Stable Baselines\u2019 PPO algorithm</strong> for comparison with our custom model. \n                            Although this approach was ultimately not used in the final thesis, the logs are preserved here for reference.\n                        </p>\n                    \n                        <h5>Model Weights (<code>training_files</code>)</h5>\n                        <p>\n                            This directory contains <strong>saved model weights</strong> from various experiments. The most useful files are:\n                        </p>\n                        <pre><code>knot_gpu_optim_Large_[number].pt</code></pre>\n                        <p>\n                            Here, <strong>number</strong> refers to the upper bound crossing number. If you want to apply these models to similar problems, \n                            use a file where the number is greater than the maximum crossing number you are solving for.\n                        </p>\n                    \n                        <h5>Core Python Files</h5>\n                        <p>The repository includes various Python scripts used to develop and train the reinforcement learning model.</p>\n                        <ul>\n                            <li><code>knot_gpu_Large.py</code> \u2013 The most important file, containing the model architecture and training loops. This file was responsible for the results presented in my thesis.</li>\n                            <li>Other Python files \u2013 Various iterations of experiments and auxiliary scripts.</li>\n                        </ul>\n                    \n                        <h5>Supercomputer Job Script (<code>knot_jobscript</code>)</h5>\n                        <p>\n                            To efficiently train the model on BYU\u2019s <strong>supercomputer</strong>, I created a <strong>Bash script</strong> for job submission. \n                            While not required to run the model, this script provides insight into configuring large-scale training runs.\n                        </p>\n\n                        \n\n                        <h4>Results</h4>\n\n                        <p>\n                            Before addressing the results achieved by our agent, we describe how the agent was trained. Starting off, the agent is given a range of crossing numbers to consider. \n                            The environment samples a random knot between those crossing numbers and presents it to the environment. The agent is given 500 chances to solve this knot. \n                            (Each epoch provides 100 chances for the agent, so the agent is given 5 epochs total.) \n                            If the agent solves the knot 20 times (allowing for the agent to optimize a solution it found for that knot), a new knot is sampled and the agent begins again. \n                            If the agent does not solve it 20 times in those 500 attempts, we sample and give the agent a new knot attempt. \n                            This process takes approximately 3.5 hours on our GPU for each crossing number range.\n                        </p>\n                    \n                        <p>\n                            We started off by training on knots with five crossings or less, then slowly increased the complexity. \n                            We found that allowing our algorithm to see and work on easier knots to solve while still being challenged by higher crossing knots \n                            aided in the success of the algorithm long term. In our experiments, our algorithm learned to construct minimal genus slice surfaces \n                            for (some, but not all) knots up to 13 crossings.\n                        </p>\n                    \n                        <h5>Training Results</h5>\n                    \n                        <p>\n                            We separate our result figures below into two main parts. The left panel presents the raw training score data. \n                            Each graph (and associated color) represents the model's progress on a different GPU. Since we trained on four GPUs at once, \n                            we have four different sets of training data. If the agent did not find a slice surface with the correct Euler characteristic for the knot, \n                            it was given a reward of <code>-350</code>. Thus, when the agent is successful, the graph contains a spike near <code>0</code>, \n                            but when unsuccessful, the reward does not climb above <code>-350</code>.\n                        </p>\n                    \n                        <p>\n                            The right panel represents the <strong>exponential moving average (EMA)</strong> for the reward. \n                            The exponential moving average places more weight on recent data points, making it more responsive to recent changes. \n                            The formula for EMA is given as:\n                        </p>\n                    \n                        <pre>\n                            EMA_t = (1 - \u03b1) * EMA_{t-1} + \u03b1 * X_t\n                        </pre>\n                    \n                        <p>Where:</p>\n                        <ul>\n                            <li><strong>EMA<sub>t</sub></strong> is the EMA at time <code>t</code>.</li>\n                            <li><strong>EMA<sub>t-1</sub></strong> is the EMA at the previous time step.</li>\n                            <li><strong>X<sub>t</sub></strong> is the value of the time series at time <code>t</code>.</li>\n                            <li><strong>\u03b1</strong> is the smoothing factor or weight applied to the most recent data point. A smaller <code>\u03b1</code> gives more weight to older data.</li>\n                        </ul>\n                    \n                        <p>\n                            For these graphs, we used a smoothing factor of <code>\u03b1 = 0.01</code>. In the initial training stages, \n                            the EMA increases, indicating the agent\u2019s improving ability to find surfaces with the correct Euler characteristic. \n                            However, as the complexity of knots increases, the agent struggles more. This is expected, as solving more complex knots takes longer.\n                        </p>\n                    \n                        <h5>Training Graphs</h5>\n                    \n                        <figure>\n                        <img src=\"projects_files/deep_knots_rl/2_5.png\" alt=\"Training Results for Range 2-5\">\n                        <img src=\"projects_files/deep_knots_rl/3_6.png\" alt=\"Training Results for Range 3-6\">\n                        <img src=\"projects_files/deep_knots_rl/4_7.png\" alt=\"Training Results for Range 4-7\">\n                        <img src=\"projects_files/deep_knots_rl/5_8.png\" alt=\"Training Results for Range 5-8\">\n                        <img src=\"projects_files/deep_knots_rl/6_9.png\" alt=\"Training Results for Range 6-9\">\n                        <img src=\"projects_files/deep_knots_rl/6_10.png\" alt=\"Training Results for Range 6-10\">\n                        <img src=\"projects_files/deep_knots_rl/7_11.png\" alt=\"Training Results for Range 7-11\">\n                        <img src=\"projects_files/deep_knots_rl/6_11.png\" alt=\"Training Results for Range 6-11\">\n                        <img src=\"projects_files/deep_knots_rl/7_12.png\" alt=\"Training Results for Range 7-12\">\n                        <img src=\"projects_files/deep_knots_rl/8_13.png\" alt=\"Training Results for Range 8-13\">\n                        <img src=\"projects_files/deep_knots_rl/8_14.png\" alt=\"Training Results for Range 8-14\">\n                        </figure>\n\n                        <p>As you can see, the model eventually died...</p>\n                    \n                        <h5>Example of a Successful Solution</h5>\n                    \n                        <p>\n                            Below is an example of our agent finding a minimal genus slice surface for the 10-crossing knot with braid word:\n                        </p>\n                    \n                        <code>[2, -1, -1, 3, 1, 2, -4, 3, 4, 1]</code>\n                    \n                        <p>\n                            For this knot, our algorithm received a reward of <code>0.7</code> by performing the moves:\n                        </p>\n                    \n                        <code>[8, 10, 10, 8, 10, 8, 8, 8, 0, 8, 0, 8, 8, 8, 9, 8, 8, 10, 0, 8, 8, 8, 9, 1, 8, 9, 8, 8, 8, 0]</code>\n                    \n                        <p>Here is a visualization of these moves being applied to the prescribed knot:</p>\n                    \n                        <figure>\n                        <img src=\"projects_files/deep_knots_rl/result_moves1.png\" alt=\"Move Sequence Step 1\">\n                        <img src=\"projects_files/deep_knots_rl/result_moves2.png\" alt=\"Move Sequence Step 2\">\n                        <img src=\"projects_files/deep_knots_rl/result_moves3.1.png\" alt=\"Move Sequence Step 3.1\">\n                        <img src=\"projects_files/deep_knots_rl/result_moves3.2.png\" alt=\"Move Sequence Step 3.2\">\n                        <img src=\"projects_files/deep_knots_rl/result_moves4.png\" alt=\"Move Sequence Step 4\">\n                        </figure>\n\n                        <h4>Future Work</h4>\n\n                        <p>One improvement that could be made to this project in the future to improve performance is using GPUs with larger memory capacity. Our model size was ultimately limited to the memory on the GPU, which could very well be what kept us from consistently finding minimal genus slice surfaces for knots with more than 13 crossings.</p>\n\n                           <p>Additionally, we might not have been using the most effective way to represent knots. The use of alternative knot representation methods (other than braids) could yield more effective approaches, potentially enhancing the overall performance of our deep reinforcement learning model in uncovering the minimal slice genus of knots.</p>\n                            \n                            <p>With ongoing research in the Deep RL space new algorithms are being developed which outperform PPO. Exploring and incorporating these future developments will be instrumental in increasing the progress we have made for finding the minimal slice genus of knots.</p>\n\n                        <!-- <h4>Citations</h4> -->\n\n                        <!-- <ol>\n                            <span id=\"adams\">[1]</span> Collin C. Adams, <em>The Knot Book</em>. American Mathematical Society, 2004. ISBN: 978-0821836781.\n                        </ol>\n                        <ol\n                            <span id=\"birman\">[2]</span> Joan S Birman. <em>Braids, links, and mapping class groups</em>. 82. Princeton University Press, 1974.\n                        </ol>    -->\n\n                        <!-- <ol>\n                            <span id=\"seifert\">[1]</span> Heinrich Seifert. <em>\u00dcber das Geschlecht von Knoten</em>. In: <em>Mathematische Annalen</em> 110 (1935), pp. 571\u2013592. DOI: 10.1007/BF01448044.\n                        </ol> -->\n\n                        <!-- <p>\n                            <a id=\"adams\"></a>Adams, C. C. (1994). The Knot Book: An Elementary Introduction to the Mathematical Theory of Knots. W. H. Freeman and Company. -->",
      "quote": "Knit your hearts with an unslipping knot.",
      "quoteAuthor": "William Shakespeare",
      "tags": [
        "RL",
        "Topology",
        "PyTorch"
      ],
      "field": "Topology"
    },
    {
      "slug": "labor-dynamics",
      "title": "Labor Economy Dynamics ODE Model",
      "date": "2023-12-07",
      "description": "A project using ordinary differential equations (odes) to model the labor economy dynamics in the United States.",
      "content": "<p>This project was a joint effort by me, and my friends <a href=\"https://www.linkedin.com/in/jasonwvasquez/\">Jason Vasquez</a>, <a href=\"https://www.linkedin.com/in/ethan-crawford-766463169/\">Ethan Crawford</a>,\n                            and <a href=\"https://www.linkedin.com/in/benjamin-mcmullin/\">BenJ McMullin</a>. When I say \"we,\" I am talking about all of us together.</p>\n\n                        <p>The labor market, including the unemployment rate and the amount of workers looking for jobs, can have a large impact on the economy.\n                            The more people employed means more money being spent, which in turn means more money being made. \n                            Furthermore, rise in unemployment can lead to a recession. Being able to predict the labor market can help us prepare for a recession and help us understand the economy better.\n                            In this paper, we adapt an SIR model to characterize the dynamics of employed, unemployed, and retired individuals in the labor market. \n                            Additionally, we employ a quasi predator-prey model to illustrate the oscillatory behavior observed in the white-collar and blue-collar industries. \n                            By comparing the SIR model to the predator-prey model, we aim to enhance our understanding of the complex interactions within the labor market, \n                            providing potential insights for recession prediction and economic analysis.</p>\n\n\n                        <figure>\n                            <img src=\"projects_files/labor_dynamics/ode_board.jpg\" alt=\"Blackboard with math on it.\" width=\"90%\" height=\"90%\">\n                            <figcaption>This is an image of Camillo De Lellis, taken by Thomas Robert Clarke. You can find the image <a href=\"https://www.ias.edu/ideas/curiosities-partial-differential-equations\">here</a>.</figcaption>\n                        </figure>\n                        \n                        <p></p>\n\n                        <h4>\n                            Background and Motivation\n                        </h4>\n\n                        <p>\n                        One thing that is certain in life is that people will always need jobs. Not only this, but people \n                        will often lose their jobs. Furthermore, people will (eventually) retire from their jobs.\n                        The focus of our project is modeling this situation.\n                        </p>\n\n                        <p>\n                        In recent studies exploring the complexities of employment trajectories and occupational sectors, researchers \n                        have employed various modeling approaches such as Agent-Based Modeling <a href='#9'>[9]</a>, and Markov Chains <a href='#14'>[14]</a>. However, \n                        in a departure from conventional methodologies, our investigation takes an innovative turn by adapting \n                        the SIR (Susceptible-Infectious-Recovered) model <a href='#5'>[5]</a>, typically utilized for studying disease dynamics, to the \n                        realm of employment dynamics. This unique application aims to unravel the intricate propagation of employment \n                        statuses, specifically delving into the transitions between being employed, unemployed, and retired.\n                        </p>\n\n                        <p>\n                        A discernible trend has emerged in recent times, \n                        notably influenced by the technological revolution. The surge in interest and demand for tech-oriented careers \n                        has prompted a significant shift away from traditional blue-collar professions. This migration has led to a \n                        dual challenge: a scarcity of skilled workers in the blue-collar sector and an oversaturation of the tech \n                        industry <a href='#3'>[3]</a>. To capture this relationship between white and blue collar jobs, we also create a quasi-predator-prey \n                        framework inspired by ecological models, which offers insights into the cyclical dynamics between these sectors. \n                        </p>\n                        <p>\n                        Motivated by the imperative to comprehend and address the consequences of this evolving employment landscape, \n                        our research aims to contribute valuable insights for informing strategic policies and industry interventions. \n                        By analyzing the strengths of both the predator-prey framework and the SIR model, we aspire to provide a \n                        comprehensive understanding of the intricate dynamics shaping the contemporary employment scenario.\n                        </p>\n\n                        <h4>Modeling</h4>\n\n                        <h5>Theoretical Framework</h5>\n\n                        <p>\n                            The Susceptible-Infectious-Recovered (SIR) model, developed by Kermack and McKendrick in 1927 <a href='#5'>[5]</a>, \n                            is a foundational mathematical framework for understanding the spread of infectious diseases in populations. \n                            It divides individuals into susceptible, infectious, and recovered compartments, capturing the dynamics of disease transmission.\n                            In our model, we adapt the SIR model to represent the dynamics of the employment market through the labor force,\n                            unemployed, and retired populations.\n                        </p>\n\n                        <h5>Previous Work</h5>\n\n                        <p>We begin by building off the work of ElFadily et. al. <a href='#2'>[2]</a>. In their work, ElFadily et. al. proposed a model\n                            representing the labor force and unemployed populations. They begin by defining their equations as\n                           \n                           $$\\frac{dL}{dt} = \\gamma U - (\\sigma + \\mu)L, \\quad \\frac{dU}{dt} = \\rho \\left(1 - \\frac{L_{\\tau} + U_{\\tau}}{N_c} \\right)L_{\\tau} + \\sigma L - (\\gamma + \\mu)U,$$\n                           \n                           \n                           where $L$ is the labor force, $U$ is the unemployed population, with initial conditions for (1) defined as:\n                           \n                           $$L(0) > 0, \\quad U(0) > 0,$$\n                           \n                           $$(L(\\theta),U(\\theta)) = (\\varphi_1(\\theta), \\varphi_2(\\theta)), \\quad \\theta \\in [-\\tau,0],$$\n                           \n                           where $\\varphi_i\\in C([-\\tau, 0], \\mathbb{R}^+),$  $i=1,2$.\n                           \n                           The parameters are defined as follows:\n                           \n                           <ul>\n                            <li> $\\gamma$: employment rate</li>\n                            <li> $\\sigma$: rate of job loss </li>\n                            <li> $\\mu$: mortality rate</li>\n                            <li> $\\rho$: maximum population growth rate </li>\n                            <li> $N_c$: population carrying capacity </li>\n                            <li> $\\tau$: time lag needed to contribute in the reproductive process of a new individual looking for a job</li>\n                           </ul>\n\n                           </p>\n\n                           <h5>Modifications: The Retirement Group</h5>\n\n                           <p>\n                            With this information in mind, we can begin to adapt this model to fit our desired model structure. We begin by adding a third population, the retired population, $R$.\n                                We can then define our new equations as\n\n\n                                $$\\frac{dL}{dt} = \\gamma U - (\\sigma + \\mu)L {\\color{red} - \\left(\\frac{\\Sigma}{L + U}\\right) L + \\omega\\left(\\frac{\\Sigma}{L + U}\\right) R + \\rho L},$$\n\n                                $$\\frac{dU}{dt} = \\rho\\left(1 - \\frac{L + U}{N_c} \\right)L + \\sigma L -(\\mu + \\gamma)U,$$\n\n                                $$\\frac{dR}{dt} = {\\color{red}\\left(\\frac{\\Sigma}{L + U}\\right) L - \\omega\\left(\\frac{\\Sigma}{L + U}\\right) R - \\mu R},$$\n\n                                which simplify to \n\n                                $$\\frac{dL}{dt} = \\gamma U - (\\sigma + \\mu)L {\\color{red} + (\\omega R - L)\\left(\\frac{\\Sigma}{L + U}\\right) + \\rho L},$$\n                                        \n                                $$\\frac{dU}{dt} = \\rho\\left(1 - \\frac{L + U}{N_c} \\right)L + \\sigma L -(\\mu + \\gamma)U$$\n                                        \n                                $$\\frac{dR}{dt} = {\\color{red}(L - \\omega R)\\left(\\frac{\\Sigma}{L + U}\\right) - \\mu R}.$$\n\n                                One of the first things to note from our equations is the removal of the time lag $\\tau$. \n                                This is because, instead of factoring in people when they are born, we are instead factoring them\n                                in when they turn of working age (for simplification, age 16). This reduces unnecessary complexity in our model. \n                                Additionally, we make two assumptions about unemployed people, being they will\n                                neither retire directly from unemployment, nor contribute to the growth of the population. A final thing to note\n                                is that we have added two new parameters, $\\omega$ and $\\Sigma$. We define $\\Sigma$ to be the number of people who\n                                retire each year in the United States, and $\\omega$ to be the rate at which retired people enter back\n                                into the full-time workforce (which is a dimensionless constant). We can then define our new initial conditions as:\n\n                                $$L(0) > 0, \\quad U(0) > 0, \\quad R(0) > 0,$$\n                                        \n                                $$(L(\\theta),U(\\theta), R(\\theta)) = (\\varphi_1(\\theta), \\varphi_2(\\theta), \\varphi_3(\\theta)), \\quad \\theta \\in [-\\tau,0],$$\n\n                                where $\\varphi_i\\in C([-\\tau, 0], \\mathbb{R}^+),$ $i=1,2,3$. Incorporating nuanced dynamics into our model, we introduce the following terms and elucidate their significance within the equations\n                                \n                                <ul>\n                                    <li>$\\pm\\left(\\frac{\\Sigma}{L + U}\\right) L$: Captures retirements relative to the total workforce, considering the annual number of retirees ($\\Sigma$) as a percentage of the employed population ($L$).</li>\n                                    <li>$\\pm \\omega\\left(\\frac{\\Sigma}{L + U}\\right) R$: Models retired individuals re-entering the workforce, with $\\omega$ representing the transition rate.</li>\n                                    <li>$\\rho L$: Represents natural job growth, proportional to the employed population.</li>\n                                    <li>$-\\mu R$: Represents the natural attrition of retired individuals due to mortality at each time step.</li>\n                                </ul>\n                           </p>\n\n                        <h5>Modifications: The Blue- and White-Collar Groups</h5>\n\n                        <p>\n                            We can further model the labor market by examining the relationship between two industries commonly referred to as the white-collar and blue-collar industries.\n                            In recent years, we have seen an explosion of jobs and interest in the white-collar field, specifically in the tech industry, while the blue-collar industry has seen a decline in interest.\n                            This has led to an oversaturation of the tech industry and a scarcity of labor in the blue-collar industry, which in turn has slowed growth in the former and led to increased growth in the latter.\n                            This cyclical relationship mirrors that of a predator-prey relationship, and we can model it as such. </p>\n\n                            <p>\n                            Choosing a predator-prey model to represent blue-collar and white-collar jobs is justified by its ability to capture dynamic interactions and \n                            cyclic behavior between the two job categories. This modeling approach incorporates feedback loops, reflecting the influence each job type has \n                            on the other, and naturally represents population dynamics in response to economic, educational, or technological factors. \n                            Additionally, the model's adaptability allows for the inclusion of additional factors, offering flexibility in addressing \n                            the multifaceted dynamics of the labor market. This choice opens avenues for research and exploration of hypothetical scenarios, \n                            providing a structured framework to analyze and understand the interplay between different job categories over time.</p>\n\n                            <p>\n                            The classical predator-prey model is given by the following equations:\n\n                            $$ \\frac{dx}{dt} = \\rho x - a x y, \\quad \\frac{dy}{dt} = -\\mu y + \\varepsilon a x y.$$\n\n                            where $x$ is the prey population, $y$ is the predator population, and $\\rho$, $a$, $\\mu$, and $\\varepsilon$ are parameters <a href='#6'>[6]</a> <a href='#13'>[13]</a>. \n                            We can adapt this model by defining the following:\n\n                            $$\\frac{dx}{dt} = \\rho x {\\color{red}\\left(1-\\frac{x}{k}\\right)} - a x y, \\quad \\frac{dy}{dt} = -\\mu y + \\varepsilon a x y + {\\color{red} \\beta y \\left(1-\\frac{y}{C}\\right)}.$$\n\n                            where \n\n                            <ul>\n                                <li> $x$: Blue-collar jobs </li>\n                                <li> $y$: White-collar jobs </li>\n                                <li> $\\rho$: Growth rate of blue-collar jobs </li>\n                                <li> $a$: Rate at which people switch from blue collar to white collar jobs </li>\n                                <li> $\\mu$: Decay rate of white-collar jobs </li>\n                                <li> $\\varepsilon$: The rate at which job transitioning affects the labor market </li>\n                                <li> $k$: Carrying capacity for blue-collar jobs </li>\n                                <li> $C$: Carrying capacity for white-collar jobs. </li>\n                                <li> $\\beta$: Growth rate of white-collar jobs </li>\n                            </ul>\n\n                            Thus, we can interpret the new additions to our model as:\n                            \n                            <ul>\n                            <li> $\\rho x (1-\\frac{x}{k})$: Carrying capacity term for the blue-collar (prey) population, reflecting growth proportional to the number of blue-collar workers. </li>\n                            <li> $\\beta y (1-\\frac{y}{C})$: Carrying capacity term for the white-collar (predator) population, constrained by the workforce size and capable of independent growth. </li>\n                            </ul>\n\n                            The reason that only white-collar jobs have a decay rate is meant to model the current market situation. The white-collar market is becoming\n                            oversaturated so as it increases in population it decays, whereas the blue-collar market has not reached this point and never does with the initial conditions given.\n                            We also made the assumption that the white-collar market converts whereas the blue-collar market does not. This was a choice to model the specific\n                            circumstance we attempt to analyze. In the real world, this could go both ways with varying degrees, but for simplification we made it only one way.\n                        </p>\n\n\n                        <h5>Labor Force, Unemployment, and Retirement Situations</h5>\n\n                        <p>\n                    <ul>\n                    <li> $\\sigma = 0.013905$: Derived from comprehensive data on total layoffs and discharges in the United States (2000-2023) <a href='#4'>[4]</a>.</li>\n                    <li> $\\rho = 0.014577$: Maximum growth rate calculated from MacroTrends Excel data (2000-2022) <a href='#7'>[7]</a>.</li>\n                    <li> $\\gamma = 0.6062$: Employment rate average (2000-2022) from the Bureau of Labor Statistics <a href='#11'>[11]</a>.</li>\n                    <li> $\\mu = 0.008498$: Mortality rate derived from 2000-2022 mortality data <a href='#12'>[12]</a>.</li>\n                    <li> $N_c = 260,000,000$: Population of individuals aged 16 and above in the United States in 2022 <a href='#1'>[1]</a>.</li>\n                    <li> $\\Sigma = 775,045$: Annual retirees in the U.S. (2000-2021) using Social Security Administration data <a href='#10'>[10]</a>, calculated by\n\n                    $$\\Sigma = \\frac{1}{2021 - 2001}\\sum_{i=2001}^{2021}(x_i - x_{i-1})$$</li>\n\n                    <li> $\\omega = 0.063$: Rate at which retirees re-enter the workforce based on research by Maestas <a href='#8'>[8]</a>.</li>\n                    </ul>\n\n                        We began testing our model by running it for 60 years with the current numbers for the United States (see <a href='#figure1'>figure 1</a>). \n\n                        <figure>\n                            <img src=\"projects_files/labor_dynamics/results_lur_1.png\" alt=\"Figure1.\" width=\"90%\" height=\"90%\" id=\"figure1\">\n                            <figcaption text-align=center><b>Figure 1:</b> Initial conditions: $L(0) = 157,000,000$, $U(0) = 6,500,000$, $R(0) = 48,590,000$.</figcaption>\n                        </figure>\n                        \n                        To test the robustness of our model, we ran it with different initial conditions that do not represent the current situation in the United States. \n                        We first decreased the number in the labor force, increased the number of unemployed, and decreased the number of retired. We made these changes rather conservative\n                        by only slightly perturbing the real numbers. We then ran the model for 60 years (see <a href='#figure2'>figure 2</a>).\n                        Parallel to <a href='#figure1'>figure 1</a>, we can see that the model still reaches an equilibrium despite the initial conditions being skewed from their true values.\n  \n                        <figure>\n                            <img src=\"projects_files/labor_dynamics/results_lur_2.png\" alt=\"Figure2.\" width=\"90%\" height=\"90%\" id=\"figure2\">\n                            <figcaption text-align=center><b>Figure 2:</b> Initial conditions: $L(0) = 100,000,000$, $U(0) = 50,000,000$, $R(0) = 10,000,000$.</figcaption>\n                        </figure>\n                        \n                        \n                        We ran our model, once again, against a different set of initial conditions. This time, we significantly decreased the number of people in the occupied\n                        labor force, significantly increased the number of unemployed (ensuring that the number of unemployed was much greater than the number of people in the labor force), and moderately decreased the number of retired. We then ran the model for 60 years (see <a href='#figure3'>figure 3</a>).\n                        As you can see, the model still reaches an equilibrium, despite the initial conditions being heavily skewed from their true values.\n                        \n                        <figure>\n                            <img src=\"projects_files/labor_dynamics/results_lur_3.png\" alt=\"Figure3.\" width=\"90%\" height=\"90%\" id=\"figure3\">\n                            <figcaption text-align=center><b>Figure 3:</b> Initial conditions: $L(0) = 18,000,000$, $U(0) = 200,000,000$, $R(0) = 10,000,000$.</figcaption>\n                        </figure>\n                        \n                        We ran a final test, this time having the number of retired people set as greater than the number of\n                        people in the labor force and unemployed combined. We then ran the model for 60 years (see <a href='#figure4'>figure 4</a>).\n                        \n                        <figure>\n                            <img src=\"projects_files/labor_dynamics/results_lur_4.png\" alt=\"Figure4.\" width=\"90%\" height=\"90%\" id=\"figure4\">\n                            <figcaption text-align=center><b>Figure 4:</b> Initial conditions: $L(0) = 17,500,000$, $U(0) = 20,400,000$, $R(0) = 195,000,000$.</figcaption>\n                        </figure>\n                        \n                        Unlike the previous graphs, we can see that the model does not reach an equilibrium in 60 years. \n                        While the number of people in the labor force rises and the number of retired people falls, \n                        this change does not appear to be significant enough to reach an equilibrium. However,\n                        when ran again for $T = 100$ years, we can see that the model gets closer to an equilibrium, but \n                        still does not reach one (see <a href='#figure5'>figure 5</a>).\n                        \n                        <figure>\n                            <img src=\"projects_files/labor_dynamics/results_lur_5.png\" alt=\"Figure5.\" width=\"90%\" height=\"90%\" id=\"figure5\">\n                            <figcaption text-align=center><b>Figure 5:</b> Initial conditions: $L(0) = 17,500,000$, $U(0) = 20,400,000$, $R(0) = 195,000,000$.</figcaption>\n                        </figure>\n                        </p>\n\n                        <h5>White-Collar and Blue-Collar Simulations</h5>\n\n                        <p>\n                        For our white- and blue-collar model, we experimented with different hyperparameters to see how they would affect the model. \n                    </p>\n                    \n                    <p>\n                        In our first run of the model, we used parameters $\\rho = 7$, $a = 5$, $\\mu = 1$, $\\varepsilon = .2$, $k = 3$, $\\beta = 1$, $C = 1.5$. As we see, the model oscilates\n                        slightly in the beginning, and then settles into a stable equilibrium (see <a href='#figure6'>figure 6</a>). The initial conditions come from data on the US Labor market and percentage of workers in white-collar or blue-collar jobs <a href='#11'>[11]</a>.\n                        \n                        <figure>\n                          <img src=\"projects_files/labor_dynamics/blue_vs_white2.png\" alt=\"Figure6.\" width=\"90%\" height=\"90%\" id=\"figure6\">\n                          <figcaption text-align=center><b>Figure 6:</b> Parameters $\\rho = 7$, $a = 5$, $\\mu = 1$, $\\varepsilon = .2$, $k = 3$, $\\beta = 1$, $C = 1.5$. Zooming in between years $10-50$, we can see the oscillations more clearly.</figcaption>\n                        </figure>\n                        \n                        \n                        Consider now a new set of initial conditions, namely $\\rho = 7$, $a = 5$, $\\mu=2$ $\\varepsilon = .2$, $k = 3$, $\\beta = 1$, $C = 1.5$, and the same set with $\\mu=1$.\n                        Despite only the slight change of $\\mu$ by $1$, our model predicts completely different results (see <a href='#figure7'>figure 7</a>).\n                        \n                        <figure>\n                          <img src=\"projects_files/labor_dynamics/bad_paramenters.png\" alt=\"Figure7.\" width=\"90%\" height=\"90%\" id=\"figure7\">\n                          <figcaption text-align=center><b>Figure 7:</b> The left graph corresponds to $\\mu = 2$, while the right graph corresponds to $\\mu = 1$, others parameters are kept the same.</figcaption>\n                        </figure>\n                    </p>\n\n                    <h4>Results</h4>\n\n                    <p>\n                        The SIR and predator-prey models in the context of employment dynamics offers a comprehensive framework for understanding \n                        the complex interactions within the labor market. Here are some key observations and conclusions drawn from the presented models:</p>\n\n                        <p>\n                    Overall, our SIR model of the labor market shows remarkable stability. \n                    We can see that, regardless of the initial conditions, the model reaches an equilibrium, \n                    with the number of employed, unemployed, and retired individuals remaining relatively constant.\n                    When we used initial conditions that reflected the current numbers for the United States,\n                    the model saw relatively little change as time went on (see <a href='#figure1'>figure 1</a>).\n                    With initial conditions that represented a larger than average unemployed population, the model corrected itself and reached a\n                    similar equilibrium as the previous model (see <a href='#figure2'>figure 2</a>).\n                    Finally, when presented with initial populations that were flipped, the model still stabilized to the same equilibrium\n                    (see <a href='#figure3'>figure 3</a>).\n                    </p>\n\n                    <p>\n                    The predator-prey model is very sensitive to changes in the hyperparameters as even a small change can cause the model to behave very \n                    differently. We see in <a href='#figure7'>figure 7</a> that a change of $1$ in $\\mu$ causes the populations to entirely flip. \n                    This model, while not as robust as the SIR labor force model, still shows some interesting results. It is interesting to see how the relationships in the model\n                    caused oscillations in the different populations. The oscillations are small enough that they are not visible on the graph, but they are still present, and can mimic the overall \n                    labor force where a swing of thousands of jobs is noticed by the economy as a whole (see <a href='#figure6'>figure 6</a>). \n                    A strength of this second model is exactly that, being able to see the oscillations while keeping the oscillations to a scale that would be realistic in the real world.\n                    </p>\n\n                    <h4>Analysis/Conclusions</h4>\n\n                    <p>\n                        In our model, modified from ElFadily et. al. <a href='#2'>[2]</a>, we have extended their adapted SIR framework to capture more interesting dynamics of the labor market. \n                    </p>\n                        In our adaptation, \n                        we introduced an additional compartment for Retired $(R)$ individuals, reflecting the life cycle of employment. The key \n                        modifications involve incorporating terms that represent the natural attrition of the retired population, their potential re-entry into \n                        the workforce, and the growth of job opportunities proportional to the number of employed individuals. These adjustments provide \n                        a nuanced representation of the labor market's temporal evolution, accounting for retirement dynamics, mortality, and the cyclical \n                        nature of job creation and re-entry. This enhanced model allows for a more comprehensive understanding of the complex interactions \n                        within the labor market over time.\n                        </p>\n\n                        <p>\n                        Despite the strengths of our SIR model, there are weaknesses present. One weakness is that changing the initial conditions can cause \n                        the results to differ significantly between each other during the first few years. While it is true that the solutions \n                        end up reaching similar values as $T$ grows, those first few years of difference can pose a problem. Another more significant \n                        weakness is that this model only considers how the labor markets interact with each other. One major factor in the labor market \n                        is the current state of the economy, and our model does not take that into account. Thus, one improvement that can be made is finding a way to \n                        include present economic conditions.\n                        </p>\n\n                        <p>\n                        The predator-prey model, while not as robust as the SIR model, still shows some exciting results. \n                        It is interesting to see how the relationships in the model cause oscillations between the different populations. \n                        The oscillations are small enough that they are not visible on the graph, but they are still present, and can mimic the overall labor force.\n                        However, this second model is very unstable and requires several large simplifications. This model fails to be a accurate representation of the labor market, \n                        and does little else other than show small oscillations. Given more time, we would have loved to expand on this idea and come up with a stable robust model that \n                        can illustrate the oscillations between the white-collar and blue-collar industries.\n                        </p>\n\n                        <p>\n                        Despite its weaknesses, our models provide insights into the long-term stability and equilibrium of the workforce. \n                        The inclusion of retirement-related terms allows policymakers and economists to analyze the impact of demographic shifts on employment trends and anticipate \n                        workforce fluctuations. Moreover, the explicit consideration of job creation and re-entry mechanisms offers a more realistic representation of economic dynamics, \n                        enabling better predictions of labor market behavior. Understanding the cyclical nature of job opportunities and retiree contributions provides valuable insights \n                        for economic planning, workforce management, and policy development. This modified SIR model, by bridging epidemiological principles with labor market dynamics, \n                        contributes to a holistic framework for studying the interplay between demographic factors and economic trends, supporting informed decision-making in the real world.\n                    </p>\n                        \n\n                        <h4>Citations</h4>\n\n                        <ol>\n                            <li id=\"1\">Annie E. Casey Foundation. KIDS COUNT Data Center. <a href=\"https://datacenter.aecf.org/data/tables/99-total-population-by-child-and-adult-populations#detailed/1/any/false/1095,2048,574,1729,37,871,870,573,869,36/39,40,41/416,417\">Link</a>, 2023. Data retrieved from KIDS COUNT Data Center website.</li>\n                            <li id=\"2\">Michele Cal\u00ec, Sanaa ElFadily, and Abdelilah Kaddar. Modeling and mathematical analysis of labor force evolution. <i>Modelling and Simulation in Engineering</i>, 2019:2562468, 2019.</li>\n                            <li id=\"3\">Dana Wilkie. The Blue-Collar Drought. <a href=\"https://www.shrm.org/hr-today/news/all-things-work/pages/the-blue-collar-drought.aspx\">Link</a>, 2023. Accessed: 7 December 2023.</li>\n                            <li id=\"4\">Federal Reserve Bank of St. Louis. Federal Reserve Economic Data. <a href=\"https://fred.stlouisfed.org/series/JTSLDR\">Link</a>, 2023. Data retrieved from the Federal Reserve Economic Data (FRED) website.</li>\n                            <li id=\"5\">W. O. Kermack and A. G. McKendrick. A contribution to the mathematical theory of epidemics. <i>Proceedings of the Royal Society of London. Series A</i>, 115(772):700\u2013721, 1927.</li>\n                            <li id=\"6\">Alfred J. Lotka. Elements of physical biology. <i>Proceedings of the National Academy of Sciences of the United States of America</i>, 14(8):659\u2013664, 1925.</li>\n                            <li id=\"7\">MacroTrends LLC. MacroTrends. <a href=\"https://www.macrotrends.net/countries/USA/united-states/population-growth-rate\">Link</a>, 2023. Data retrieved from MacroTrends website.</li>\n                            <li id=\"8\">Nicole Maestas. Back to work: Expectations and realizations of work after retirement. <i>Journal of Human Resources</i>, 45(3):718\u2013748, 2010.</li>\n                            <li id=\"9\">F\u00e1bio Neves, Pedro Campos, and Sandra Silva. Innovation and employment: An agent-based approach. <i>Journal of Artificial Societies and Social Simulation</i>, 22(1):8, 2019.</li>\n                            <li id=\"10\">Social Security Administration. Number of beneficiaries receiving benefits on December 31, 1970-2022. <a href=\"https://www.ssa.gov/oact/STATS/OASDIbenies.html\">Link</a>, 2023. Data retrieved from Social Security Administration website.</li>\n                            <li id=\"11\">U.S. Bureau of Labor Statistics. HOUSEHOLD DATA ANNUAL AVERAGES 1. Employment status of the civilian noninstitutional population, 1952 to date. <a href=\"https://www.bls.gov/cps/cpsaat01.pdf\">Link</a>, 2023. Data retrieved from the Bureau of Labor Statistics website.</li>\n                            <li id=\"12\">USAFacts. Deaths Per 100,000 People. <a href=\"https://usafacts.org/data/topics/people-society/health/longevity/mortality-rate/\">Link</a>, 2023. Data retrieved from USAFacts website.</li>\n                            <li id=\"13\">Vito Volterra. Fluctuations in the abundance of a species considered mathematically. <i>Nature</i>, 118(2972):558\u2013560, 1926.</li>\n                            <li id=\"14\">Mark Zais and Dan Zhang. A markov chain model of military personnel dynamics. <i>International Journal of Production Research</i>, 54(6):1863\u20131885, 2016.</li>\n                          </ol>\n\n                        <!-- <ol>\n                            <span id=\"adams\">[1]</span> Collin C. Adams, <em>The Knot Book</em>. American Mathematical Society, 2004. ISBN: 978-0821836781.\n                        </ol>\n                        <ol\n                            <span id=\"birman\">[2]</span> Joan S Birman. <em>Braids, links, and mapping class groups</em>. 82. Princeton University Press, 1974.\n                        </ol>    -->\n\n                        <!-- <ol>\n                            <span id=\"seifert\">[1]</span> Heinrich Seifert. <em>\u00dcber das Geschlecht von Knoten</em>. In: <em>Mathematische Annalen</em> 110 (1935), pp. 571\u2013592. DOI: 10.1007/BF01448044.\n                        </ol> -->\n\n                        <!-- <p>\n                            <a id=\"adams\"></a>Adams, C. C. (1994). The Knot Book: An Elementary Introduction to the Mathematical Theory of Knots. W. H. Freeman and Company. -->",
      "quote": "In order to solve this differential equation you look at it until a solution occurs to you.",
      "quoteAuthor": "George Polya",
      "tags": [
        "Math",
        "Dynamics",
        "ODE"
      ],
      "field": "Economics"
    },
    {
      "slug": "nhl-predictor",
      "title": "NHL Live Win Predictor",
      "date": "2024-04-13",
      "description": "A project using shallow learning to both predict, and get the live win percentage of NHL games.",
      "content": "<p>This project was a joint effort by me and my friends <a href=\"https://www.linkedin.com/in/jasonwvasquez/\">Jason Vasquez</a>,\n                             <a href=\"https://www.linkedin.com/in/jeffxhansen/\">Jeff Hansen</a>,\n                              and <a href=\"https://www.linkedin.com/in/benjamin-mcmullin/\">BenJ McMullin</a>. When I say \"we,\" I am talking about the four of us.</p>\n\n                        <p>\n                            The goal of this project is simple: predict the outcomes of NHL games from any given state during the game. \n                            To solve this problem we use three primary methods: Bayesian Network to train a DAG, XGBoost on game states, and an MCMC game simulator. \n                            We use each of these methods to generate win probabilities for each time step throughout various NHL games, thus emulating a live win probability. \n                            Finally, we analyze the accuracy of these three methods and considered ethical implications of our results.\n                        </p>\n\n                        <figure>\n                            <img src=\"projects_files/nhl_predictor/good_xgb_example(2).png\" alt=\"Salt Lake Bees player throwing a pitch.\" width=\"90%\" height=\"90%\">\n                        </figure>\n                        \n                        <p></p>\n\n                        <h4>\n                            Why this Project\n                        </h4>\n\n                        <p>\n                            In the world of sports analytics, predicting the outcomes of games is a common and challenging problem, \n                            with live win predictions adding an extra layer of complexity. \n                            For most sports, there are a plethora of widely accepted\u2014yet hidden\u2014predictive models and methods that are used to predict games. \n                            In addition to this, most sports have easily accessible statistics and graphics that give current win probabilities for any live game.\n                        </p>\n\n                        <p>\n                            Hockey; however, is a different story. While there are some methods used to predict the outcome of National Hockey League (NHL) games, these models\n                            typically belong to sport books and their nuances are not publicly disclosed. Additionally, hockey analytics is not as\n                            developed as it is in other sports, such as basketball or baseball <a href='#1'>[1]</a>. This lack of model transparency and public interest in hockey analytics\n                            makes predicting the outcomes of NHL games a very underdeveloped and challenging problem. Previous attempts and research into predicting NHL games\n                            has relied on methods such as decision trees and artificial neural networks <a href='#5'>[5]</a> (from 2014), na\u00efve bayes and support vector machines <a href='#6'>[6]</a> (from 2013),\n                            and Monte Carlo simulations <a href='#7'>[7]</a> (from 2014).\n                        </p>\n\n                        <p>\n                            In addition to model research, some effort has also gone into developing new features that can be used to better predict the game outcomes. \n                            The two biggest engineered classes of features are the Corsi\n                            and Fenwick<sup>1</sup> metrics (both around 2007); however, we do not use these\n                            in our approach because future research shows they do not improve model performance <a href='#4'>[4]</a>.\n                        </p>\n\n                        <p>\n                            Our project seeks a similar outcome to the research mentioned above: predict the outcomes of NHL games. Not only this,\n                            but we seek to provide live, accurate, win probabilities for any given game state. Despite the simplicity of the problem statement, \n                            the solution is not so straightforward. The NHL provides fast-paced games with many events\n                            occuring in quick succession. Our goal is to use this abundance of data and new approaches to build upon previous research.\n                        </p>\n\n                        <p>\n                            Our motivation for this project exists strictly as fans of the sport and as data scientists. Our model is not intended to be used for gambling or any other\n                            nefarious purposes\u2014any use of this model for such purposes is a misuse of our work.\n                        </p>\n\n                        <sup>1: These metrics were created by sports bloggers Tim Barnes and Mark Fenwick, respectively. \n                            We were unable to locate the original blog posts talking about these metrics, but a good article to learn more about the math can be\n                            found here https://thehockeywriters.com/corsi-fenwick-stats-what-are-they/.</sup>\n\n                        <h4>The Dataset</h4>\n\n                        <p>\n                            Our data came from the hockeyR Github repository <a href='#3'>[3]</a>. This repository contains an abundance of data about every NHL game\n                            that has occured since the 2010-11 season. This data includes information about the events that transpire in a game (hits, shots, goals, etc.),\n                            which teams are playing, who is on the ice, and the final score of the game. The data is stored in a series of <code>.csv.gz</code> files, allowing for\n                            easy access and manipulation.\n                        </p>\n\n                        <p>\n                            Each game in a season is given a unique identifier (<code>game_id</code>), which is constant across all events in a game. Every event that occurs in a game\n                            will be stored in the <code>event_type</code> column. There are 17 unique event types, including game start, faceoff, shot, hit, and goal.\n                            Most of these event types are not relevant to our analysis, so we remove them from the dataset. After removing the unnecessary events, we are left with\n                            nine: blocked shot, faceoff, giveaway, goal, hit, missed shot, penalty, shot, and takeaway. These events are attributed to the\n                            team and player that perform the event. We only take into consideration the team that performs the event and discard the player information to reduce noise and to avoid many one-hot encoded columns.\n                        </p>\n\n                        <p>\n                            The data also contains information about when the event occured. This appears in a variaty of formats, but we only\n                            use the <code>game_time_remaining</code> column. <code>game_time_remaining</code> starts\n                            at 3600 (60 minutes) and counts down to 0. If the game goes into extra time, i.e., it is tied after 60 minutes, <code>game_time_remaining</code> will\n                            be a negative value.\n                        </p>\n\n                        <p>\n                            We found that our data did not contain any missing values that were not easily explainable. For example, if a game is starting, there will be no\n                            events for penalties, which will result in a <code>NaN</code> value in the penalties column. Additionally, any data that was confusing or not easily explainable\n                            (for example the home team having 7 players on the ice and the away team having 5), was manually verified by watching a clip of the game where\n                            the event occured to make sure the event was recorded correctly. We did not find any incorrectly recorded events, so we \n                            did not remove any strange events from out dataset.\n                        </p>\n\n                        <p>\n                            The models were each trained on the 2021-2023 seasons (totaling two seasons), and tested on the most recent 2023-2024 season. \n                            This split was chosen because in practice our model would have access to historical data but not have access to the current season's data as it happens.\n                        </p>\n\n                        <h4>Methods</h4>\n\n                        <h5>Bayesian Network</h5>\n\n                        <p>\n                            We first used a Bayesian network to establish a benchmark for probability using several key features.\n                        </p>\n\n                        <p>\n                            Bayesian networks are probabilistic graphical models that represent probabilistic relationships among a set of variables using a directed acyclic graph (DAG). \nIn a Bayesian network, nodes represent random variables, and directed edges between nodes represent probabilistic dependencies between the variables. Each node in the graph is associated with a conditional probability distribution that quantifies the probability of that variable given its parent variables in the graph.\n                        </p>\n\n                        <p>\n                            For our purposes, we predefined the structure of the network, and used the data to calculate the conditional probabilities for each node. We then used the network to calculate the probability of a team winning given the current state of the game.\n                        </p>\n\n                        <p>\n                            The computational complexity of Bayesian Network inference is high, with exact inference being an NP-hard problem <a href='#2'>[2]</a>. \n                            Using the python package <code>pgmpy</code>, we originally tried to fit a network with all 26 of our features, but our computational resources failed to fit this network.\n                            Then, to get a baseline for our future predictions, we simply fitted the model with the base features of time remaining (tr), home goals (hg), away goals (ag), home shots (hs), away shots (as), home blocked shots (hbs), and away blocked shots (abs) in order to predict wins (w). \n                            These features were chosen as priors because of our opinion that they are the most important to the game, based upon our knowledge of hockey.\n                        </p>\n\n                        <p>\n                            The conditional dependencies of the chosen network are shown in the DAG below:\n                        </p>\n\n                        <figure>\n                            <img src=\"projects_files/nhl_predictor/dag.png\" alt=\"The Bayesian Network used to predict NHL games.\" width=\"100%\" height=\"100%\">\n                            <figcaption text-align=center>The Bayesian Network used to predict NHL games.\n                            </figcaption>\n                        </figure>\n\n                        <p>\n                            This model was chosen for the task because different stats in hockey are conditionally dependent of each other, so by modeling those conditional\n                            dependencies and feeding them into the model, we can hopefully acheive a more accurate prediction of the outcome of the game.\n                        </p>\n\n                        <h5>XGBoost</h5>\n\n                        <p>\n                            In our NHL analysis research, we partitioned the dataset into segments \n                            corresponding to individual teams' games over multiple seasons. \n                            This enabled the creation of time series data in the form of a state vector, capturing the \n                            play-by-play dynamics of each team's matches. We then trained \n                            separate XGBoost models for each team to learn their unique patterns \n                            and strategies. We chose to create an XGBoost model for each team\n                            because we felt it would be more accurate than a single model for all teams.\n                            This is because some teams do very well and some teams do very poorly, and\n                            we felt this difference necessitated a model for each team.\n                        </p>\n\n                        <p>\n                            XGBoost was chosen because of its ability to handle large datasets and model complex relationships between features. \n                            XGBoost is able to attain a high degree of accuracy with a relatively small training time or overhead and was an ideal choice for our research.\n                        </p>\n\n                        <h5>MCMC Game Simulation</h5>\n\n                        <p>\n                            To simulate hockey games, we created a Markov Chain where the states are a tuple of three consecutive\n                            events that occurred. For example, if the home team won a faceoff, lost the puck, and \n                            then the away team shot the puck and missed, the next potential event in the Markov chain would look like table below:\n                        </p>\n\n                        <div style=\"text-align: center;\">\n                            <table> \n                              <tr>\n                                <th>Next Event</th>\n                                <th>Transition Probability</th>\n                              </tr>\n                              <tr>\n                                <td>Shot Away</td>\n                                <td>0.1853207</td>\n                              </tr>\n                              <tr>\n                                <td>Blocked Shot Away</td>\n                                <td>0.1138666</td>\n                              </tr>\n                              <tr>\n                                <td>...</td>\n                                <td>...</td>\n                              </tr>\n                              <tr>\n                                <td>Penalty Away</td>\n                                <td>0.0167210</td>\n                              </tr>\n                              <tr>\n                                <td>Goal Home</td>\n                                <td>0.0085793</td>\n                              </tr>\n                            </table>\n                          </div>\n                        \n                        <p>\n                            The probabilities of transitioning from one triple-state to another triple-state is calculated by:\n\n                            $$\n                            \\begin{aligned}\n                                P(s_{t+1} &= (B,C,D) | s_t = (A,B,C)) = P(D | (A,B,C)) \\\\\n                                &= \\frac{\\{ \\text{number of times (A,B,C,D) happend}\\}}{\\{ \\text{number of times (A,B,C) happened}\\}} \\\\\n                            \\end{aligned}\n                            $$\n\n                            Where $A,B,C,D$ represent events that can occur in a game, and the tuple $(A,B,C,D)$ represents that \"$A$ then \n                            $B$ then $C$ then $D$\" happened right after each other in a game.\n                        </p>\n\n                        <p>\n                            To simulate a game, we performed a Monte Carlo algorithm (see below)\n                            that acts as a random walk through the Markov chain (note that the <code>KDE_times()</code> is a KDE model fit on the amount of seconds that transpired between \n                            each hockey event for all NHL games over the course of 13 years).\n                        </p>\n\n                        <p>\n                            To predict a team's winning probability, we would simulate 50 hockey games with the initial starting states \n                            $(s_0, s_1, s_2)$ set to the most recent events in the hockey game. By looking at each games\n                            final event counts we compute the winner. We then compute the \n                            probability as $P(\\text{home winning}) = \\{\\text{number home simulation wins}\\}/50$ and \n                            $P(\\text{away winning}) = 1-P(\\text{home winning})$.\n                        </p>\n\n                        <figure>\n                            <img src=\"projects_files/nhl_predictor/algo.png\" alt=\"The Monte Carlo algorithm used to simulate NHL games.\" width=\"100%\" height=\"100%\">\n                            <figcaption text-align=center>The Monte Carlo algorithm used to simulate NHL games.\n                            </figcaption>\n                        </figure>\n\n                        <h4>Results and Analysis</h4>\n\n                        <h5>Bayesian Network</h5>\n\n                        <p>\n                            Overall, the Bayesian network performed well and was able to produce realistic win probabilities. Because it defined a joint distribution using a DAG and\n                            then used that distribution to fit the data, it was able to correctly predict accuracies using the few features provided. However, the Bayesian network struggled to capture the intricate dependencies\n                            of factors other than goals that could affect the win probabilities, so the predicted probabilities are little more than an over all probability calculation of goals and time reamining given historical data.\n                            We see this in the probability graph in the figure below, where the changes in probability correspond to the goals scored in the game. \n                        </p>\n\n                        <figure>\n                            <img src=\"projects_files/nhl_predictor/good_bayesian_example.png\" alt=\"The Bayesian Network's predicted win probabilities.\" width=\"100%\" height=\"100%\">\n                            <figcaption text-align=center>The Bayesian Network's predicted win probabilities.\n                            </figcaption>\n                        </figure>\n\n                        <p>\n                            Win probability graph using the Bayesian Network. As seen above, the win probability stayed rather stagnant until a goal was scored, where the probability shifted.\n                        </p>\n\n                        <h5>XGBoost</h5>\n\n                        <p>\n                            For XGBoost, we leveraged the <code>.predict_proba()</code> method to predict win probabilities after the model was fit on the data.\n                            Using this method, we generated \n                            probabilistic predictions at various stages of a game, allowing us to \n                            plot the evolving probability of each team winning throughout the match\n                            (see the figure below). \n                            This approach allowed for more insights into momentum shifts (when one team \n                            has an advantage due to them having more players on the ice due to \n                            a penalty by the opposing team), and key moments (such as goals and \n                            penalties, and critical plays influencing game outcomes) than the Bayesian network, resulting in more variance in the win probability graph.\n                        </p>\n\n                        <figure>\n                            <img src=\"projects_files/nhl_predictor/good_xgb_example.png\" alt=\"The XGBoost model's predicted win probabilities.\" width=\"100%\" height=\"100%\">\n                            <figcaption text-align=center>The XGBoost model's predicted win probabilities.\n                            </figcaption>\n                        </figure>\n\n                        <p>\n                            We can see a high variance of win probability during this game. It is interesting to\n                            see how a goal being scored impacts the win probability. It is also interesting to see how\n                            the probability shifts without a goal being scored, implying the other events in the game\n                            are important to the win probability.\n                        </p>\n\n                        <h5>MCMC Simulator</h5>\n\n                        <p>\n                            To evaluate our simulation's effectiveness at accurately modeling actual hockey \n                            games, we combined a dataset with the final event counts for 1500 actual hockey \n                            games and for 1500 simulated games. We then performed two experiments. First, We \n                            trained a KMeans, Logistic Regression, RandomForest and XGBoost model on this \n                            dataset with default parameters, to see \n                            if they could classify games as simulated or real. With a 70-30 train-test split, \n                            the accuracies on the test set were $47.9\\%$, $48.5\\%$, $67.5\\%$ and $69.3\\%$ respectively. \n                            Second, we fit and transformed this dataset using PCA, t-SNE and UMAP \n                            dimensionality reductions using various perplexity and neighbor hyperparameters to see \n                            if these algorithms clustered the synthetic games and actual games in \n                            different clusters. As shown in the figure below, the simulated\n                            games and actual games are all clustered together. From these two experiments, we conclude\n                            that our simulator effectively models live NHL games.\n                        </p>\n\n                        <figure>\n                            <img src=\"projects_files/nhl_predictor/pca_tsne_umap_sim_act.png\" alt=\"PCA, t-SNE, and UMAP clustering of simulated and real NHL games.\" width=\"100%\" height=\"100%\">\n                            <figcaption text-align=center>A PCA, t-SNA, and UMAP clustering of simulated and real NHL games.\n                            </figcaption>\n                        </figure>\n\n                        <h5>Method Comparison</h5>\n\n                        <p>\n                            Each of our methods has one general purpose: predict the winner of the game regardless of the point of the game. \n                            To evaluate our models, we randomly selected 100 games outside of the training data. \n                            To ensure that the test set represented the actual data, we selected the test set with 50 home wins and 50 away wins, \n                            and we also made sure there was an even representation of each of the 32 NHL teams. \n                            We then used each model to predict a winner at various points throughout each game. \n                            From these predictions on the 100 games, we computed each model's accuracy for each point in the game.\n                        </p>\n\n                        <div style=\"text-align: center;\" font-size=11px>\n                            <table>\n                              <tr>\n                                <th>Game Seconds</th>\n                                <th>Game State</th>\n                                <th>MCMC</th>\n                                <th>XGBoost</th>\n                                <th>Bayesian</th>\n                              </tr>\n                              <tr>\n                                <td>0</td>\n                                <td>Beginning of Game</td>\n                                <td>47%</td>\n                                <td>56%</td>\n                                <td>51%</td>\n                              </tr>\n                              <tr>\n                                <td>900</td>\n                                <td>15 Minutes</td>\n                                <td>68%</td>\n                                <td>55%</td>\n                                <td>69%</td>\n                              </tr>\n                              <tr>\n                                <td>1800</td>\n                                <td>30 Minutes</td>\n                                <td>73%</td>\n                                <td>58%</td>\n                                <td>69%</td>\n                              </tr>\n                              <tr>\n                                <td>2700</td>\n                                <td>45 Minutes</td>\n                                <td>79%</td>\n                                <td>77%</td>\n                                <td>80%</td>\n                              </tr>\n                              <tr>\n                                <td>3540</td>\n                                <td>1 Minute Left</td>\n                                <td>94%</td>\n                                <td>86%</td>\n                                <td>91%</td>\n                              </tr>\n                              <tr>\n                                <td>~3800</td>\n                                <td>OT</td>\n                                <td>96%</td>\n                                <td>93%</td>\n                                <td>100%</td>\n                              </tr>\n                            </table>\n                          </div>\n                          \n                          <p>(Results of each model's accuracy at various points throughout 100 games sampled from the test set.)</p>\n\n                          <p>\n                            In the table above, we note that each model increased in accuracy as the game progressed. \n                            The Bayesian network and MCMC model were the most accurate, while the XGBoost model had lower accuracy.\n                            We infer that the XGBoost models were overfit, as on the training data it would regularly achieve higher than $90\\%$ accuracy at most points during the game. \n                            Also, during experimentation with the models, we noticed that the MCMC simulator tended to produce higher probabilities for the away team winning, \n                            and the XGBoost model tended to give higher probabilities to the home team winning. \n                            The Bayesian Network was unbiased towards the home or away team. \n                            We trained the MCMC model on the flow of events in the game, so it appears that the model picked up on a more predictive flow in how away teams play. \n                            Though the MCMC model was generally the most accurate, it takes about 2 minutes to simulate 50 games and get a single prediction while XGBoost and the Bayesian network can make a prediction in under two seconds.\n                          </p>\n\n\n                        <h4>Conclusion</h4>\n\n                        <p>\n                            In this project, we used three different methods to predict the outcomes of NHL games. We used a Bayesian Network to model the conditional dependencies of key features, XGBoost to model the play-by-play dynamics of each team, and an MCMC simulator to simulate the flow of events in a hockey game. \n                            We found that the Bayesian network and MCMC simulator were more accurate than XGBoost, and Bayesian network also was significantly faster than the MCMC simulator. Therefore we can conclude that the Bayesian network is the best performer for this project, implying that the conditional dependencies and potential causal relationships between features are important for predicting the outcome of hockey games.\n                            The win percentages predicted are more for entertainment and educational purposes, and should not be used for gambling or performance purposes, as they do not have a incredibly high degree of accuracy. There are many variables in a hockey game that affect win probability, and given more time and more data we would love to model other factors such as momentum, historical success, coaching, etc. Our project was limited by the data\n                            we had access to and computational resources, but nonetheless we were able to produce a model that can predict the outcome of NHL games with a reasonable degree of accuracy depending on the time of game remaining.\n                        </p>\n\n                        <p>\n                            To see all of our code, please visit our\n                            <a href=\"https://github.com/jeffxhansen/NHL_Win_Predictor/tree/main\">Github</a>.\n                        </p>\n\n                        <h4>Citations</h4>\n\n                        <ul>\n                            <li><a id='1'>[1]</a> Jair Brooks-Davis. Why isn\u2019t hockey as popular as other sports? those who love the game weigh in. 03 2022.</li>\n                            <li><a id='2'>[2]</a> David Maxwell Chickering, Dan Geiger, and David Heckerman. Learning bayesian networks: Search methods and experimental results. In Doug Fisher and Hans-Joachim Lenz, editors, Pre-proceedings of the Fifth International Workshop on Artificial Intelligence and Statistics, volume R0 of Proceedings of Machine Learning Research, pages 112\u2013128. PMLR, 04\u201307 Jan 1995. Reissued by PMLR on 01 May 2022.</li>\n                            <li><a id='3'>[3]</a> Dan Morse. hockeyR-data: A collection of hockey datasets for use with the hockeyR package. https://github.com/danmorse314/ hockeyR-data, 2024. Accessed: March 2024.</li>\n                            <li><a id='4'>[4]</a> Stephen Pettigrew. Why we should be trying to do better than corsi and fenwick, 05 2014.</li>\n                            <li><a id='5'>[5]</a> Gianni Pischedda. Predicting nhl match outcomes with ml models. International Journal of Computer Applications, 101:15\u201322, 09 2014.</li>\n                            <li><a id='6'>[6]</a> Jason Weissbock, Herna Viktor, and Diana Inkpen. Use of performance metrics to forecast success in the national hockey league. In European Conference on Machine Learning: Sports Analytics and Machine Learning Workshop, 2013.</li>\n                            <li><a id='7'>[7]</a> Josh Weissbock. Forecasting success in the national hockey league using in-game statistics and textual data. 2014.</li>\n                        </ul>\n\n                        <!-- <ol>\n                            <span id=\"adams\">[1]</span> Collin C. Adams, <em>The Knot Book</em>. American Mathematical Society, 2004. ISBN: 978-0821836781.\n                        </ol>\n                        <ol\n                            <span id=\"birman\">[2]</span> Joan S Birman. <em>Braids, links, and mapping class groups</em>. 82. Princeton University Press, 1974.\n                        </ol>    -->\n\n                        <!-- <ol>\n                            <span id=\"seifert\">[1]</span> Heinrich Seifert. <em>\u00dcber das Geschlecht von Knoten</em>. In: <em>Mathematische Annalen</em> 110 (1935), pp. 571\u2013592. DOI: 10.1007/BF01448044.\n                        </ol> -->\n\n                        <!-- <p>\n                            <a id=\"adams\"></a>Adams, C. C. (1994). The Knot Book: An Elementary Introduction to the Mathematical Theory of Knots. W. H. Freeman and Company.",
      "quote": "I am score.",
      "quoteAuthor": "Evgeni Malkin",
      "tags": [
        "Analytics",
        "Bayesian",
        "XGBoost"
      ],
      "field": "Bayesian Analytics"
    },
    {
      "slug": "pitch-predictor",
      "title": "MLB Pitch Predictor",
      "date": "2023-10-16",
      "description": "A project using shallow and deep learning models to predict the next pitch in an MLB game.",
      "content": "<p>This project was a joint effort by me and my friend <a href=\"https://www.linkedin.com/in/jasonwvasquez/\">Jason Vasquez</a>. When I say \"we,\" I am talking about me and Jason.</p>\n\n                        <p>\n                            Pitch prediction is a very complicated process that remains (for the most part) unsolved. Many different independent researchers, as well as well-funded MLB analytics departments, \n                            have set out to achieve the highest degree of accuracy possible. However, with all of the factors involved, including the unpredictable nature of pitchers, any degree of accuracy \n                            that would be noticeably helpful to in-game batters remains elusive. Nevertheless, this problem remains a fun classification/prediction exercise that allows for experimenting with all types of Neural Networks. \n                            For our project, we established accuracy baselines, first on random sampling of pitching distributions and then on a shallow CatBoost machine learning model. Then, we iteratively experimented with different models, \n                            feature engineering, activation functions, and optimizers to attempt to achieve the highest degree of accuracy we could. \n                            Our ultimate goal was to beat 51% accuracy, which was what the author of the dataset on Kaggle achieved by using built-in Keras models.\n                        </p>\n\n                        <figure>\n                            <img src=\"projects_files/pitch_predictor/pitcher_initial.jpg\" alt=\"Salt Lake Bees player throwing a pitch.\" width=\"90%\" height=\"90%\">\n                        </figure>\n                        \n                        <p></p>\n\n                        <h4>\n                            The Dataset\n                        </h4>\n\n                        <p>\n                            The data from our dataset was downloaded from <a href=\"https://www.kaggle.com/datasets/pschale/mlb-pitch-data-20152018\">Kaggle</a>, courtesy of <a href=\"https://www.kaggle.com/pschale\">Paul Schale</a>, it included all pitches from the \n                            2015-2019 MLB seasons including the game situation at the pitch. This data has been widely used for a variety of baseball analytics (sabermetrics) projects.\n                        </p>\n\n                        <h4>Dataset Exploration</h4>\n\n                        <p>\n                            The data includes a <code>pitches.csv</code>, which includes each pitch, the pitcher id, batter id, game id, and lots of information about the pitch such as spin rate, velocity, break angle, etc. \n                            It also contains a prelabeled column <code>pitch type</code> that we will use as our truth values. There is also a file called <code>atbats.csv</code>, which includes information about the \n                            specific at bat, including the score, runners on base, result of the at bat, handedness of the pitcher, and handedness of the batter. Finally, there is a <code>players.csv</code> that includes \n                            the player name to link to the id's in the above tables. For this project, we want to only use data that would be available to a batter or team before a pitch is thrown, \n                            such as the handedness of pitcher, runners on, and most importantly, prior pitches that have been thrown. We calculated a pitch distribution for each pitcher, and our first accuracy baseline was a \n                            model that randomly sampled from the pitcher's distribution for each pitch. Figure 1 below shows the distribution as well as distribution of 2500 random samples for 4 random pitchers in the database. \n                            The random model produced an accuracy of 32.9%.\n                        </p>\n\n                        <figure>\n                            <img src=\"projects_files/pitch_predictor/sampled_dists.png\" alt=\"The sampled distributions.\" width=\"90%\" height=\"90%\">\n                        </figure>\n\n\n                        <h4>Our Methodology</h4>\n\n                        <p>\n                        After our random model, we established another random baseline by fitting our data to a shallow machine learning classifier, CatBoost. This model achieved a 45% accuracy. Our goal was to be higher than 51% with a Deep Learning model.\n                        </p>\n\n                        <p>\n                        To prepare the data, we randomly split pitches into training or testing data using the built-in <code>sklearn.model_selection.train_test_split()</code>. With the data split, we prepared the data by combining the data frames and \n                        filtering out the columns we wouldn't use for our model. We were left with 15 features, namely pitcher name, pitcher handedness, batter, batter stance, batter score, ball count, strike count, outs, runner on 1st, \n                        runner on 2nd, runner on 3rd, inning, pitcher score, and game id. We also had approximately 3.5 million pitches in our dataset, and about 80% of those were in our training data, leaving about 20% for our testing data. \n                        </p>\n                        \n                        <p>\n                        Our first approach was thinking that a Recurrent Neural Network would be the best model for this task. This would allow us to treat a game like a paragraph, with \n                        each pitch a character. Because of the nature of baseball, past pitches (and their success rate) influence future pitches, so we were hopeful a type of RNN model would capture the \n                        patterns and learn how to predict future pitches from past pitches. This provided some difficulties with the data cleaning, as we had to treat a game of pitches like a sequence, and \n                        then pass in one game at a time to our model. However, as the games have different amounts of pitches, we had to pad them which provided some difficulties. \n                        We also tried not padding or batching the pitches and passing one pitch at a time to the RNN, reinitializing the hidden layer every game, but this did not provide \n                        us with the accuracy we were hoping for. Experimenting with an LSTM gate, and a GRU cell provided marginally better accuracy than vanilla RNN, but we were still topping out in the low 40s.\n                    </p>\n                        \n                    <p>\n                        After this, we thought that a transformer with self-attention would be a good way to capture the sequential patterns in the data. With the data still grouped by game and padded, \n                        we passed it through a self-attention transformer. This also slightly bumped up our accuracy, but not to the levels we were hoping for or believed we could get.\n                        At this point, we shifted our approach, we stopped grouping the data by games and realized that feature engineering might be able to unlock higher accuracy for us. \n                        We went through the data, added a column (feature) for each pitch type, and then filled in the columns with how many pitches of that type the pitcher had thrown up to that point in the game. \n                        So at the beginning of the game, all of those columns were 0, but as the pitcher started to throw pitches the columns would populate with past pitch types. \n                        This allowed us to batch the data and not group it by game, which provided for much faster and more efficient training, but it still had the sequential information that was needed for accuracy. \n                        In a way, it became a pseudo-markov chain because each row of data contained all of the information it needed about past events in its current state. \n                        However, as each row only knew about past pitches, this didn't violate the principle of being able to predict pitches in real-time.\n                    </p>\n                        \n                    <p>\n                        With our updated dataset, we shifted the focus of our model to dense linear networks, and experimented with several different amounts of layers, activation functions, etc. \n                        We found that this method worked best of all, and initially saw a result of around 51%. We also formatted the code in a way that allowed us to run it on the supercomputer so we could train for \n                        longer with denser networks. With using the <a href=\"https://arxiv.org/abs/2010.07468\">AdaBelief optimizer</a>, and Penalized TanH as an activation function on our deep model, we achieved accuracy of around 55% after 10 epochs of training. \n                        Then, as a final experiment, we attached a self-attention transformer to our linear model, and found that while the final accuracy didn't improve all that much, the network trained much faster, which is still an improvement. \n                        Figure on the top is the test accuracy with the deep linear model, and the figure on the bottom is with the transformer in the model.\t\n                        </p>\n\n                        <figure>\n                            <img src=\"projects_files/pitch_predictor/test_acc_deep_linear.png\" alt=\"The test accuracy for our deep linear model\" width=\"100%\" height=\"100%\">\n                            <figcaption text-align=center>The test accuracy for our deep linear model.\n                            </figcaption>\n                        </figure>\n\n                        <figure>\n                            <img src=\"projects_files/pitch_predictor/test_acc_for_transformer.png\" alt=\"The test accuracy for our transformer involved model.\" width=\"100%\" height=\"100%\">\n                            <figcaption text-align=center>The test accuracy for our transformer involved model.\n                            </figcaption>\n                        </figure>\n\n                        <h4>Analysis</h4>\n\n                        <p>\n                            After experimenting, we were able to achieve 54-55% accuracy on our test data, and the average for the training data accuracy was in the same range. \n                            We consider this a success, as it beats our baseline accuracy models, and beats the 51% accuracy that the author of the dataset achieved in his Kaggle post. \n                            We consider the most important element to our increased accuracy to be the novel techniques we used on our model and the feature engineering that encoded important information in the dataset before training. \n                            Our first approach of using a Recurrent Neural Network was not our final approach, because it did not achieve our accuracy goal, and after several iterations we were able to achieve higher accuracy without an RNN. \n                            We believe that one limitation that we had was the scope of our data. We had access to 4 seasons of pitch data which only provided just over 3 million pitches. \n                            We are hopeful that with more data and more experimentation, we could achieve even higher results.\n                        </p>\n\n                        <p>\n                            To see all of our code, please visit our\n                            <a href=\"https://github.com/dylanskinner65/PitchPredictor\">Github</a>.\n                        </p>\n\n                        <!-- <h4>Citations</h4> -->\n\n                        <!-- <ol>\n                            <span id=\"adams\">[1]</span> Collin C. Adams, <em>The Knot Book</em>. American Mathematical Society, 2004. ISBN: 978-0821836781.\n                        </ol>\n                        <ol\n                            <span id=\"birman\">[2]</span> Joan S Birman. <em>Braids, links, and mapping class groups</em>. 82. Princeton University Press, 1974.\n                        </ol>    -->\n\n                        <!-- <ol>\n                            <span id=\"seifert\">[1]</span> Heinrich Seifert. <em>\u00dcber das Geschlecht von Knoten</em>. In: <em>Mathematische Annalen</em> 110 (1935), pp. 571\u2013592. DOI: 10.1007/BF01448044.\n                        </ol> -->\n\n                        <!-- <p>\n                            <a id=\"adams\"></a>Adams, C. C. (1994). The Knot Book: An Elementary Introduction to the Mathematical Theory of Knots. W. H. Freeman and Company. -->",
      "quote": "It's unbelievable how much you don't know about the game you've been playing all your life",
      "quoteAuthor": "Mickey Mantle",
      "tags": [
        "DL",
        "MLB",
        "CatBoost"
      ],
      "field": "Deep Learning"
    },
    {
      "slug": "trash-detector",
      "title": "Trash Detector with YOLOv8",
      "date": "2024-03-16",
      "description": "A project where I built a trash detecting fine-tuned computer vision model to detect trash and recycling in an image.",
      "content": "<!-- \n                        <p>This project was a joint effort by me, and my friends <a href=\"https://www.linkedin.com/in/jasonwvasquez/\">Jason Vasquez</a>, <a href=\"https://www.linkedin.com/in/gwen-martin-98057b220/\">Gwen Martin</a>,\n                            and <a href=\"https://www.linkedin.com/in/dallinstewart/\">Dallin Stewart</a>. When I say \"we,\" I am talking about all of us together.</p> -->\n\n                        <p>This was my first computer vision project and it was a lot of fun! Throughout this project I had to learn how to convert a dataset from COCO form to a YOLO form, \n                            utilize bash scripts to do multi-GPU training on a super computer, and build a streamlit app to allow for interactive use!</p>\n\n                        <p>\n                            Here is a video of how my project works! (Built with a streamlit app!)\n                        </p>\n\n                        <figure>\n                            <video width=\"90%\" height=\"90%\" controls>\n                                <source src=\"projects_files/trash_detector/trash_detector_demo.mp4\" type=\"video/mp4\">\n                                Your browser does not support the video tag.\n                            </video>\n                            <figcaption><b>Figure 1:</b> A video of the trash detector in action.</figcaption>\n                        </figure>\n                    \n\n                        <!-- <ol>\n                            <span id=\"adams\">[1]</span> Collin C. Adams, <em>The Knot Book</em>. American Mathematical Society, 2004. ISBN: 978-0821836781.\n                        </ol>\n                        <ol\n                            <span id=\"birman\">[2]</span> Joan S Birman. <em>Braids, links, and mapping class groups</em>. 82. Princeton University Press, 1974.\n                        </ol>    -->\n\n                        <!-- <ol>\n                            <span id=\"seifert\">[1]</span> Heinrich Seifert. <em>\u00dcber das Geschlecht von Knoten</em>. In: <em>Mathematische Annalen</em> 110 (1935), pp. 571\u2013592. DOI: 10.1007/BF01448044.\n                        </ol> -->\n\n                        <!-- <p>\n                            <a id=\"adams\"></a>Adams, C. C. (1994). The Knot Book: An Elementary Introduction to the Mathematical Theory of Knots. W. H. Freeman and Company. -->",
      "quote": "\u201cThe greatest threat to our planet is the belief that someone else will save it.\u201d",
      "quoteAuthor": "Robert Swan",
      "tags": [
        "CV",
        "YOLOv8",
        "Streamlit"
      ],
      "field": "Computer Vision"
    }
  ]
}